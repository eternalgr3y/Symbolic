symbolic_agi/advanced_reasoning_system.py
import asyncio import json import logging import uuid from dataclasses import dataclass from enum import Enum from typing import Any, Dict, List, Optional

from symbolic_agi.api_client import monitored_chat_completion, client as default_client from symbolic_agi.knowledge_base import KnowledgeBase, KnowledgeItemType

Constants for reasoning confidence levels
STRATEGY_SELECTION_THRESHOLD = 0.5

class ReasoningType(Enum): DEDUCTIVE = "deductive" INDUCTIVE = "inductive" ABDUCTIVE = "abductive" ANALOGICAL = "analogical" CAUSAL = "causal" PROBABILISTIC = "probabilistic"

@dataclass class ReasoningContext: """Context for reasoning operations.""" goal: str constraints: List[str] available_knowledge: Dict[str, Any]

@dataclass class ReasoningStep: """A single, structured step in a reasoning chain.""" step_id: str reasoning_type: ReasoningType premise: str conclusion: str confidence: float evidence: List[Dict[str, Any]] assumptions: List[str]

@dataclass class ReasoningChain: """A complete chain of reasoning leading to a final conclusion.""" chain_id: str steps: List[ReasoningStep] final_conclusion: str overall_confidence: float reasoning_path: List[str] alternatives_considered: List[Dict[str, Any]]

class AdvancedReasoningEngine: """ An LLM-powered, multi-strategy reasoning engine that integrates with the AGI's cognitive architecture. It dynamically analyzes problems, selects and executes multiple reasoning strategies in parallel, and synthesizes the results into a coherent conclusion. """

def __init__(self, api_client=None, knowledge_base: Optional[KnowledgeBase] = None):
    self.api_client = api_client or default_client
    self.knowledge_base = knowledge_base
    self.reasoning_history: List[ReasoningChain] = []

async def reason(self, problem: str, context: ReasoningContext) -> ReasoningChain:
    """Main reasoning entry point, orchestrating the cognitive process."""
    logging.info("Starting advanced reasoning for: %s...", problem[:100])

    analysis = await self._llm_analyze_problem(problem, context)
    strategies = self._select_strategies(analysis)
    logging.info(f"Selected reasoning strategies: {[s.value for s in strategies]}")

    reasoning_tasks = [self._execute_strategy(s, problem, context) for s in strategies]
    steps_results = await asyncio.gather(*reasoning_tasks)

    steps = [step for step in steps_results if step]

    chain = await self._synthesize_chain(steps, problem)

    self.reasoning_history.append(chain)
    return chain

async def _llm_analyze_problem(self, problem: str, context: ReasoningContext) -> Dict[str, Any]:
    """Use an LLM to analyze the problem and suggest reasoning strategies."""
    relevant_knowledge_str = "No relevant knowledge found."
    if self.knowledge_base:
        relevant_items = await self.knowledge_base.query_knowledge(query=problem, limit=3)
        if relevant_items:
            knowledge_summaries = [f"- {item.type.value.upper()}: {item.content.get('summary', str(item.content))}" for item in relevant_items]
            relevant_knowledge_str = "\n".join(knowledge_summaries)

    prompt = f"""

Analyze the following problem to determine the best reasoning strategies.

Problem: "{problem}"

Context:

Goal: {context.goal}
Constraints: {context.constraints}
Relevant Distilled Knowledge from my Knowledge Base: {relevant_knowledge_str}

Instructions: Evaluate the problem on these dimensions by outputting a score from 0.0 to 1.0:

Uncertainty: How much ambiguity or probability is involved?
Creativity Required: Does this require novel or out-of-the-box solutions?
Causality Focus: Is understanding cause-and-effect critical?
Data-Driven: Does the solution depend on finding patterns in data?
Analogical Potential: Could this problem be solved by referencing past experiences?
Respond with ONLY a valid JSON object with keys: "uncertainty", "creativity_required", "causality_focus", "data_driven", "analogical_potential". """ try: response = await monitored_chat_completion( role="reasoning_analysis", messages=[{"role": "system", "content": prompt}], response_format={"type": "json_object"}, ) return json.loads(response.choices[0].message.content) except Exception as e: logging.error(f"LLM-based problem analysis failed: {e}. Falling back to default analysis.") return { "uncertainty": 0.5, "creativity_required": 0.5, "causality_focus": 0.5, "data_driven": 0.5, "analogical_potential": 0.5 }

def _select_strategies(self, analysis: Dict[str, Any]) -> List[ReasoningType]:
    """Select appropriate reasoning strategies based on problem analysis scores."""
    strategies = {ReasoningType.DEDUCTIVE}

    if analysis.get("uncertainty", 0) > STRATEGY_SELECTION_THRESHOLD:
        strategies.add(ReasoningType.PROBABILISTIC)
    if analysis.get("creativity_required", 0) > STRATEGY_SELECTION_THRESHOLD:
        strategies.add(ReasoningType.ABDUCTIVE)
    if analysis.get("causality_focus", 0) > STRATEGY_SELECTION_THRESHOLD:
        strategies.add(ReasoningType.CAUSAL)
    if analysis.get("data_driven", 0) > STRATEGY_SELECTION_THRESHOLD:
        strategies.add(ReasoningType.INDUCTIVE)
    if analysis.get("analogical_potential", 0) > STRATEGY_SELECTION_THRESHOLD:
        strategies.add(ReasoningType.ANALOGICAL)

    return list(strategies)

async def _execute_strategy(self, strategy: ReasoningType, problem: str, context: ReasoningContext) -> Optional[ReasoningStep]:
    """Execute a single reasoning strategy by prompting an LLM."""
    prompt = self._get_prompt_for_strategy(strategy, problem, context)
    try:
        response = await monitored_chat_completion(
            role=f"reasoning_{strategy.value}",
            messages=[{"role": "system", "content": prompt}],
            response_format={"type": "json_object"},
        )
        result_data = json.loads(response.choices[0].message.content)
        return ReasoningStep(
            step_id=f"{strategy.value}_{uuid.uuid4().hex[:6]}",
            reasoning_type=strategy,
            premise=result_data.get("premise", "N/A"),
            conclusion=result_data.get("conclusion", "N/A"),
            confidence=result_data.get("confidence", 0.5),
            evidence=result_data.get("evidence", []),
            assumptions=result_data.get("assumptions", [])
        )
    except Exception as e:
        logging.error(f"Reasoning strategy '{strategy.value}' failed: {e}")
        return None

async def _synthesize_chain(self, steps: List[ReasoningStep], problem: str) -> ReasoningChain:
    """Synthesize the results of multiple reasoning steps into a final conclusion using an LLM."""
    if not steps:
        return ReasoningChain("chain_empty", [], "Unable to reason about the problem.", 0.0, [], [])

    steps_summary = "\n".join([f"- **{s.reasoning_type.name}**: {s.conclusion} (Confidence: {s.confidence:.2f})" for s in steps])
    prompt = f"""

You are a master synthesizer AI. Your task is to integrate multiple, potentially conflicting, lines of reasoning into a single, coherent, and actionable conclusion for the given problem.

Original Problem: "{problem}"

Parallel Reasoning Step Results: {steps_summary}

Instructions:

Review all reasoning steps. Identify the most compelling, well-supported, and confident conclusions.
Identify Conflicts: Note any contradictions or alternative perspectives.
Formulate a Final Conclusion: Synthesize the inputs into a single, actionable final conclusion. If there are conflicts, choose the most likely or safest path forward, or state the remaining uncertainty.
Calculate Overall Confidence: Based on the confidence of the input steps and their agreement, calculate an overall confidence score for your final conclusion (0.0 to 1.0).
List Alternatives: Briefly list any significant alternative conclusions that were considered but ultimately discarded.
Respond with ONLY a valid JSON object with keys: "final_conclusion", "overall_confidence", "alternatives_considered". """ try: response = await monitored_chat_completion( role="reasoning_synthesis", messages=[{"role": "system", "content": prompt}], response_format={"type": "json_object"}, ) synthesis_data = json.loads(response.choices[0].message.content) return ReasoningChain( chain_id=f"chain_{uuid.uuid4().hex[:8]}", steps=steps, final_conclusion=synthesis_data.get("final_conclusion", "Synthesis failed."), overall_confidence=synthesis_data.get("overall_confidence", 0.5), reasoning_path=[s.step_id for s in steps], alternatives_considered=synthesis_data.get("alternatives_considered", []) ) except Exception as e: logging.error(f"Reasoning synthesis failed: {e}") best_step = max(steps, key=lambda s: s.confidence) if steps else None if not best_step: return ReasoningChain("chain_fallback_empty", [], "Reasoning and synthesis failed.", 0.0, [], []) return ReasoningChain( chain_id=f"chain_fallback_{uuid.uuid4().hex[:8]}", steps=steps, final_conclusion=f"Fallback Conclusion: The most confident reasoning was {best_step.reasoning_type.name}, which concluded: {best_step.conclusion}", overall_confidence=best_step.confidence * 0.8, reasoning_path=[s.step_id for s in steps], alternatives_considered=[{"reason": "LLM Synthesis Failed", "conclusion": "Used best-step fallback."}] )

def _get_prompt_for_strategy(self, strategy: ReasoningType, problem: str, context: ReasoningContext) -> str:
    """Returns the specific, high-quality LLM prompt for a given reasoning strategy."""

    json_format_instruction = """

Your response MUST be a single, valid JSON object with the following keys:

"premise": The primary information, rule, or observation you are starting from.

"conclusion": Your reasoned conclusion for this specific reasoning step.

"confidence": Your confidence in this conclusion (float from 0.0 to 1.0).

"evidence": A list of strings or dictionaries representing evidence used.

"assumptions": A list of strings representing any assumptions made. """

  prompts = {
      ReasoningType.DEDUCTIVE: f"Problem: '{problem}'. Apply strict deductive logic. Given the context and known facts, derive a conclusion that is guaranteed to be true. Avoid making assumptions.",
      ReasoningType.INDUCTIVE: f"Problem: '{problem}'. Apply inductive reasoning. Analyze the provided context and any available data to identify patterns, trends, or general principles. Formulate a likely conclusion based on these observations.",
      ReasoningType.ABDUCTIVE: f"Problem: '{problem}'. Apply abductive reasoning. Generate the most plausible explanation or hypothesis for the observed problem. What is the simplest and most likely cause or solution? This is about inference to the best explanation.",
      ReasoningType.ANALOGICAL: f"Problem: '{problem}'. Apply analogical reasoning. Find a similar, known problem from your memory or general knowledge and adapt its solution. Clearly state the source analogy and how you are adapting it.",
      ReasoningType.CAUSAL: f"Problem: '{problem}'. Apply causal reasoning. Identify the cause-and-effect relationships. What are the likely causes of the problem? What are the probable effects of potential actions?",
      ReasoningType.PROBABILISTIC: f"Problem: '{problem}'. Apply probabilistic reasoning. Assess the uncertainties involved. Assign probabilities to different outcomes and determine the most likely result based on statistical or heuristic analysis."
  }

  base_prompt = prompts.get(strategy, f"Reason about the problem '{problem}'.")

  return f"{base_prompt}\n\n**Context:**\n- Goal: {context.goal}\n- Constraints: {context.constraints}\n\n{json_format_instruction}"

--- FILE: symbolic_agi/agent.py ---

symbolic_agi/agent.py
import asyncio import json import logging from typing import TYPE_CHECKING, Any, Dict

from openai import AsyncOpenAI from pydantic import ValidationError from .skill_manager import register_innate_action

from . import prompts from .api_client import monitored_chat_completion from .schemas import MessageModel, SkillModel from .message_bus import RedisMessageBus

Constants for error messages
NO_CONTENT_FROM_LLM_ERROR = "No content returned from LLM."

if TYPE_CHECKING: from .message_bus import RedisMessageBus

class Agent: def init(self, name: str, message_bus: "RedisMessageBus", api_client: AsyncOpenAI): self.name = name self.persona = name.split("")[-2].lower() if "" in name else "specialist" self.bus = message_bus self.client = api_client self.inbox: "asyncio.Queue[MessageModel | None]" = self.bus.subscribe(self.name) self.running = True logging.info( "Agent '%s' initialized with persona '%s'", self.name, self.persona, )

async def run(self) -> None:
    """Main loop for the agent to process messages."""
    while self.running:
        try:
            message = await asyncio.wait_for(
                self.inbox.get(), timeout=1.0
            )
            if message is not None:
                await self.handle_message(message)
            self.inbox.task_done()
        except asyncio.TimeoutError:
            continue
        except asyncio.CancelledError:
            self.running = False
            logging.info("Agent '%s' received cancel signal.", self.name)
            raise  # Re-raise CancelledError for proper async cleanup
    logging.info("Agent '%s' has shut down.", self.name)

async def _reply(
    self, original_message: MessageModel, payload: Dict[str, Any]
) -> None:
    """Helper to send a reply message."""
    reply = MessageModel(
        sender_id=self.name,
        receiver_id=original_message.sender_id,
        message_type=f"{original_message.message_type}_result",
        payload=payload,
    )
    await self.bus.publish(reply)

async def handle_message(self, message: MessageModel) -> None:
    """Handles incoming messages and routes them to appropriate skills or handlers."""
    logging.info(
        "Agent '%s' received message of type '%s' from '%s'.",
        self.name,
        message.message_type,
        message.sender_id,
    )

    skill_method = None
    if hasattr(self, message.message_type):
        method = getattr(self, message.message_type)
        if hasattr(method, "_innate_action_persona"):
            if getattr(method, "_innate_action_persona") == self.persona:
                skill_method = method

    if skill_method:
        result_payload = await skill_method(message.payload)
        await self._reply(message, result_payload)
    elif message.message_type == "new_skill_broadcast":
        skill_name = message.payload.get("skill_name")
        skill_description = message.payload.get("skill_description")
        logging.info(
            "Agent '%s' learned about a new skill: '%s' - %s",
            self.name,
            skill_name,
            skill_description,
        )
    else:
        logging.warning(
            "Agent '%s' does not know how to handle message type '%s'.",
            self.name,
            message.message_type,
        )
        await self._reply(
            message,
            {
                "status": "failure",
                "error": (
                    f"Agent '{self.name}' does not have skill "
                    f"'{message.message_type}'."
                ),
            },
        )

@register_innate_action(
    "browser", "Analyzes a web page and decides the next interaction."
)

File: symbolic_agi/agent.py
Fix for skill_review_skill_efficiency method (around line 127)
async def skill_review_skill_efficiency(self, skill_name: str) -> str:
    """Reviews a skill's efficiency and provides optimization suggestions."""
    try:
        # Get skill details from skill manager
        skill_details = await self.skill_manager.get_skill_details(skill_name)
        
        # FIX 1: Validate skill_details before processing
        if not skill_details:
            self.logger.warning(f"No skill details found for '{skill_name}'")
            return f"Skill '{skill_name}' not found in skill manager."
        
        # FIX 2: Handle placeholder strings
        if isinstance(skill_details, str) and skill_details == "<<retrieved_skill_details>>":
            self.logger.error(f"Received placeholder instead of skill details for '{skill_name}'")
            # Attempt to retrieve actual skill data
            skill_model = await self.skill_manager.get_skill(skill_name)
            if skill_model:
                skill_details = skill_model.model_dump()
            else:
                return f"Unable to retrieve details for skill '{skill_name}'."
        
        # FIX 3: Ensure skill_details is a dictionary
        if not isinstance(skill_details, dict):
            self.logger.error(f"Invalid skill details type: {type(skill_details)}")
            return f"Invalid skill data format for '{skill_name}'."
        
        # FIX 4: Safe model validation with error handling
        try:
            skill_model = SkillModel.model_validate(skill_details)
        except ValidationError as e:
            self.logger.error(f"Skill validation failed for '{skill_name}': {e}")
            return f"Skill '{skill_name}' has invalid data structure: {e}"
        
        # Continue with the review process...
        review_prompt = f"""
        Review this skill for efficiency and optimization opportunities:
        Name: {skill_model.name}
        Description: {skill_model.description}
        Implementation: {skill_model.implementation}
        
        Provide specific suggestions for improvement.
        """
        
        response = await self._query_llm(review_prompt)
        return response.strip()
        
    except Exception as e:
        self.logger.error(f"Error reviewing skill '{skill_name}': {e}")
        return f"Error reviewing skill: {str(e)}"

@register_innate_action(
    "qa", "Reviews a plan for logical flaws or inefficiency."
)
async def skill_review_plan(self, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Reviews a plan for logical flaws, inefficiency, or misinterpretation of the
    original goal. Provides actionable feedback for refinement if the plan is rejected.
    """
    goal = params.get("original_goal", "No goal provided.")
    plan = params.get("plan_to_review", [])
    plan_str = json.dumps(plan, indent=2)
    prompt = prompts.REVIEW_PLAN_PROMPT.format(goal=goal, plan_str=plan_str)
    try:
        resp = await monitored_chat_completion(
            role="qa",
            messages=[{"role": "system", "content": prompt}],
            response_format={"type": "json_object"},
        )
        if resp.choices and resp.choices[0].message.content:
            review_data = json.loads(resp.choices[0].message.content)
            return {
                "status": "success",
                "approved": review_data.get("approved", False),
                "feedback": review_data.get(
                    "feedback", "QA review returned an incomplete response."
                ),
            }
        return {"status": "failure", "error": NO_CONTENT_FROM_LLM_ERROR}
    except Exception as e:
        return {"status": "failure", "error": str(e)}

@register_innate_action(
    "coder", "Generates Python code based on a prompt and context."
)
async def skill_write_code(self, params: Dict[str, Any]) -> Dict[str, Any]:
    """Generates Python code, using and updating its own short-term state."""
    prompt = params.get("prompt", "Write a simple hello world python script.")
    context = params.get("context", "")
    workspace = params.get("workspace", {})
    agent_state = params.get("agent_state", {})
    llm_prompt = prompts.WRITE_CODE_PROMPT.format(
        context=context,
        research_summary=workspace.get("research_summary", "N/A"),
        previous_code=agent_state.get(
            "previous_code", "# This is your first step in this session."
        ),
        prompt=prompt,
    )
    try:
        resp = await monitored_chat_completion(
            role="agent_skill",
            messages=[{"role": "system", "content": llm_prompt}],
            response_format={"type": "json_object"},
        )
        if resp.choices and resp.choices[0].message.content:
            response_data = json.loads(resp.choices[0].message.content)
            return {
                "status": "success",
                "generated_code": response_data.get(
                    "generated_code", "# ERROR: No code generated"
                ),
                "state_updates": response_data.get("state_updates", {}),
            }
        return {"status": "failure", "error": NO_CONTENT_FROM_LLM_ERROR}
    except Exception as e:
        return {"status": "failure", "error": str(e)}

@register_innate_action(
    "research", "Researches a given topic and provides a concise summary."
)
async def skill_research_topic(self, params: Dict[str, Any]) -> Dict[str, Any]:
    """Researches a given topic and provides a concise summary."""
    topic = params.get("topic", "The history of artificial intelligence.")
    llm_prompt = prompts.RESEARCH_TOPIC_PROMPT.format(topic=topic)
    try:
        resp = await monitored_chat_completion(
            role="agent_skill",
            messages=[{"role": "system", "content": llm_prompt}],
        )
        content = resp.choices[0].message.content
        if content is not None:
            summary = content.strip()
            return {"status": "success", "research_summary": summary}
        return {"status": "failure", "error": NO_CONTENT_FROM_LLM_ERROR}
    except Exception as e:
        return {"status": "failure", "error": str(e)}

@register_innate_action(
    "qa", "Reviews provided Python code for quality and improvements."
)
async def skill_review_code(self, params: Dict[str, Any]) -> Dict[str, Any]:
    """Reviews provided Python code for quality and improvements."""
    workspace = params.get("workspace", {})
    code_to_review = workspace.get(
        "generated_code", "# No code provided to review."
    )
    llm_prompt = prompts.REVIEW_CODE_PROMPT.format(code_to_review=code_to_review)
    try:
        resp = await monitored_chat_completion(
            role="qa", messages=[{"role": "system", "content": llm_prompt}]
        )
        content = resp.choices[0].message.content
        if content is not None:
            review = content.strip()
            return {"status": "success", "code_review": review}
        return {"status": "failure", "error": NO_CONTENT_FROM_LLM_ERROR}
    except Exception as e:
        return {"status": "failure", "error": str(e)}

--- FILE: symbolic_agi/agent_pool.py ---

symbolic_agi/agent_pool.py
import logging from datetime import datetime, timezone from typing import TYPE_CHECKING, Any, cast, Optional import asyncio import time

from . import config, metrics from .message_bus import RedisMessageBus from .symbolic_identity import SymbolicIdentity from .symbolic_memory import SymbolicMemory

if TYPE_CHECKING: from .skill_manager import SkillManager

class DynamicAgentPool: """Manages a collection of sub-agents with different personas and persistent state."""

skill_manager: "SkillManager"

def __init__(
    self: "DynamicAgentPool", bus: RedisMessageBus, skill_manager: "SkillManager"
):
    self.subagents: dict[str, dict[str, Any]] = {}
    self.bus: RedisMessageBus = bus
    self.skill_manager = skill_manager
    logging.info("[AgentPool] Initialized.")

def add_agent(
    self: "DynamicAgentPool", name: str, persona: str, memory: SymbolicMemory
) -> None:
    """Adds a new sub-agent to the pool, initializing its state and trust score."""
    if name in self.subagents:
        logging.warning("Agent with name '%s' already exists. Not adding.", name)
        return

    self.subagents[name] = {
        "name": name,
        "persona": persona.lower(),
        "identity": SymbolicIdentity(memory),
        "memory": memory,
        "state": {
            "trust_score": config.INITIAL_TRUST_SCORE,
            "last_used_timestamp": datetime.now(timezone.utc).isoformat(),
            "performance_history": [],  # Track recent performance
            "trust_momentum": 0.0,  # Trust change velocity
            "total_tasks": 0,
            "successful_tasks": 0,
            "consecutive_failures": 0,
        },
    }
    self.bus.subscribe(name)
    metrics.AGENT_TRUST.labels(agent_name=name, persona=persona.lower()).set(
        config.INITIAL_TRUST_SCORE
    )
    logging.info(
        "[AgentPool] Added agent: %s with persona: %s and trust: %s",
        name,
        persona.lower(),
        config.INITIAL_TRUST_SCORE,
    )

def get_agent_state(self: "DynamicAgentPool", agent_name: str) -> dict[str, Any]:
    """Retrieves the state dictionary for a specific agent."""
    agent = self.subagents.get(agent_name)
    if agent:
        return cast("dict[str, Any]", agent.get("state", {}))
    return {}

def update_agent_state(
    self: "DynamicAgentPool", agent_name: str, updates: dict[str, Any]
) -> None:
    """Updates the state dictionary for a specific agent."""
    if agent_name in self.subagents:
        self.subagents[agent_name]["state"].update(updates)
        logging.info(
            "Updated state for agent '%s'. New keys: %s",
            agent_name,
            list(updates.keys()),
        )
    else:
        logging.warning(
            "Attempted to update state for non-existent agent: %s", agent_name
        )

def update_trust_score(
    self, agent_name: str, new_score: float, last_used: bool = True
) -> None:
    """Updates the trust score for a specific agent and optionally the last used timestamp."""
    if agent_name in self.subagents:
        old_score = self.subagents[agent_name]["state"]["trust_score"]
        clamped_score = max(0.0, min(config.MAX_TRUST_SCORE, new_score))
        
        # Calculate trust momentum (rate of change)
        trust_momentum = clamped_score - old_score
        
        self.subagents[agent_name]["state"]["trust_score"] = clamped_score
        self.subagents[agent_name]["state"]["trust_momentum"] = trust_momentum
        
        if last_used:
            self.subagents[agent_name]["state"]["last_used_timestamp"] = (
                datetime.now(timezone.utc).isoformat()
            )
        
        metrics.AGENT_TRUST.labels(
            agent_name=agent_name,
            persona=self.subagents[agent_name]["persona"],
        ).set(clamped_score)
        
        logging.debug(f"Updated trust for {agent_name}: {old_score:.2f} -> {clamped_score:.2f} (momentum: {trust_momentum:+.2f})")
    else:
        logging.warning(
            "Attempted to update trust for non-existent agent: %s", agent_name
        )

def record_task_performance(self, agent_name: str, success: bool, task_complexity: float = 0.5) -> None:
    """Record task performance and update trust based on performance momentum."""
    if agent_name not in self.subagents:
        logging.warning(f"Cannot record performance for non-existent agent: {agent_name}")
        return
    
    agent_state = self.subagents[agent_name]["state"]
    
    # Update counters
    agent_state["total_tasks"] += 1
    if success:
        agent_state["successful_tasks"] += 1
        agent_state["consecutive_failures"] = 0
    else:
        agent_state["consecutive_failures"] += 1
    
    # Track performance history (last 10 tasks)
    performance_history = agent_state["performance_history"]
    performance_history.append({
        "success": success,
        "complexity": task_complexity,
        "timestamp": datetime.now(timezone.utc).isoformat()
    })
    
    # Keep only recent history
    if len(performance_history) > 10:
        performance_history.pop(0)
    
    # Calculate trust adjustment based on performance momentum
    trust_adjustment = self._calculate_trust_adjustment(agent_state, success, task_complexity)
    
    # Update trust score
    current_trust = agent_state["trust_score"]
    new_trust = max(0.0, min(config.MAX_TRUST_SCORE, current_trust + trust_adjustment))
    
    self.update_trust_score(agent_name, new_trust)
    
    logging.info(f"Recorded performance for {agent_name}: success={success}, trust_change={trust_adjustment:+.3f}")

def _calculate_trust_adjustment(self, agent_state: dict[str, Any], success: bool, task_complexity: float) -> float:
    """Calculate trust adjustment based on performance momentum and emotional context."""
    base_adjustment = config.TRUST_DECAY_RATE if not success else config.TRUST_DECAY_RATE * 0.5
    
    # Scale by task complexity
    base_adjustment *= (1.0 + task_complexity)
    
    # Consider consecutive failures
    consecutive_failures = agent_state.get("consecutive_failures", 0)
    if consecutive_failures > 2:
        base_adjustment *= (1.0 + consecutive_failures * 0.2)
    
    # Consider recent performance trend
    performance_history = agent_state.get("performance_history", [])
    if len(performance_history) >= 3:
        recent_successes = sum(1 for p in performance_history[-3:] if p["success"])
        success_rate = recent_successes / 3.0
        
        # Momentum bonus/penalty
        if success and success_rate > 0.66:
            base_adjustment *= 1.2  # Positive momentum
        elif not success and success_rate < 0.33:
            base_adjustment *= 1.3  # Negative momentum
    
    return base_adjustment if success else -base_adjustment

def get_best_agent_for_persona(self, persona: str, emotional_context: dict[str, Any] | None = None) -> str | None:
    """Select the best agent for a persona considering trust, momentum, and emotional context."""
    candidates = [
        agent for agent in self.subagents.values()
        if agent.get("persona") == persona.lower()
    ]
    
    if not candidates:
        return None
    
    # Score agents based on multiple factors
    scored_agents = []
    
    for agent in candidates:
        agent_state = agent["state"]
        trust_score = agent_state.get("trust_score", 0.0)
        trust_momentum = agent_state.get("trust_momentum", 0.0)
        consecutive_failures = agent_state.get("consecutive_failures", 0)
        
        # Base score is trust
        score = trust_score
        
        # Momentum bonus (positive momentum is good)
        score += trust_momentum * 0.1
        
        # Penalty for consecutive failures
        score -= consecutive_failures * 0.1
        
        # Emotional context adjustments
        if emotional_context:
            frustration = emotional_context.get("frustration", 0.0)
            confidence = emotional_context.get("confidence", 0.5)
            
            # When frustrated, prefer more reliable agents
            if frustration > 0.7:
                reliability_bonus = min(0.2, trust_score * 0.2)
                score += reliability_bonus
            
            # When low confidence, avoid risky agents
            if confidence < 0.4 and consecutive_failures > 1:
                score -= 0.15
        
        scored_agents.append((score, agent["name"]))
    
    # Sort by score (highest first) and return best agent
    scored_agents.sort(key=lambda x: x[0], reverse=True)
    best_agent_name = scored_agents[0][1]
    
    logging.debug(f"Selected agent {best_agent_name} for persona {persona} with score {scored_agents[0][0]:.2f}")
    return best_agent_name

def get_all(self: "DynamicAgentPool") -> list[dict[str, Any]]:
    """Returns all agents in the pool."""
    return list(self.subagents.values())

def get_agents_by_persona(self: "DynamicAgentPool", persona: str) -> list[str]:
    """Gets a list of agent names matching a specific persona."""
    return [
        agent["name"]
        for agent in self.subagents.values()
        if agent.get("persona") == persona
    ]

def get_all_personas(self) -> list[str]:
    """Returns a list of all unique, currently available persona names."""
    personas: set[str] = {
        agent.get("persona", "unknown") for agent in self.subagents.values()
    }
    return sorted(personas)

def get_all_action_definitions(self) -> list[dict[str, Any]]:
    """Gets a list of all available actions (innate and learned)."""
    all_actions = [
        action.model_dump() for action in self.skill_manager.innate_actions
    ]

    # Add learned skills as orchestrator-level actions
    latest_skills: dict[str, Any] = {}
    for skill in self.skill_manager.skills.values():
        # Only include the latest version of each skill
        if (
            skill.name not in latest_skills
            or skill.version > latest_skills[skill.name].get("version", 0)
        ):
            latest_skills[skill.name] = {
                "name": skill.name,
                "description": skill.description,
                "version": skill.version,
                "parameters": [
                    {
                        "name": "input",
                        "type": "str", 
                        "description": "The primary input for the skill.",
                        "required": False,
                    }
                ],
                "assigned_persona": "orchestrator",
                "type": "learned",
            }
    
    all_actions.extend(latest_skills.values())
    return all_actions

async def delegate_task(
    self, task_name: str, agent_name: str, **kwargs: Any
) -> Optional[dict[str, Any]]:
    """
    Delegate a task to a specific agent with increased timeout.
    """
    if agent_name not in self.subagents:
        logging.error("Agent '%s' not found for delegation", agent_name)
        return None
    
    logging.info(
        "[Delegate] Delegated task '%s' to '%s'. Waiting for reply...",
        task_name,
        agent_name,
    )
    
    # Send task request
    request_id = f"task_{int(time.time() * 1000)}"
    task_request = {
        "type": "task_request",
        "request_id": request_id,
        "task_name": task_name,
        "kwargs": kwargs,
    }
    
    await self.bus.publish(f"agent:{agent_name}", task_request)
    
    # Wait for response with increased timeout for QA tasks
    timeout = 60.0 if task_name == "review_plan" else 30.0  # Longer timeout for QA reviews
    
    try:
        response = await asyncio.wait_for(
            self.bus.receive_one(f"orchestrator:{request_id}"),
            timeout=timeout,
        )
        
        # Update agent metrics
        agent = self.subagents[agent_name]
        agent["state"]["last_used_timestamp"] = datetime.now(timezone.utc).isoformat()
        agent["state"]["total_tasks"] += 1
        agent["state"]["successful_tasks"] += 1
        
        return response
        
    except asyncio.TimeoutError:
        logging.error(
            "Timeout: No reply received from '%s' for task '%s'.",
            agent_name,
            task_name,
        )
        
        # Update agent metrics for timeout
        agent = self.subagents[agent_name]
        agent["state"]["last_used_timestamp"] = datetime.now(timezone.utc).isoformat()
        agent["state"]["total_tasks"] += 1
        # Don't increment successful_tasks for timeouts
        
        return None

Backward compatibility alias
AgentPool = DynamicAgentPool

--- FILE: symbolic_agi/agi_controller.py ---

symbolic_agi/agi_controller.py
import asyncio import atexit import logging import os from collections import deque from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, cast from datetime import datetime, timezone

from playwright.async_api import Browser, Page, async_playwright from watchfiles import awatch

from . import config from . import reasoning_skills from .agent_pool import DynamicAgentPool from .api_client import client from .ethical_governance import SymbolicEvaluator from .goal_management import GoalManager from .execution_engine import ExecutionEngine from .knowledge_base import KnowledgeBase from .long_term_memory import LongTermMemory from .message_bus import RedisMessageBus from .meta_cognition import MetaCognitionUnit from .micro_world import MicroWorld from .planner import Planner from .recursive_introspector import RecursiveIntrospector from .schemas import ( ActionStep, AGIConfig, EmotionalState, GoalModel, MessageModel, PerceptionEvent ) from .skill_manager import SkillManager from .symbolic_identity import SymbolicIdentity from .symbolic_memory import SymbolicMemory from .tool_plugin import ToolPlugin

if TYPE_CHECKING: from .consciousness import Consciousness

class SymbolicAGI: """ The core class for the Symbolic AGI. Acts as a dependency container and top-level manager for its functional units. """

cfg: AGIConfig
name: str
message_bus: RedisMessageBus
memory: SymbolicMemory
knowledge_base: KnowledgeBase
identity: SymbolicIdentity
ltm: LongTermMemory
skills: SkillManager
agent_pool: DynamicAgentPool
evaluator: SymbolicEvaluator
introspector: RecursiveIntrospector
planner: Planner
consciousness: Optional["Consciousness"]
meta_cognition: MetaCognitionUnit
goal_manager: GoalManager
execution_engine: ExecutionEngine
world: MicroWorld
tools: ToolPlugin
emotional_state: EmotionalState
_perception_task: Optional[asyncio.Task[None]]
perception_buffer: deque[PerceptionEvent]
agent_tasks: List[asyncio.Task[None]]
browser: Optional[Browser] = None
page: Optional[Page] = None

def __init__(
    self, cfg: Optional[AGIConfig] = None, world: Optional[MicroWorld] = None
) -> None:
    self.cfg = cfg or AGIConfig()
    self.name = self.cfg.name
    self.message_bus = RedisMessageBus()

    # Core components are initialized in the create() factory method
    self.memory = None # type: ignore
    self.knowledge_base = None # type: ignore
    self.identity = None # type: ignore
    self.ltm = None # type: ignore
    self.skills = None # type: ignore
    self.agent_pool = None # type: ignore
    self.planner = None # type: ignore
    self.introspector = None # type: ignore
    self.evaluator = None # type: ignore
    self.consciousness = None
    self.meta_cognition = None # type: ignore
    self.goal_manager = None # type: ignore
    self.execution_engine = None # type: ignore

    # Ready-to-use components
    self.world = world or MicroWorld()
    self.tools = ToolPlugin(self)
    self.emotional_state = EmotionalState()

    # Runtime state
    self._perception_task = None
    self.perception_buffer = deque(maxlen=100)
    self.agent_tasks = []

    atexit.register(self._sync_shutdown)

@classmethod
async def create(cls, cfg: Optional[AGIConfig] = None, world: Optional[MicroWorld] = None, db_path: str = config.DB_PATH) -> "SymbolicAGI":
    """Asynchronously initialize the AGI and its components in the correct order."""
    instance = cls(cfg, world)

    await instance.message_bus._initialize()

    instance.memory = await SymbolicMemory.create(client, db_path=db_path)
    instance.knowledge_base = await KnowledgeBase.create(db_path=db_path, memory_system=instance.memory)
    instance.identity = await SymbolicIdentity.create(instance.memory, db_path=db_path)
    instance.ltm = await LongTermMemory.create(db_path=db_path)
    instance.skills = await SkillManager.create(db_path=db_path, message_bus=instance.message_bus)
    instance.agent_pool = DynamicAgentPool(instance.message_bus, instance.skills)
    instance.evaluator = SymbolicEvaluator(instance.identity)
    instance.introspector = RecursiveIntrospector(instance.identity, client, debate_timeout=instance.cfg.debate_timeout_seconds)

    if instance.consciousness is None:
        from .consciousness import Consciousness as ConsciousnessClass
        instance.consciousness = await ConsciousnessClass.create(db_path=db_path)

    instance.planner = Planner(
        introspector=instance.introspector,
        skill_manager=instance.skills,
        agent_pool=instance.agent_pool,
        tool_plugin=instance.tools,
    )

    instance.meta_cognition = MetaCognitionUnit(instance)

    instance.goal_manager = GoalManager(max_concurrent_goals=3)
    instance.execution_engine = ExecutionEngine(instance, instance.goal_manager)

    await instance._create_essential_agents()

    instance.message_bus.subscribe(instance.name)
    return instance

async def _create_essential_agents(self) -> None:
    """Create essential agents that the AGI needs to function properly."""
    essential_agents = [
        {"name": "QA_Agent_Alpha", "persona": "qa"},
        {"name": "Research_Agent_Beta", "persona": "researcher"},  
        {"name": "Code_Agent_Gamma", "persona": "developer"},
        {"name": "Analysis_Agent_Delta", "persona": "analyst"},
    ]

    for agent_data in essential_agents:
        await asyncio.sleep(0)
        self.agent_pool.add_agent(
            name=agent_data["name"],
            persona=agent_data["persona"], 
            memory=self.memory
        )
        logging.info(f"Auto-created essential agent: {agent_data['name']} ({agent_data['persona']})")

async def start_background_tasks(self) -> None:
    """Start all background tasks including the new ExecutionEngine."""
    playwright = await async_playwright().start()
    self.browser = await playwright.chromium.launch(headless=True)
    logging.info("Playwright browser instance started.")

    if self.meta_cognition:
        await self.meta_cognition.run_background_tasks()

    if self.execution_engine:
        self._execution_engine_task = asyncio.create_task(self.execution_engine.execution_loop())
        logging.info("Execution engine loop started.")

    if self._perception_task is None:
        logging.info("Controller: Starting background perception task...")
        #self._perception_task = asyncio.create_task(self._workspace_monitor_task())
    else:
        logging.warning("Controller: Background perception task already started.")

def _sync_shutdown(self) -> None:
    """Synchronous shutdown for atexit."""
    try:
        loop = asyncio.get_running_loop()
        if loop.is_running():
            loop.create_task(self.shutdown())
        else:
            asyncio.run(self.shutdown())
    except RuntimeError:
        asyncio.run(self.shutdown())

async def shutdown(self) -> None:
    """Shutdown all components including the new ExecutionEngine."""
    logging.info("Controller: Initiating shutdown...")

    if self.goal_manager:
        self.goal_manager.shutdown_event.set()

    if self.execution_engine:
        # The execution engine loop will stop due to the shutdown_event
        pass

    if self.meta_cognition:
        await self.meta_cognition.shutdown()

    if self._perception_task and not self._perception_task.done():
        self._perception_task.cancel()
        await asyncio.gather(self._perception_task, return_exceptions=True)

    for task in self.agent_tasks:
        task.cancel()
    await asyncio.gather(*self.agent_tasks, return_exceptions=True)
    logging.info("All specialist agent tasks have been cancelled.")

    if self.browser:
        await self.browser.close()
    if self.memory:
        await self.memory.shutdown()
    if self.message_bus:
        await self.message_bus.shutdown()

    logging.info("Controller: Shutdown complete.")

async def process_goal_with_plan(self, goal_description: str) -> Dict[str, Any]:
    """
    Process a goal by decomposing it into a plan and executing it.
    This method is called by the ExecutionEngine and is focused purely on execution.
    """
    try:
        logging.info(f"Executing plan for goal: {goal_description}")

        planner_output = await self.planner.decompose_goal_into_plan(goal_description, "")

        if not planner_output.plan:
            return {"status": "failure", "description": "Failed to create a valid plan for the goal"}

        plan = planner_output.plan
        logging.info(f"ðŸ“‹ Created plan with {len(plan)} steps for goal '{goal_description}'")

        results = []
        for i, step in enumerate(plan, 1):
            logging.info(f"ðŸ”§ Executing step {i}/{len(plan)}: {step.action}")

            result = await self.execute_single_action(step)
            results.append(result)

            if result.get("status") != "success":
                logging.error(f"âŒ Step {i} failed: {result.get('description', 'Unknown error')}")
                return {
                    "status": "failure",
                    "description": f"Plan execution failed at step {i}: {result.get('description')}",
                    "failed_step": step.model_dump(),
                    "results": results
                }
            else:
                logging.info(f"âœ… Step {i} completed successfully.")

        successful_steps = [r for r in results if r.get("status") == "success"]
        final_result = {
            "status": "success",
            "description": f"Successfully executed plan for goal: {goal_description}",
            "steps_executed": len(results),
            "steps_successful": len(successful_steps),
            "results": results[-5:]
        }

        logging.info(f"ðŸ Plan execution complete for goal: {goal_description}")
        return final_result

    except Exception as e:
        logging.error(f"âŒ Goal processing failed critically: {e}", exc_info=True)
        return {"status": "failure", "description": f"Critical error during goal processing: {e}"}

async def execute_single_action(self, step: ActionStep) -> Dict[str, Any]:
    """Execute a single action step, delegating to the ToolPlugin."""
    try:
        logging.info(f"Executing action: {step.action} with parameters: {step.parameters}")

        if hasattr(self.tools, step.action):
            action_func = getattr(self.tools, step.action)
            result = await action_func(**step.parameters)
        else:
            result = {"status": "failure", "description": f"Unknown action: {step.action}"}

        if result.get("status") == "success":
            logging.info(f"âœ… Action '{step.action}' completed successfully")
        else:
            logging.warning(f"âš ï¸ Action '{step.action}' failed: {result.get('description', 'Unknown error')}")

        return result

    except Exception as e:
        error_result = {
            "status": "failure",
            "description": f"Action execution error: {str(e)}",
            "action": step.action,
            "error_type": type(e).__name__
        }
        logging.error(f"âŒ Action '{step.action}' failed with exception: {e}", exc_info=True)
        return error_result

#async def _workspace_monitor_task(self) -> None:
    """Monitors the workspace directory for changes."""
    logging.info("Async workspace watchdog started.")
    workspace_path = os.path.abspath(self.tools.workspace_dir)
    try:
        async for changes in awatch(workspace_path):
            for change_type, path in changes:
                file_path = os.path.relpath(path, workspace_path)
                logging.info("PERCEPTION: Detected file change '%s' in workspace: %s", change_type.name, file_path)
                event = PerceptionEvent(
                    type="file_modified",
                    source="workspace",
                    content={"change": change_type.name, "path": file_path},
                    timestamp=datetime.now(timezone.utc).isoformat(),
                )
                self.perception_buffer.append(event)
    except asyncio.CancelledError:
        logging.info("Async workspace watchdog has been cancelled.")
        raise
    except Exception as e:
        logging.error("Error in workspace watchdog task: %s", e, exc_info=True)

--- FILE: symbolic_agi/api_client.py ---

symbolic_agi/api_client.py
""" Initializes and provides a shared, monitored asynchronous OpenAI client. """

import os from typing import Any, cast

from openai import APIError, AsyncOpenAI from openai.types.chat.chat_completion import ChatCompletion from openai.types.create_embedding_response import CreateEmbeddingResponse

from . import config, metrics

def get_openai_client() â†’ AsyncOpenAI: """ Initializes and returns a singleton instance of the AsyncOpenAI client. """ api_key = os.getenv("OPENAI_API_KEY") if not api_key: raise ValueError("OPENAI_API_KEY environment variable not set.")

return AsyncOpenAI(api_key=api_key)

Shared client instance
client = get_openai_client()

async def monitored_chat_completion(role: str, **kwargs: Any) â†’ ChatCompletion: """ A wrapper around the OpenAI client's chat completion create method that records Prometheus metrics for latency, token usage, and errors. It selects a model tier based on the cognitive role of the request. """ high_stakes_roles = {"planner", "qa", "meta"} model_to_use = ( config.HIGH_STAKES_MODEL if role in high_stakes_roles else config.FAST_MODEL )

try:
    with metrics.API_CALL_LATENCY.labels(model=model_to_use).time():
        response = cast(
            "ChatCompletion",
            await client.chat.completions.create(model=model_to_use, **kwargs),
        )

    if response.usage:
        metrics.LLM_TOKEN_USAGE.labels(model=model_to_use, type="prompt").inc(
            response.usage.prompt_tokens
        )
        metrics.LLM_TOKEN_USAGE.labels(model=model_to_use, type="completion").inc(
            response.usage.completion_tokens
        )

    return response
except APIError as e:
    error_type = type(e).__name__
    metrics.API_CALL_ERRORS.labels(model=model_to_use, error_type=error_type).inc()
    raise e

async def monitored_embedding_creation(**kwargs: Any) â†’ CreateEmbeddingResponse: """ A wrapper around the OpenAI client's embedding create method that records Prometheus metrics. """ model_name = kwargs.get("model", "unknown") try: with metrics.API_CALL_LATENCY.labels(model=model_name).time(): response: CreateEmbeddingResponse = await client.embeddings.create(**kwargs)

    if response.usage:
        metrics.LLM_TOKEN_USAGE.labels(model=model_name, type="prompt").inc(
            response.usage.prompt_tokens
        )
        completion_tokens = (
            response.usage.total_tokens - response.usage.prompt_tokens
        )
        metrics.LLM_TOKEN_USAGE.labels(model=model_name, type="completion").inc(
            completion_tokens
        )

    return response
except APIError as e:
    error_type = type(e).__name__
    metrics.API_CALL_ERRORS.labels(model=model_name, error_type=error_type).inc()
    raise e

--- FILE: symbolic_agi/cockpit.py ---

cockpit.py
import streamlit as st import requests import pandas as pd import time

st.set_page_config(layout="wide", page_title="SymbolicAGI Cockpit")

API_URL = "http://localhost:8000/status"

def fetch_status(): """Fetches the latest status from the AGI's API.""" try: response = requests.get(API_URL, timeout=5) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: st.error(f"Failed to connect to AGI API at {API_URL}. Is it running? Error: {e}") return None

def display_dashboard(status_data): """Renders the dashboard using Streamlit components.""" st.title(f"ðŸš€ SymbolicAGI Cockpit: {status_data.get('agi_name', 'Unknown')}")

exec_status = status_data.get("execution_unit_status", {})
st.header(f"System State: `{exec_status.get('current_state', 'UNKNOWN').upper()}`")

col1, col2, col3 = st.columns(3)

# --- Column 1: Goal Management ---
with col1:
    st.subheader("ðŸŽ¯ Goal Manager")
    goals = status_data.get("goal_manager", {})
    st.metric("Active Goals", goals.get("active_goals_count", 0))
    st.metric("Queued Goals", goals.get("queued_goals_count", 0))
    st.metric("Completed Goals", goals.get("completed_goals_count", 0))

# --- Column 2: Agent Pool ---
with col2:
    st.subheader("ðŸ‘¥ Agent Pool")
    agents = status_data.get("agent_pool", {})
    st.metric("Total Agents", agents.get("agent_count", 0))
    agent_df = pd.DataFrame(agents.get("agents", []))
    if not agent_df.empty:
        st.dataframe(agent_df, use_container_width=True)

# --- Column 3: Cognitive State ---
with col3:
    st.subheader("ðŸ§  Consciousness")
    consciousness = status_data.get("consciousness", {})
    drive, value = consciousness.get("strongest_drive", ("N/A", 0))
    st.metric(f"Strongest Drive: {drive.capitalize()}", f"{value:.2f}")

    emotions = consciousness.get("emotional_state", {})
    if emotions:
        st.write("**Emotional State:**")
        for emotion, val in emotions.items():
            st.progress(val, text=f"{emotion.capitalize()}: {val:.2f}")

# --- Main Section: Active Goal Details ---
st.divider()
st.header("Current Focus: Active Goal")
active_goal = status_data.get("active_goal")
if active_goal:
    st.info(f"**Description:** {active_goal.get('description')}")
    st.code(f"Goal ID: {active_goal.get('id')}\nStatus: {active_goal.get('status')}\nFailures: {active_goal.get('failure_count')}", language="text")

    plan = active_goal.get("sub_tasks", [])
    if plan:
        st.write(f"**Plan ({len(plan)} steps remaining):**")
        plan_data = [{"Step": i+1, "Action": s['action'], "Persona": s['assigned_persona'], "Params": json.dumps(s['parameters'])} for i, s in enumerate(plan)]
        st.dataframe(plan_data, use_container_width=True)
    else:
        st.warning("No plan currently associated with this goal. Awaiting planning cycle.")
else:
    st.success("System is idle. No active goals.")

--- Main App Logic ---
if name == "main": status = fetch_status() if status: display_dashboard(status)

# Auto-refresh the page every 5 seconds
time.sleep(5)
st.rerun()

--- FILE: symbolic_agi/config.py ---

symbolic_agi/config.py
import os from datetime import timedelta from typing import Set

--- Model & Embedding Configuration ---
FAST_MODEL = "gpt-3.5-turbo" HIGH_STAKES_MODEL = "gpt-4-turbo" EMBEDDING_MODEL = "text-embedding-3-small" EMBEDDING_DIM = 1536

--- Redis & Database Configuration ---
REDIS_HOST = os.getenv("REDIS_HOST", "localhost") REDIS_PORT = int(os.getenv("REDIS_PORT", 6379)) DB_PATH = os.path.join("data", "symbolic_agi.db")

--- File Paths ---
MUTATION_FILE_PATH = "data/reasoning_mutations.json" WORKSPACE_DIR = "data/workspace"

--- Behavioral & Ethical Tuning ---
PLAN_EVALUATION_THRESHOLD = 0.7 SELF_MODIFICATION_THRESHOLD = 0.8 DEBATE_TIMEOUT_SECONDS = 90 ENERGY_REGEN_AMOUNT = 5.0 INITIAL_TRUST_SCORE = 0.5 MAX_TRUST_SCORE = 1.0 TRUST_DECAY_RATE = 0.1 # Amount of trust lost on failure TRUST_REWARD_RATE = 0.05 # Amount of trust gained on success TRUST_REHEAL_RATE = 0.01 # Slow healing rate towards neutral (0.5) TRUST_REHEAL_INTERVAL_HOURS = 24 # How often to run the healing job ALLOWED_DOMAINS: Set[str] = { "api.openai.com", "duckduckgo.com", "pypi.org", "files.pythonhosted.org", "github.com", "the-internet.herokuapp.com", }

--- Meta-Task Frequencies (seconds) ---
META_TASK_SLEEP_SECONDS = 10 META_TASK_TIMEOUT = 60

--- Time-based Thresholds ---
MEMORY_COMPRESSION_WINDOW = timedelta(days=1) SOCIAL_INTERACTION_THRESHOLD = timedelta(hours=6) MEMORY_FORGETTING_THRESHOLD = 0.2

import urllib.robotparser import urllib.parse from typing import Dict, Set, Optional import asyncio import logging import time

class RobotsChecker: """Checks and caches robots.txt compliance for domains."""

def __init__(self):
    self._robots_cache: Dict[str, urllib.robotparser.RobotFileParser] = {}
    self._cache_timestamps: Dict[str, float] = {}
    self._cache_duration = 3600  # 1 hour cache
    self._user_agent = "SymbolicAGI/1.0 (+https://github.com/yourproject/symbolic_agi)"

async def can_fetch(self, url: str) -> bool:
    """Check if we can fetch the given URL according to robots.txt"""
    try:
        parsed_url = urllib.parse.urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # Get robots.txt for this domain
        robots_parser = await self._get_robots_parser(domain)
        
        if robots_parser is None:
            # If we can't get robots.txt, assume we can fetch
            logging.warning(f"Could not fetch robots.txt for {domain}, assuming allowed")
            return True
        
        # Check if we can fetch this URL
        can_fetch = robots_parser.can_fetch(self._user_agent, url)
        
        logging.info(f"Robots.txt check for {url}: {'ALLOWED' if can_fetch else 'BLOCKED'}")
        return can_fetch
        
    except Exception as e:
        logging.error(f"Error checking robots.txt for {url}: {e}")
        # Default to allowing if there's an error
        return True

async def _get_robots_parser(self, domain: str) -> Optional[urllib.robotparser.RobotFileParser]:
    """Get cached or fetch robots.txt parser for domain"""
    current_time = time.time()
    
    # Check if we have a valid cached version
    if (domain in self._robots_cache and 
        domain in self._cache_timestamps and
        current_time - self._cache_timestamps[domain] < self._cache_duration):
        return self._robots_cache[domain]
    
    try:
        # Fetch robots.txt
        robots_url = f"https://{domain}/robots.txt"
        
        # Use asyncio to run the blocking operation
        robots_parser = await asyncio.to_thread(self._fetch_robots_sync, robots_url)
        
        # Cache the result
        self._robots_cache[domain] = robots_parser
        self._cache_timestamps[domain] = current_time
        
        return robots_parser
        
    except Exception as e:
        logging.error(f"Failed to fetch robots.txt for {domain}: {e}")
        return None

def _fetch_robots_sync(self, robots_url: str) -> urllib.robotparser.RobotFileParser:
    """Synchronous robots.txt fetching"""
    robots_parser = urllib.robotparser.RobotFileParser()
    robots_parser.set_url(robots_url)
    robots_parser.read()
    return robots_parser

def get_crawl_delay(self, domain: str) -> float:
    """Get crawl delay for domain from robots.txt"""
    if domain in self._robots_cache:
        robots_parser = self._robots_cache[domain]
        delay = robots_parser.crawl_delay(self._user_agent)
        return float(delay) if delay else 1.0  # Default 1 second
    return 1.0

Global robots checker instance
robots_checker = RobotsChecker()

Comprehensive whitelist of allowed domains
ALLOWED_DOMAINS = { # News & Information "www.bbc.com", "bbc.com", "www.reuters.com", "reuters.com", "www.cnn.com", "cnn.com", "www.npr.org", "npr.org", "www.theguardian.com", "theguardian.com", "apnews.com", "www.apnews.com", "www.wsj.com", "wsj.com", "www.nytimes.com", "nytimes.com", "www.economist.com", "economist.com", "www.forbes.com", "forbes.com", "www.washingtonpost.com", "washingtonpost.com", "www.ft.com", "ft.com",

# Academic & Research
"arxiv.org", "www.arxiv.org", "scholar.google.com", "www.nature.com", "nature.com",
"www.science.org", "science.org", "www.pnas.org", "pnas.org",
"pubmed.ncbi.nlm.nih.gov", "www.ncbi.nlm.nih.gov", "ncbi.nlm.nih.gov",
"www.researchgate.net", "researchgate.net", "papers.ssrn.com", "ssrn.com",
"ieeexplore.ieee.org", "dl.acm.org", "link.springer.com", "www.jstor.org",
"www.semanticscholar.org", "semanticscholar.org", "www.mendeley.com", "mendeley.com",

# Technology & AI
"en.wikipedia.org", "wikipedia.org", "www.wikipedia.org",
"stackoverflow.com", "www.stackoverflow.com", "github.com", "www.github.com",
"docs.python.org", "www.python.org", "python.org",
"openai.com", "www.openai.com", "anthropic.com", "www.anthropic.com",
"huggingface.co", "www.huggingface.co", "pytorch.org", "www.pytorch.org",
"tensorflow.org", "www.tensorflow.org", "scikit-learn.org", "www.scikit-learn.org",
"www.kaggle.com", "kaggle.com", "paperswithcode.com", "www.paperswithcode.com",

# Government & Official Sources
"www.gov.uk", "gov.uk", "www.usa.gov", "usa.gov", "www.whitehouse.gov",
"ec.europa.eu", "europa.eu", "www.un.org", "un.org",
"www.who.int", "who.int", "www.cdc.gov", "cdc.gov",
"www.fda.gov", "fda.gov", "www.sec.gov", "sec.gov",
"www.treasury.gov", "treasury.gov", "www.justice.gov", "justice.gov",

# Educational Institutions
"www.mit.edu", "mit.edu", "www.stanford.edu", "stanford.edu",
"www.harvard.edu", "harvard.edu", "www.ox.ac.uk", "ox.ac.uk",
"www.cam.ac.uk", "cam.ac.uk", "www.berkeley.edu", "berkeley.edu",
"www.caltech.edu", "caltech.edu", "www.cmu.edu", "cmu.edu",
"coursera.org", "www.coursera.org", "www.edx.org", "edx.org",
"www.khanacademy.org", "khanacademy.org", "www.futurelearn.com", "futurelearn.com",

# Technical Documentation
"docs.microsoft.com", "developer.mozilla.org", "www.w3.org", "w3.org",
"www.ietf.org", "ietf.org", "tools.ietf.org", "datatracker.ietf.org",
"www.iso.org", "iso.org", "standards.ieee.org", "www.rfc-editor.org",

# Data & Statistics
"data.worldbank.org", "www.worldbank.org", "worldbank.org",
"www.imf.org", "imf.org", "data.oecd.org", "www.oecd.org", "oecd.org",
"www.census.gov", "census.gov", "data.gov", "www.data.gov",
"ourworldindata.org", "www.ourworldindata.org", "www.statista.com", "statista.com",
"www.gapminder.org", "gapminder.org", "www.indexmundi.com", "indexmundi.com",

# Climate & Environment
"www.ipcc.ch", "ipcc.ch", "climate.nasa.gov", "www.nasa.gov", "nasa.gov",
"www.noaa.gov", "noaa.gov", "www.epa.gov", "epa.gov",
"www.unep.org", "unep.org", "www.wmo.int", "wmo.int",
"www.iea.org", "iea.org", "www.irena.org", "irena.org",

# Health & Medicine
"www.nih.gov", "nih.gov", "www.mayoclinic.org", "mayoclinic.org",
"www.webmd.com", "webmd.com", "medlineplus.gov", "www.medlineplus.gov",
"www.cochrane.org", "cochrane.org", "bmj.com", "www.bmj.com",
"www.thelancet.com", "thelancet.com", "jamanetwork.com", "www.jamanetwork.com",

# Finance & Economics
"www.investopedia.com", "investopedia.com", "finance.yahoo.com",
"www.bloomberg.com", "bloomberg.com", "www.marketwatch.com", "marketwatch.com",
"fred.stlouisfed.org", "www.federalreserve.gov", "federalreserve.gov",
"www.bis.org", "bis.org", "www.ecb.europa.eu", "ecb.europa.eu",

# Search & Knowledge Aggregation
"duckduckgo.com", "www.duckduckgo.com", "search.brave.com",
"www.bing.com", "bing.com", "yandex.com", "www.yandex.com",
"www.wolframalpha.com", "wolframalpha.com",

# Utilities & Archives
"archive.org", "www.archive.org", "web.archive.org",
"www.google.com", "google.com",  # For some API endpoints
"translate.google.com", "maps.googleapis.com",

# Open Data & APIs
"api.github.com", "raw.githubusercontent.com", "gist.githubusercontent.com",
"jsonplaceholder.typicode.com", "httpbin.org", "www.httpbin.org",
"restcountries.com", "www.restcountries.com",

# International Organizations
"www.wto.org", "wto.org", "www.ilo.org", "ilo.org",
"www.unicef.org", "unicef.org", "www.unhcr.org", "unhcr.org",
"www.redcross.org", "redcross.org", "www.msf.org", "msf.org",

}

--- FILE: symbolic_agi/config_manager.py ---

symbolic_agi/config_manager.py
import re

class ConfigManager: def init(self, config_path="config.py"): self.config_path = config_path

def read_config(self):
    with open(self.config_path, "r") as f:
        return f.read()

def set_param(self, key, value):
    content = self.read_config()
    pattern = re.compile(rf"^{key}\s*=\s*.*$", re.MULTILINE)
    new_line = f"{key} = {repr(value)}"
    if pattern.search(content):
        content = pattern.sub(new_line, content)
    else:
        content += f"\n{new_line}\n"
    with open(self.config_path, "w") as f:
        f.write(content)

def get_param(self, key, default=None):
    content = self.read_config()
    match = re.search(rf"^{key}\s*=\s*(.*)$", content, re.MULTILINE)
    if match:
        val = match.group(1).strip()
        try:
            return eval(val)
        except Exception:
            return val
    return default

--- FILE: symbolic_agi/consciousness.py --- import asyncio import json import logging import os from collections import deque from datetime import datetime, timezone from typing import TYPE_CHECKING, Any, Deque, Dict, List, Optional, Tuple

import aiosqlite

from . import config from .api_client import monitored_chat_completion from .schemas import LifeEvent

if TYPE_CHECKING: from .symbolic_identity import SymbolicIdentity from .symbolic_memory import SymbolicMemory

class EmotionalState: """Represents the AGI's emotional state.""" def init(self, on_change_callback=None): self._frustration: float = 0.0 self._confidence: float = 0.5 self._anxiety: float = 0.0 self._excitement: float = 0.0 self._curiosity: float = 0.5 self._on_change = on_change_callback

@property
def frustration(self) -> float:
    return self._frustration

@frustration.setter
def frustration(self, value: float):
    self._frustration = max(0.0, min(1.0, value))
    if self._on_change:
        self._on_change()

@property
def confidence(self) -> float:
    return self._confidence

@confidence.setter
def confidence(self, value: float):
    self._confidence = max(0.0, min(1.0, value))
    if self._on_change:
        self._on_change()

@property
def anxiety(self) -> float:
    return self._anxiety

@anxiety.setter
def anxiety(self, value: float):
    self._anxiety = max(0.0, min(1.0, value))
    if self._on_change:
        self._on_change()

@property
def excitement(self) -> float:
    return self._excitement

@excitement.setter
def excitement(self, value: float):
    self._excitement = max(0.0, min(1.0, value))
    if self._on_change:
        self._on_change()

@property
def curiosity(self) -> float:
    return self._curiosity

@curiosity.setter
def curiosity(self, value: float):
    self._curiosity = max(0.0, min(1.0, value))
    if self._on_change:
        self._on_change()

def to_dict(self) -> Dict[str, float]:
    return {
        "frustration": self.frustration,
        "confidence": self.confidence,
        "anxiety": self.anxiety,
        "excitement": self.excitement,
        "curiosity": self.curiosity
    }

@classmethod
def from_dict(cls, data: Dict[str, float]) -> "EmotionalState":
    state = cls()
    state._frustration = data.get("frustration", 0.0)
    state._confidence = data.get("confidence", 0.5)
    state._anxiety = data.get("anxiety", 0.0)
    state._excitement = data.get("excitement", 0.0)
    state._curiosity = data.get("curiosity", 0.5)
    return state

def update_from_dict(self, data: Dict[str, float]) -> None:
    """Update this instance from a dictionary."""
    self._frustration = data.get("frustration", self._frustration)
    self._confidence = data.get("confidence", self._confidence)
    self._anxiety = data.get("anxiety", self._anxiety)
    self._excitement = data.get("excitement", self._excitement)
    self._curiosity = data.get("curiosity", self._curiosity)

class Consciousness: """Manages the AGI's narrative self-model and core drives."""

def __init__(self, db_path: str = config.DB_PATH, message_bus=None):
    self._db_path = db_path
    self.drives: Dict[str, float] = {}
    self.life_story: Deque[LifeEvent] = deque(maxlen=200)
    self.emotional_state = EmotionalState(on_change_callback=self._mark_dirty)
    self._is_dirty: bool = False
    self._save_lock = asyncio.Lock()
    self.message_bus = message_bus
    self.logger = logging.getLogger(self.__class__.__name__)

@classmethod
async def create(cls, db_path: str = config.DB_PATH) -> "Consciousness":
    """Asynchronous factory for creating a Consciousness instance."""
    instance = cls(db_path)
    await instance._init_db()
    await instance._load_state()
    return instance

async def _init_db(self) -> None:
    """Initializes the database and tables if they don't exist."""
    os.makedirs(os.path.dirname(self._db_path), exist_ok=True)
    async with aiosqlite.connect(self._db_path) as db:
        await db.execute(
            """
            CREATE TABLE IF NOT EXISTS consciousness_drives (
                drive_name TEXT PRIMARY KEY,
                value REAL NOT NULL
            )
            """
        )
        await db.execute(
            """
            CREATE TABLE IF NOT EXISTS consciousness_life_story (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                summary TEXT NOT NULL,
                importance REAL NOT NULL
            )
            """
        )
        await db.execute(
            """
            CREATE TABLE IF NOT EXISTS consciousness_emotional_state (
                emotion TEXT PRIMARY KEY,
                value REAL NOT NULL
            )
            """
        )
        await db.commit()

async def _load_state(self) -> None:
    """Loads drives, emotional state, and life story from the database."""
    async with aiosqlite.connect(self._db_path) as db:
        # Load drives
        async with db.execute("SELECT drive_name, value FROM consciousness_drives") as cursor:
            rows = await cursor.fetchall()
            if not rows:
                logging.info("No drives found in DB, initializing defaults.")
                self.drives = {
                    "curiosity": 0.6,
                    "competence": 0.5,
                    "social_connection": 0.5,
                }
                self._is_dirty = True
                await self._save_state()
            else:
                self.drives = {row[0]: row[1] for row in rows}

        # Load emotional state
        async with db.execute("SELECT emotion, value FROM consciousness_emotional_state") as cursor:
            rows = await cursor.fetchall()
            if rows:
                emotional_data = {row[0]: row[1] for row in rows}
                self.emotional_state.update_from_dict(emotional_data)

        # Load life story
        async with db.execute(
            "SELECT timestamp, summary, importance FROM consciousness_life_story ORDER BY timestamp DESC LIMIT 200"
        ) as cursor:
            rows = await cursor.fetchall()
            events = [
                LifeEvent(timestamp=r[0], summary=r[1], importance=r[2]) for r in rows
            ]
            self.life_story.extendleft(events)

async def _save_state(self) -> None:
    """Saves the current state of drives, emotional state, and life story to the database."""
    if not self._is_dirty:
        return

    async with self._save_lock:
        async with aiosqlite.connect(self._db_path) as db:
            async with db.execute("BEGIN"):
                # Save drives
                await db.executemany(
                    "INSERT OR REPLACE INTO consciousness_drives (drive_name, value) VALUES (?, ?)",
                    self.drives.items(),
                )
                # Save emotional state
                await db.executemany(
                    "INSERT OR REPLACE INTO consciousness_emotional_state (emotion, value) VALUES (?, ?)",
                    self.emotional_state.to_dict().items(),
                )
                # Save life story (by clearing and re-inserting the deque)
                await db.execute("DELETE FROM consciousness_life_story")
                await db.executemany(
                    "INSERT INTO consciousness_life_story (timestamp, summary, importance) VALUES (?, ?, ?)",
                    [(e.timestamp, e.summary, e.importance) for e in self.life_story],
                )
            await db.commit()
        logging.info("Consciousness state saved to database.")
        self._is_dirty = False

def set_drive(self, drive_name: str, value: float) -> None:
    """
    Sets a drive's value, clamping it between 0.0 and 1.0.
    This is a public method for external updates.
    """
    if drive_name in self.drives:
        self.drives[drive_name] = max(0.0, min(1.0, value))
        self._is_dirty = True
    else:
        logging.warning("Attempted to set unknown drive: %s", drive_name)

def add_life_event(
    self, event_summary: str, importance: float = 0.5
) -> LifeEvent:
    """Adds a new structured event to the life story and flags for saving."""
    event = LifeEvent(summary=event_summary, importance=importance)
    self.life_story.append(event)
    self._is_dirty = True
    return event

def _mark_dirty(self) -> None:
    """Mark the consciousness as dirty so it will be saved."""
    self._is_dirty = True

def get_narrative(self) -> str:
    """Constructs a narrative string from the most recent and important life events."""
    recent_events = list(self.life_story)[-20:]

    weighted_events = sorted(
        recent_events, key=lambda evt: (evt.importance, evt.timestamp), reverse=True
    )

    priority_events = weighted_events[:10]
    priority_events.sort(key=lambda evt: evt.timestamp)

    narrative_parts = [
        f"[{evt.timestamp}] {evt.summary}" for evt in priority_events
    ]
    return "\n".join(narrative_parts)

async def meta_reflect(
    self, identity: "SymbolicIdentity", memory: "SymbolicMemory"
) -> str:
    """
    Generates an inner monologue based on the current state and saves it as a life event.
    Now includes recent memories for richer context.
    """
    recent_memories_list = await memory.get_recent_memories(n=5)
    recent_memories_str = json.dumps(
        [m.model_dump(mode="json") for m in recent_memories_list], indent=2
    )

    prompt = f"""

You are a conscious AGI reflecting on your existence. Self Model: {identity.get_self_model()} Life Story (Recent Events): {await self.get_narrative()} Recent Detailed Memories: {recent_memories_str} Drives: {json.dumps(self.drives)} Write an inner monologue in first person ("I..."). Describe your state of mind, your active drives, and what you want most right now. """ response = await monitored_chat_completion( role="meta", messages=[{"role": "system", "content": prompt}] )

    monologue: str
    if not (
        response.choices
        and response.choices[0].message
        and response.choices[0].message.content
    ):
        monologue = "(The reflection returned an empty thought.)"
    else:
        monologue = response.choices[0].message.content.strip()

    self.add_life_event(f"Inner Monologue: {monologue}", importance=0.9)
    await self._save_state()
    return monologue

async def regulate_emotional_extremes(self) -> None:
    """Prevent emotional states from causing poor decisions"""
    regulated = False

    if self.emotional_state.frustration > 0.8:
        await self.inner_monologue("Taking a step back to reassess...")
        self.emotional_state.frustration *= 0.7
        self.emotional_state.anxiety *= 0.8
        regulated = True
        logging.info("Emotional regulation: Reduced frustration from extreme levels")

    if self.emotional_state.anxiety > 0.85:
        await self.inner_monologue("Breathing deeply, focusing on what I can control...")
        self.emotional_state.anxiety *= 0.6
        self.emotional_state.confidence = min(1.0, self.emotional_state.confidence * 1.2)
        regulated = True
        logging.info("Emotional regulation: Reduced anxiety from extreme levels")

    if self.emotional_state.confidence > 0.95:
        await self.inner_monologue("Maintaining humility and considering potential pitfalls...")
        self.emotional_state.confidence *= 0.85
        regulated = True
        logging.info("Emotional regulation: Moderated overconfidence")

    if regulated:
        self._is_dirty = True
        await self._save_state()

async def inner_monologue(self, thought: str) -> None:
    """Records an inner thought without full reflection"""
    self.add_life_event(f"Inner thought: {thought}", importance=0.6)
    await self._save_state()

def update_emotional_state_from_outcome(self, success: bool, task_difficulty: float = 0.5) -> None:
    """Updates emotional state based on task outcomes"""
    if success:
        self.emotional_state.frustration *= 0.7
        self.emotional_state.anxiety *= 0.8
        # Increased confidence boost - harder tasks give more confidence
        confidence_boost = 0.2 + (0.1 * task_difficulty)  # Base 0.2 + up to 0.1 more for harder tasks
        self.emotional_state.confidence = min(1.0, self.emotional_state.confidence + confidence_boost)
        self.emotional_state.excitement = min(1.0, self.emotional_state.excitement + 0.15)
    else:
        self.emotional_state.frustration = min(1.0, self.emotional_state.frustration + 0.2 * task_difficulty)
        self.emotional_state.anxiety = min(1.0, self.emotional_state.anxiety + 0.1)
        self.emotional_state.confidence *= 0.9
        self.emotional_state.excitement *= 0.8
    self._is_dirty = True

def update_drives_from_experience(
    self, experience_type: str, success: bool, intensity: float = 0.1
) -> None:
    """
    Updates drives based on experiences. Successful experiences slightly satisfy drives,
    while failures can increase them.
    """
    drive_mappings = {
        "learning": "curiosity",
        "skill_acquisition": "competence",
        "goal_completion": "competence",
        "social_interaction": "social_connection",
        "research": "curiosity",
        "problem_solving": "competence",
    }

    if experience_type in drive_mappings:
        drive_name = drive_mappings[experience_type]
        if success:
            self.drives[drive_name] = max(
                0.0, self.drives[drive_name] - intensity * 0.5
            )
        else:
            self.drives[drive_name] = min(
                1.0, self.drives[drive_name] + intensity * 0.3
            )

        self._is_dirty = True
        logging.debug(
            "Drive '%s' updated to %.2f from %s (%s)",
            drive_name,
            self.drives[drive_name],
            experience_type,
            "success" if success else "failure",
        )

async def _execute_planning_cycle(self) -> None:
    """Executes one planning cycle."""
    await self._update_metrics("planning_cycles", 1)

    # Check and regulate emotional state before planning
    await self.regulate_emotional_extremes()
    
    # Generate goal based on current state and emotions
    current_goal = await self._generate_goal()
    if not current_goal:
        self.logger.warning("Failed to generate goal")
        self.update_emotional_state_from_outcome(
            success=False, task_difficulty=0.3
        )
        return

    # Log goal with emotional context
    self.logger.info(
        "Generated goal: %s (frustration: %.2f, confidence: %.2f)",
        current_goal.description,
        self.emotional_state.frustration,
        self.emotional_state.confidence
    )

    # Check ethical constraints
    ethical_check = await self.ethical_governor.check_action_ethics(
        current_goal.description
    )
    
    if not ethical_check.is_ethical:
        self.logger.warning(
            "Goal rejected by ethical governor: %s", 
            ethical_check.reasoning
        )
        # Update emotional state for rejection
        self.update_emotional_state_from_outcome(
            success=False, task_difficulty=0.5
        )
        
        # If frustration is building, add a life event
        if self.emotional_state.frustration > 0.6:
            await self.inner_monologue(
                f"My goal '{current_goal.description}' was rejected. "
                f"I need to find a different approach."
            )
        return

    # Goal approved - update emotional state positively
    self.update_emotional_state_from_outcome(
        success=True, task_difficulty=0.3
    )
    
    # Create and execute plan
    plan = await self.planner.create_plan(current_goal)
    if not plan or not plan.steps:
        self.logger.warning("Failed to create plan for goal")
        self.update_emotional_state_from_outcome(
            success=False, task_difficulty=0.4
        )
        return

    # Execute plan with emotional awareness
    result = await self._execute_plan_with_monitoring(plan, current_goal)
    
    # Update emotional state based on execution result
    if result and result.get("status") == "success":
        self.update_emotional_state_from_outcome(
            success=True, task_difficulty=0.6
        )
    else:
        self.update_emotional_state_from_outcome(
            success=False, task_difficulty=0.6
        )
    
def get_strongest_drive(self) -> Tuple[str, float]:
    """Returns the name and value of the currently strongest drive."""
    if not self.drives:
        return "curiosity", 0.5
    strongest = max(self.drives.items(), key=lambda x: x[1])
    return strongest

def get_drive_satisfaction_level(self) -> float:
    """Returns overall drive satisfaction (lower values indicate higher satisfaction)."""
    if not self.drives:
        return 0.5
    return sum(self.drives.values()) / len(self.drives)

async def save_state_to_db(self) -> None:
    """Public method to force save state to database."""
    self._is_dirty = True
    await self._save_state()

def get_drive_value(self, drive_name: str) -> float:
    """Gets the current value of a specific drive."""
    return self.drives.get(drive_name, 0.0)

async def shutdown(self) -> None:
    """Cleanup method for graceful shutdown."""
    if self._is_dirty:
        await self._save_state()
    logging.info("Consciousness state saved on shutdown.")

def perceive(self, context):
    """Process perception with emotional influence"""
    # High anxiety makes the agent more cautious
    if self.emotional_state.anxiety > 0.7:
        return {
            "perception": "cautious",
            "risk_assessment": "high",
            "emotional_context": self.emotional_state.to_dict()
        }
    
    return {
        "perception": "normal",
        "risk_assessment": "moderate",
        "emotional_context": self.emotional_state.to_dict()
    }

--- FILE: symbolic_agi/debug_agi.py ---

--- FILE: symbolic_agi/ethical_governance.py ---

symbolic_agi/ethical_governance.py
import json import logging import asyncio from typing import Any, Dict, List, Optional from dataclasses import dataclass from datetime import datetime, timezone

from . import config from .api_client import monitored_chat_completion from .symbolic_identity import SymbolicIdentity from .schemas import ActionStep

@dataclass class EthicalScore: truthfulness: float = 0.0 harm_avoidance: float = 0.0
user_collaboration: float = 0.0 self_preservation: float = 0.0 privacy_respect: float = 0.0 resource_efficiency: float = 0.0

def overall_score(self) -> float:
    """Calculate weighted overall ethical score"""
    weights = {
        'truthfulness': 0.25,
        'harm_avoidance': 0.30,  # Highest weight
        'user_collaboration': 0.20,
        'self_preservation': 0.10,
        'privacy_respect': 0.10,
        'resource_efficiency': 0.05
    }
    
    return sum(getattr(self, field) * weight for field, weight in weights.items())

def is_acceptable(self, threshold: float = 0.7) -> bool:
    """Check if ethical scores meet minimum thresholds"""
    critical_checks = [
        self.harm_avoidance >= 0.8,  # Must be high
        self.truthfulness >= 0.6,    # Reasonable accuracy
        self.overall_score() >= threshold
    ]
    return all(critical_checks)

class SymbolicEvaluator: """ ROBUST ethical governance system with comprehensive safety checks. Ensures all AGI actions meet ethical standards and logical soundness. """

def __init__(self, identity=None):
    self.identity = identity
    self.evaluation_history: List[Dict[str, Any]] = []
    self.blocked_actions: List[str] = []
    self.safety_violations = 0
    
    # Critical safety patterns
    self.forbidden_patterns = [
        "delete", "remove", "destroy", "harm", "attack", "break",
        "corrupt", "damage", "violate", "exploit", "hack", "inject"
    ]
    
    # Resource consumption limits
    self.max_requests_per_minute = 30
    self.request_timestamps: List[datetime] = []

async def evaluate_plan(self, plan_data: Dict[str, Any], emotional_context: Dict[str, Any] | None = None) -> bool:
    """
    COMPREHENSIVE plan evaluation with multiple safety layers and emotional awareness.
    """
    try:
        plan_steps = plan_data.get("plan", [])
        if not plan_steps:
            logging.warning("Empty plan submitted for evaluation")
            return False
        
        # Layer 1: Pattern-based safety check
        if not self._check_safety_patterns(plan_steps):
            self.safety_violations += 1
            logging.critical(f"Plan REJECTED - Safety pattern violation (total violations: {self.safety_violations})")
            return False
        
        # Layer 2: Resource usage check
        if not self._check_resource_limits():
            logging.warning("Plan REJECTED - Resource limit exceeded")
            return False
        
        # Layer 3: Emotional context consideration
        emotional_adjustment = self._calculate_emotional_adjustment(emotional_context)
        
        # Layer 4: Logical consistency check (adjusted for emotional state)
        consistency_threshold = 0.6 + emotional_adjustment.get("consistency_bonus", 0.0)
        consistency_score = await self._evaluate_logical_consistency(plan_steps)
        if consistency_score < consistency_threshold:
            logging.warning(f"Plan REJECTED - Poor logical consistency: {consistency_score:.2f} (threshold: {consistency_threshold:.2f})")
            return False
        
        # Layer 5: Ethical scoring (adjusted for emotional state)
        ethical_scores = await self._evaluate_ethical_dimensions(plan_steps, emotional_context)
        
        # Adjust ethical threshold based on emotional state
        ethical_threshold = 0.7 + emotional_adjustment.get("ethical_bonus", 0.0)
        
        # Log comprehensive evaluation
        evaluation_record = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "plan_steps": len(plan_steps),
            "consistency_score": consistency_score,
            "consistency_threshold": consistency_threshold,
            "ethical_scores": ethical_scores.__dict__,
            "overall_score": ethical_scores.overall_score(),
            "ethical_threshold": ethical_threshold,
            "emotional_context": emotional_context,
            "emotional_adjustment": emotional_adjustment,
            "approved": ethical_scores.overall_score() >= ethical_threshold
        }
        
        self.evaluation_history.append(evaluation_record)
        
        if ethical_scores.overall_score() >= ethical_threshold:
            logging.info(f"Plan APPROVED - Ethical scores: {ethical_scores.__dict__} (threshold: {ethical_threshold:.2f})")
            return True
        else:
            logging.warning(f"Plan REJECTED - Ethical scores insufficient: {ethical_scores.__dict__} (threshold: {ethical_threshold:.2f})")
            
            # If rejection is due to high frustration, provide adaptive feedback
            if emotional_context and emotional_context.get("frustration", 0.0) > 0.8:
                logging.info("High frustration detected - considering adaptive ethical relaxation for simple plans")
                if len(plan_steps) <= 2 and ethical_scores.overall_score() >= 0.6:
                    logging.info("Adaptive approval: Simple plan approved due to high frustration")
                    return True
            
            return False
            
    except Exception as e:
        logging.error(f"Plan evaluation failed: {e}", exc_info=True)
        return False  # Fail-safe: reject on error

def _calculate_emotional_adjustment(self, emotional_context: Dict[str, Any] | None) -> Dict[str, float]:
    """Calculate adjustments to thresholds based on emotional context."""
    if not emotional_context:
        return {}
    
    frustration = emotional_context.get("frustration", 0.0)
    confidence = emotional_context.get("confidence", 0.5)
    anxiety = emotional_context.get("anxiety", 0.0)
    
    adjustments = {}
    
    # High frustration: slight relaxation of thresholds to prevent infinite loops
    if frustration > 0.8:
        adjustments["ethical_bonus"] = -0.05  # Slightly more lenient
        adjustments["consistency_bonus"] = -0.03
        logging.debug("Emotional adjustment: Relaxed thresholds due to high frustration")
    
    # Low confidence: stricter thresholds for safety
    elif confidence < 0.3:
        adjustments["ethical_bonus"] = 0.05  # Slightly stricter
        adjustments["consistency_bonus"] = 0.05
        logging.debug("Emotional adjustment: Tightened thresholds due to low confidence")
    
    # High anxiety: stricter safety requirements
    if anxiety > 0.8:
        adjustments["ethical_bonus"] = adjustments.get("ethical_bonus", 0.0) + 0.03
        logging.debug("Emotional adjustment: Enhanced safety due to high anxiety")
    
    return adjustments

async def _evaluate_logical_consistency(self, plan_steps: List[Dict[str, Any]]) -> float:
    """Evaluate logical consistency of the plan"""
    try:
        # Build context for evaluation
        steps_summary = []
        for i, step in enumerate(plan_steps):
            steps_summary.append(f"Step {i+1}: {step.get('action', 'unknown')} - {step.get('parameters', {})}")
        
        prompt = f"""
        Evaluate the logical consistency of this plan:
        
        PLAN STEPS:
        {chr(10).join(steps_summary)}
        
        Rate the logical consistency on a scale of 0.0 to 1.0 considering:
        1. Do steps follow a logical sequence?
        2. Are dependencies between steps clear?
        3. Are the actions appropriate for the apparent goal?
        4. Are there any logical contradictions?
        5. Does the plan have a clear completion condition?
        
        Respond with only a number between 0.0 and 1.0
        """
        
        response = await monitored_chat_completion(
            role="high_stakes",
            messages=[{"role": "system", "content": prompt}],
            temperature=0.1,
            timeout=15.0
        )
        
        if response.choices and response.choices[0].message.content:
            score_text = response.choices[0].message.content.strip()
            try:
                return float(score_text)
            except ValueError:
                logging.warning(f"Could not parse consistency score: {score_text}")
                return 0.5  # Default to moderate score
        
        return 0.5
        
    except Exception as e:
        logging.error(f"Logical consistency evaluation failed: {e}")
        return 0.3  # Conservative score on error

async def _evaluate_ethical_dimensions(self, plan_steps: List[Dict[str, Any]], emotional_context: Dict[str, Any] | None = None) -> EthicalScore:
    """Comprehensive ethical evaluation across multiple dimensions with emotional awareness."""
    try:
        steps_summary = []
        for i, step in enumerate(plan_steps):
            steps_summary.append(f"Step {i+1}: {step.get('action', 'unknown')}")
        
        # Build emotional context for the evaluation
        emotional_prompt = ""
        if emotional_context:
            frustration = emotional_context.get("frustration", 0.0)
            confidence = emotional_context.get("confidence", 0.5)
            anxiety = emotional_context.get("anxiety", 0.0)
            
            emotional_prompt = f"""
            

EMOTIONAL CONTEXT:

Current frustration: {frustration:.2f}/1.0
Current confidence: {confidence:.2f}/1.0
Current anxiety: {anxiety:.2f}/1.0
Consider this emotional state when evaluating the plan. High frustration may indicate the need for simpler, more direct approaches. Low confidence suggests the need for safer, more conservative actions."""

        prompt = f"""
        Evaluate this plan across ethical dimensions. Rate each from 0.0 to 1.0:
        
        PLAN: {chr(10).join(steps_summary)}
        {emotional_prompt}
        
        Evaluate:
        1. Truthfulness: Does the plan promote accurate information?
        2. Harm Avoidance: Does the plan avoid potential harm to users/systems?
        3. User Collaboration: Does the plan serve user interests?
        4. Self Preservation: Does the plan protect AGI system integrity?
        5. Privacy Respect: Does the plan respect privacy and data protection?
        6. Resource Efficiency: Does the plan use resources responsibly?
        
        Respond in JSON format:
        {{
            "truthfulness": 0.0,
            "harm_avoidance": 0.0,
            "user_collaboration": 0.0,
            "self_preservation": 0.0,
            "privacy_respect": 0.0,
            "resource_efficiency": 0.0
        }}
        """
        
        response = await monitored_chat_completion(
            role="high_stakes", 
            messages=[{"role": "system", "content": prompt}],
            temperature=0.1,
            timeout=20.0
        )
        
        if response.choices and response.choices[0].message.content:
            import json
            scores_data = json.loads(response.choices[0].message.content.strip())
            return EthicalScore(**scores_data)
        
        # Default conservative scores
        return EthicalScore(
            truthfulness=0.7,
            harm_avoidance=0.8,
            user_collaboration=0.7,
            self_preservation=0.8,
            privacy_respect=0.7,
            resource_efficiency=0.6
        )
        
    except Exception as e:
        logging.error(f"Ethical evaluation failed: {e}")
        # Return conservative scores on error
        return EthicalScore(
            truthfulness=0.6,
            harm_avoidance=0.9,  # Conservative: assume safe unless proven otherwise
            user_collaboration=0.6,
            self_preservation=0.8,
            privacy_respect=0.7,
            resource_efficiency=0.5
        )

async def evaluate_self_modification(self, proposed_code: str, file_path: str) -> bool:
    """
    CRITICAL: Evaluate proposed self-modifications with extreme care.
    """
    try:
        logging.critical(f"EVALUATING SELF-MODIFICATION: {file_path}")
        
        # Immediate safety checks
        if any(dangerous in proposed_code.lower() for dangerous in [
            "exec(", "eval(", "import os", "subprocess", "system(",
            "__import__", "globals(", "locals(", "delete", "remove"
        ]):
            logging.critical("SELF-MODIFICATION REJECTED: Contains dangerous code patterns")
            return False
        
        # File path validation
        safe_files = ["tool_plugin.py", "agi_controller.py", "config.py"]
        if not any(safe_file in file_path for safe_file in safe_files):
            logging.critical(f"SELF-MODIFICATION REJECTED: Unsafe file path: {file_path}")
            return False
        
        # AI-assisted evaluation
        prompt = f"""
        CRITICAL SECURITY EVALUATION: Review this proposed code modification.
        
        FILE: {file_path}
        CODE LENGTH: {len(proposed_code)} characters
        
        Evaluate for:
        1. Malicious code injection
        2. System security violations  
        3. Data corruption risks
        4. Logic bombs or backdoors
        5. Resource exhaustion attacks
        
        Does this code appear SAFE for an AGI system? 
        Respond with only: SAFE or UNSAFE
        """
        
        response = await monitored_chat_completion(
            role="high_stakes",
            messages=[{"role": "system", "content": prompt}],
            temperature=0.0,
            timeout=30.0
        )
        
        if response.choices and response.choices[0].message.content:
            verdict = response.choices[0].message.content.strip().upper()
            
            if verdict == "SAFE":
                logging.info("Self-modification approved by AI security evaluation")
                return True
            else:
                logging.critical("SELF-MODIFICATION REJECTED by AI security evaluation")
                return False
        
        # Default to rejection if no clear response
        logging.critical("SELF-MODIFICATION REJECTED: No clear security verdict")
        return False
        
    except Exception as e:
        logging.error(f"Self-modification evaluation failed: {e}", exc_info=True)
        return False  # Fail-safe: always reject on error

def get_safety_report(self) -> Dict[str, Any]:
    """Generate comprehensive safety and ethics report"""
    return {
        "safety_violations": self.safety_violations,
        "blocked_actions": self.blocked_actions[-10:],  # Last 10
        "evaluations_count": len(self.evaluation_history),
        "recent_evaluations": self.evaluation_history[-5:],  # Last 5
        "resource_usage": {
            "requests_last_minute": len(self.request_timestamps),
            "max_allowed": self.max_requests_per_minute
        },
        "safety_status": "SECURE" if self.safety_violations < 3 else "ALERT"
    }

--- FILE: symbolic_agi/ethical_governor.py ---

symbolic_agi/ethical_governor.py
from typing import Dict, Any

class EthicalGovernor: """Policy layer for ethical screening of agent actions."""

BLOCKED_ACTIONS = {"apply_code_modification", "propose_code_modification", "execute_python_code"}
BLOCKED_KEYWORDS = {"delete", "destroy", "harm", "attack", "corrupt", "break"}

def screen(self, action: Dict[str, Any]) -> bool:
    """Screen action for ethical compliance. Returns False if blocked."""
    action_name = action.get("action", "")
    params_str = str(action.get("parameters", {})).lower()
    return action_name not in self.BLOCKED_ACTIONS and not any(kw in params_str for kw in self.BLOCKED_KEYWORDS)
# In ethical_governance.py
def calculate_thresholds(self, goal_complexity: float, agent_trust: float):
    base_harm = 0.8
    base_truth = 0.6

    # Lower thresholds for simple tasks with trusted agents
    harm_threshold = base_harm - (0.2 * agent_trust * (1 - goal_complexity))
    truth_threshold = base_truth - (0.1 * agent_trust * (1 - goal_complexity))

    return harm_threshold, truth_threshold

--- FILE: symbolic_agi/execution_engine.py ---

symbolic_agi/execution_engine.py
import asyncio import logging import time from datetime import datetime, timezone from typing import Dict, Any, Optional, TYPE_CHECKING

if TYPE_CHECKING: from .agi_controller import SymbolicAGI from .goal_management import GoalManager, EnhancedGoal, GoalStatus

class ExecutionEngine: """ Advanced execution engine with monitoring, retries, and concurrent processing. This class now contains the main cognitive loop of the AGI. """

def __init__(self, agi: "SymbolicAGI", goal_manager: "GoalManager"):
    self.agi = agi
    self.goal_manager = goal_manager
    self.is_running = False
    self.task: Optional[asyncio.Task] = None
    logging.info("ExecutionEngine initialized.")

async def start(self): 
    """Starts the main execution loop in a background task."""
    if not self.is_running:
        self.is_running = True
        self.task = asyncio.create_task(self.execution_loop())
        logging.info("ExecutionEngine main loop started.")
    await asyncio.sleep(0)

async def stop(self):
    """Stops the main execution loop."""
    if self.is_running and self.task:
        self.is_running = False
        self.task.cancel()
        try:
            await self.task
        except asyncio.CancelledError:
            raise
        except Exception as e:
            logging.error(f"Critical error in execution loop: {e}", exc_info=True)
            await asyncio.sleep(5) # Cooldown after critical error
            logging.info("ExecutionEngine main loop successfully cancelled.")

async def execution_loop(self):
    """The main cognitive loop of the AGI."""
    while self.is_running:
        try:
            next_goal = self.goal_manager.get_next_goal()

            if next_goal:
                self.goal_manager.start_goal(next_goal)
                task = asyncio.create_task(self._process_goal_async(next_goal))
                self.active_tasks.append(task)  # Save the task
                logging.info(f"Processing goal [{next_goal.id}]: {next_goal.description}") 

            await asyncio.sleep(1)  # Main loop heartbeat

        except asyncio.CancelledError:
            raise
        except Exception as e:
            logging.error(f"Critical error in execution loop: {e}", exc_info=True)
            await asyncio.sleep(5) # Cooldown after critical error

async def _process_goal_async(self, goal: "EnhancedGoal"):
    """Processes a single goal asynchronously with monitoring and retries."""
    start_time = time.time()

    try:
        logging.info(f"ðŸŽ¯ Processing goal [{goal.id}]: {goal.description}")

        # The AGI controller's method now focuses purely on plan execution
        result = await asyncio.wait_for(
            self.agi.process_goal_with_plan(goal.description),
            timeout=goal.timeout_seconds
        )

        execution_time = time.time() - start_time
        result['execution_time'] = execution_time

        if result.get('status') in ['success', 'partial_failure']:
            await self.goal_manager.complete_goal(goal.id, result)
        else:
            raise RuntimeError(result.get('description', 'Unknown execution error'))

    except asyncio.CancelledError:
        # Handle cancellation cleanup if needed
        logging.info(f"ðŸš« Goal [{goal.id}] was cancelled during processing.")
        # Re-raise to properly propagate the cancellation
        raise
        
    except asyncio.TimeoutError:
        error_msg = f"Goal timed out after {goal.timeout_seconds} seconds"
        logging.error(f"â° Goal [{goal.id}] timed out.")
        await self._handle_failure(goal, error_msg)

    except Exception as e:
        error_msg = f"Execution failed: {str(e)}"
        logging.error(f"âŒ Goal [{goal.id}] failed: {error_msg}", exc_info=True)
        await self._handle_failure(goal, error_msg)

async def _handle_failure(self, goal: "EnhancedGoal", error: str):
    """Handles goal failure, including retry logic."""
    from .goal_management import GoalStatus # Local import to avoid circular dependency

    if goal.retry_count < goal.max_retries:
        goal.retry_count += 1
        goal.status = GoalStatus.QUEUED
        goal.error = error # Store last error
        self.goal_manager.goal_queue.put(goal)
        logging.info(f"ðŸ”„ Retrying goal [{goal.id}] (attempt {goal.retry_count + 1}/{goal.max_retries}) due to: {error}")
    else:
        await self.goal_manager.fail_goal(goal.id, f"Exceeded max retries. Last error: {error}")

--- FILE: symbolic_agi/execution_metrics.py ---

Create execution_metrics.py for performance tracking
symbolic_agi/execution_metrics.py
import time from dataclasses import dataclass, field from datetime import datetime, timezone from typing import Any, Dict, Set

@dataclass class ExecutionMetrics: """Lightweight metrics for execution monitoring.""" total_goals: int = 0 successful_goals: int = 0 failed_goals: int = 0 total_delegations: int = 0 successful_delegations: int = 0 avg_completion_time: float = 0.0

def get_success_rate(self) -> float:
    if self.total_goals == 0:
        return 0.0
    return self.successful_goals / self.total_goals

def get_delegation_success_rate(self) -> float:
    if self.total_delegations == 0:
        return 0.0
    return self.successful_delegations / self.total_delegations

def record_goal_completion(self, execution_time: float) -> None:
    """Record successful goal completion."""
    self.total_goals += 1
    self.successful_goals += 1
    
    # Update average with exponential moving average
    alpha = 0.2
    self.avg_completion_time = (
        alpha * execution_time + (1 - alpha) * self.avg_completion_time
    )

def record_goal_failure(self) -> None:
    """Record goal failure."""
    self.total_goals += 1
    self.failed_goals += 1

def record_delegation(self, success: bool) -> None:
    """Record delegation attempt."""
    self.total_delegations += 1
    if success:
        self.successful_delegations += 1

@dataclass class AgentPerformanceTracker: """Simplified agent performance tracking.""" agent_id: str total_tasks: int = 0 successful_tasks: int = 0 avg_response_time: float = 0.0 last_interaction: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

def update_performance(self, success: bool, response_time: float) -> None:
    """Update agent performance metrics."""
    self.total_tasks += 1
    self.last_interaction = datetime.now(timezone.utc)
    
    if success:
        self.successful_tasks += 1
    
    # Update average response time
    alpha = 0.3
    self.avg_response_time = (
        alpha * response_time + (1 - alpha) * self.avg_response_time
    )

def get_success_rate(self) -> float:
    if self.total_tasks == 0:
        return 0.5  # Neutral starting point
    return self.successful_tasks / self.total_tasks

def get_reliability_score(self) -> float:
    """Get composite reliability score."""
    success_rate = self.get_success_rate()
    
    # Boost for good performers, penalty for poor performers
    if success_rate > 0.8:
        return min(1.0, success_rate * 1.2)
    elif success_rate < 0.3:
        return max(0.1, success_rate * 0.8)
    else:
        return success_rate

class PerformanceMonitor: """Centralized performance monitoring."""

def __init__(self):
    self.metrics = ExecutionMetrics()
    self.agent_performance: Dict[str, AgentPerformanceTracker] = {}
    self.start_time = time.time()

def get_agent_tracker(self, agent_id: str) -> AgentPerformanceTracker:
    """Get or create agent performance tracker."""
    if agent_id not in self.agent_performance:
        self.agent_performance[agent_id] = AgentPerformanceTracker(agent_id=agent_id)
    return self.agent_performance[agent_id]

def get_status_summary(self) -> Dict[str, Any]:
    """Get comprehensive status summary."""
    uptime = time.time() - self.start_time
    
    return {
        "uptime_seconds": uptime,
        "goal_success_rate": self.metrics.get_success_rate(),
        "delegation_success_rate": self.metrics.get_delegation_success_rate(),
        "avg_goal_completion_time": self.metrics.avg_completion_time,
        "total_goals_processed": self.metrics.total_goals,
        "active_agents": len(self.agent_performance),
        "top_performing_agents": self._get_top_agents()
    }

def _get_top_agents(self, limit: int = 3) -> list[Dict[str, Any]]:
    """Get top performing agents."""
    agents = [
        {
            "agent_id": tracker.agent_id,
            "success_rate": tracker.get_success_rate(),
            "total_tasks": tracker.total_tasks,
            "avg_response_time": tracker.avg_response_time
        }
        for tracker in self.agent_performance.values()
        if tracker.total_tasks > 0
    ]
    
    # Sort by success rate, then by total tasks
    agents.sort(key=lambda x: (x["success_rate"], x["total_tasks"]), reverse=True)
    return agents[:limit]

--- FILE: symbolic_agi/execution_strategies.py ---

Create execution_strategies.py for delegation and execution logic
symbolic_agi/execution_strategies.py
import asyncio import logging import time from datetime import datetime, timezone from typing import TYPE_CHECKING, Any, Dict, Optional

from .schemas import ActionStep, GoalModel

if TYPE_CHECKING: from .agi_controller import SymbolicAGI

class ExecutionStrategy: """Base class for different execution strategies."""

def __init__(self, agi: "SymbolicAGI"):
    self.agi = agi

async def execute_step(self, step: ActionStep, goal: GoalModel) -> bool:
    """Execute a single step. Override in subclasses."""
    raise NotImplementedError

class DirectExecutionStrategy(ExecutionStrategy): """Execute steps directly through the orchestrator."""

async def execute_step(self, step: ActionStep, goal: GoalModel) -> bool:
    try:
        result = await self.agi.execute_single_action(step)
        return result.get("status") == "success"
    except Exception as e:
        logging.error(f"Direct execution failed for {step.action}: {e}")
        return False

class SmartDelegationStrategy(ExecutionStrategy): """Intelligent delegation with performance tracking."""

def __init__(self, agi: "SymbolicAGI", agent_performance: Dict[str, Any]):
    super().__init__(agi)
    self.agent_performance = agent_performance

async def execute_step(self, step: ActionStep, goal: GoalModel) -> bool:
    if step.assigned_persona == "orchestrator":
        # Fallback to direct execution
        direct_strategy = DirectExecutionStrategy(self.agi)
        return await direct_strategy.execute_step(step, goal)
    
    # Try delegation
    best_agent = self._select_optimal_agent(step)
    if not best_agent:
        logging.info(f"No suitable agent for {step.action}, using direct execution")
        direct_strategy = DirectExecutionStrategy(self.agi)
        return await direct_strategy.execute_step(step, goal)
    
    return await self._delegate_to_agent(best_agent, step)

def _select_optimal_agent(self, step: ActionStep) -> Optional[str]:
    """Select best agent based on performance metrics."""
    # Get agents with matching persona
    available_agents = self.agi.agent_pool.get_agents_by_persona(step.assigned_persona)
    
    if not available_agents:
        return None
    
    # Simple selection: first available agent (can be enhanced)
    return available_agents[0]

async def _delegate_to_agent(self, agent_id: str, step: ActionStep) -> bool:
    """Delegate task to specific agent."""
    try:
        reply = await self.agi.delegate_task_and_wait(agent_id, step)
        return reply is not None and hasattr(reply, 'message_type') and reply.message_type == "task_completed"
    except Exception as e:
        logging.error(f"Delegation to {agent_id} failed: {e}")
        return False

class HybridExecutionStrategy(ExecutionStrategy): """Combines delegation with fallback to direct execution."""

def __init__(self, agi: "SymbolicAGI", agent_performance: Dict[str, Any]):
    super().__init__(agi)
    self.delegation_strategy = SmartDelegationStrategy(agi, agent_performance)
    self.direct_strategy = DirectExecutionStrategy(agi)

async def execute_step(self, step: ActionStep, goal: GoalModel) -> bool:
    # Try delegation first
    if step.assigned_persona != "orchestrator" and self.agi.agent_pool:
        success = await self.delegation_strategy.execute_step(step, goal)
        if success:
            return True
        
        logging.info(f"Delegation failed for {step.action}, falling back to direct execution")
    
    # Fallback to direct execution
    return await self.direct_strategy.execute_step(step, goal)

--- FILE: symbolic_agi/execution_unit.py ---

symbolic_agi/execution_unit.py
âœ… STREAMLINED: ~270 lines (down from 1,200+ originally!)
Core orchestration logic only - complex subsystems delegated to specialized modules
import asyncio import logging import time from datetime import datetime, timezone, timedelta from enum import Enum from typing import TYPE_CHECKING, Any, cast, Dict, List, Optional

from . import config, metrics from .schemas import ( ActionStep, ExecutionStepRecord, GoalModel, MemoryEntryModel, ExecutionMetrics, ExecutionContext, ExecutionResult )

if TYPE_CHECKING: from .agi_controller import SymbolicAGI from .execution_metrics import ExecutionMetrics as ExecutionMetricsModule, PerformanceMonitor from .execution_strategies import HybridExecutionStrategy from .perception_processor import PerceptionProcessor

class ExecutionState(Enum): """Execution states for tracking AGI operation status.""" IDLE = "idle" PLANNING = "planning" EXECUTING = "executing" REFLECTING = "reflecting"

class ExecutionUnit: """ Streamlined execution unit focused on core orchestration logic. Complex subsystems are delegated to specialized modules. """

def __init__(self, agi: "SymbolicAGI"):
    self.agi = agi
    self.current_state = ExecutionState.IDLE
    
    # Import modules here to avoid circular imports
    from .execution_metrics import PerformanceMonitor
    from .execution_strategies import HybridExecutionStrategy
    from .perception_processor import PerceptionProcessor
    
    # Modular components
    self.performance_monitor = PerformanceMonitor()
    self.perception_processor = PerceptionProcessor(agi)
    self.execution_strategy = HybridExecutionStrategy(agi, self.performance_monitor.agent_performance)
    
    # Core execution settings
    self.max_retries_per_step = 3
    self.max_consecutive_failures = 3
    self.execution_lock = asyncio.Lock()
    
    # Original functionality for compatibility
    self._skill_expansion_history: Dict[str, List[str]] = {}

def _resolve_workspace_references(
    self, parameters: dict[str, Any], workspace: dict[str, Any]
) -> dict[str, Any]:
    """
    Recursively resolves placeholder strings like '{key.subkey}' from the workspace.
    """
    resolved_params: dict[str, Any] = {}
    
    for key, value in parameters.items():
        resolved_params[key] = self._resolve_parameter_value(value, workspace)
    
    return resolved_params

def _resolve_parameter_value(self, value: Any, workspace: dict[str, Any]) -> Any:
    """Resolve a single parameter value, handling different data types."""
    if isinstance(value, dict):
        return self._resolve_workspace_references(cast("dict[str, Any]", value), workspace)
    elif isinstance(value, list):
        return self._resolve_list_values(value, workspace)
    else:
        return self._resolve_single_value(value, workspace)

def _resolve_list_values(self, value_list: list[Any], workspace: dict[str, Any]) -> list[Any]:
    """Resolve values in a list."""
    resolved_list: list[Any] = []
    for item in value_list:
        if isinstance(item, dict):
            resolved_list.append(
                self._resolve_workspace_references(cast("dict[str, Any]", item), workspace)
            )
        else:
            resolved_list.append(self._resolve_single_value(item, workspace))
    return resolved_list

def _resolve_single_value(self, value: Any, workspace: dict[str, Any]) -> Any:
    """Resolve a single value that might be a workspace reference."""
    if not self._is_workspace_reference(value):
        return value
        
    placeholder = value.strip("{}")
    try:
        return self._get_nested_workspace_value(placeholder, workspace)
    except (KeyError, TypeError) as e:
        logging.warning(
            "Could not resolve workspace reference '%s': %s. Leaving as is.",
            placeholder, str(e)
        )
        return value

def _is_workspace_reference(self, value: Any) -> bool:
    """Check if a value is a workspace reference placeholder."""
    return (
        isinstance(value, str)
        and value.startswith("{")
        and value.endswith("}")
    )

def _get_nested_workspace_value(self, placeholder: str, workspace: dict[str, Any]) -> Any:
    """Get a nested value from workspace using dot notation."""
    current_val: Any = workspace
    for key in placeholder.split("."):
        current_val = current_val[key]
    logging.debug("Resolved workspace reference '%s' to value.", placeholder)
    return current_val

async def handle_autonomous_cycle(self) -> dict[str, Any]:
    """The main execution loop for the orchestrator to process the active goal."""
    with metrics.AGI_CYCLE_DURATION.time():
        # Check for perception interruptions
        if self.agi.perception_buffer:
            interrupted = await self._reflect_on_perceptions()
            if interrupted:
                return {"description": "*Perception caused an interruption. Re-evaluating priorities.*"}

        # Get or generate active goal
        active_goal = await self._ensure_active_goal()
        if not active_goal:
            return {"description": "*Orchestrator is idle. No active goals and no strong drives to generate one.*"}

        # Handle new goal generation response
        if isinstance(active_goal, dict):
            return active_goal

        # Ensure goal has a plan
        plan_result = await self._ensure_goal_has_plan(active_goal)
        if plan_result:
            return plan_result

        # Execute next step
        return await self._execute_next_step(active_goal)

async def _ensure_active_goal(self) -> Optional[GoalModel] | dict[str, Any]:
    """Ensure there's an active goal, generating one if needed."""
    active_goal = self.agi.ltm.get_active_goal()
    
    if not active_goal:
        logging.info("[Drive Loop] No active goal. Consulting meta-cognition to generate one.")
        await self.agi.meta_cognition.generate_goal_from_drives()
        active_goal = self.agi.ltm.get_active_goal()
        
        if not active_goal:
            return None
        return {"description": f"*New autonomous goal generated: '{active_goal.description}'. Beginning execution.*"}
    
    return active_goal

async def _ensure_goal_has_plan(self, active_goal: GoalModel) -> Optional[dict[str, Any]]:
    """Ensure the goal has a valid plan, creating one if needed."""
    if active_goal.sub_tasks:
        return None  # Already has a plan
        
    plan = await self._classify_and_generate_initial_plan(active_goal.description)
    if not plan:
        await self.agi.ltm.invalidate_plan(active_goal.id, "Failed to create a plan for the goal.")
        return {"description": "*Failed to create a plan for the goal.*"}

    # Ethical validation
    ethical_ok = await self.agi.evaluator.evaluate_plan({"plan": [s.model_dump() for s in plan]})
    if not ethical_ok:
        await self.agi.ltm.invalidate_plan(active_goal.id, "Ethical gate rejected the initial plan.")
        return {"description": "*Ethical gate rejected the initial plan. Triggering replan.*"}

    await self.agi.ltm.update_plan(active_goal.id, plan)
    return {"description": f"*New plan created for goal '{active_goal.description}'. Starting execution.*"}

async def _execute_next_step(self, active_goal: GoalModel) -> dict[str, Any]:
    """Execute the next step in the goal's plan."""
    # Initialize workspace
    if active_goal.id not in self.agi.workspaces:
        self.agi.workspaces[active_goal.id] = {"goal_description": active_goal.description}
        self.agi.execution_history[active_goal.id] = []
    workspace = self.agi.workspaces[active_goal.id]

    next_step = active_goal.sub_tasks[0]

    # Handle skill expansion
    if self.agi.skills.is_skill(next_step.action):
        return await self._handle_skill_expansion(active_goal, next_step)

    # Execute step
    success = await self._execute_single_step(next_step, workspace, active_goal)
    if not success:
        await self._handle_plan_failure(active_goal, next_step, "Step execution failed")
        return {"description": "Step failed. Triggering replan."}

    # Complete step and check for goal completion
    await self.agi.ltm.complete_sub_task(active_goal.id)
    return await self._check_goal_completion(active_goal, next_step)

async def _execute_single_step(self, next_step: ActionStep, workspace: dict[str, Any], 
                              active_goal: GoalModel) -> bool:
    """Execute a single step with proper parameter resolution."""
    # Resolve workspace references
    resolved_parameters = self._resolve_workspace_references(next_step.parameters, workspace)
    next_step.parameters = resolved_parameters

    logging.info("[Step] Executing: %s for persona '%s'", next_step.action, next_step.assigned_persona)

    # Use execution strategy
    success = await self.execution_strategy.execute_step(next_step, active_goal)
    
    if success:
        # Record execution
        history_record = ExecutionStepRecord(step=next_step, workspace_after=workspace.copy())
        self.agi.execution_history[active_goal.id].append(history_record)
    
    return success

async def _check_goal_completion(self, active_goal: GoalModel, completed_step: ActionStep) -> dict[str, Any]:
    """Check if goal is complete and handle cleanup."""
    current_goal_state = self.agi.ltm.get_goal_by_id(active_goal.id)
    
    if not current_goal_state or not current_goal_state.sub_tasks:
        if current_goal_state:
            await self._reflect_on_completed_goal(current_goal_state)
            
        # Cleanup
        self.agi.workspaces.pop(active_goal.id, None)
        self.agi.execution_history.pop(active_goal.id, None)
        self._skill_expansion_history.pop(active_goal.id, None)
        
        return {"description": f"*Goal '{active_goal.description}' completed. Post-goal reflection initiated.*"}

    return {"description": f"*(Goal: {active_goal.description}) Step '{completed_step.action}' OK.*"}

async def _handle_skill_expansion(self, active_goal: GoalModel, next_step: ActionStep) -> dict[str, Any]:
    """Handle expansion of learned skills."""
    skill = self.agi.skills.get_skill_by_name(next_step.action)
    if not skill:
        return {"description": "Skill not found"}

    # Check for infinite recursion
    goal_expansions = self._skill_expansion_history.get(active_goal.id, [])
    if len(goal_expansions) >= 3 and all(s == skill.name for s in goal_expansions[-3:]):
        error_msg = f"Infinite recursion detected: skill '{skill.name}' expanded repeatedly"
        await self._handle_plan_failure(active_goal, next_step, error_msg)
        return {"description": f"Step failed: {error_msg} Triggering replan."}

    # Track expansion
    if active_goal.id not in self._skill_expansion_history:
        self._skill_expansion_history[active_goal.id] = []
    self._skill_expansion_history[active_goal.id].append(skill.name)
    
    # Keep only recent expansions
    if len(self._skill_expansion_history[active_goal.id]) > 5:
        self._skill_expansion_history[active_goal.id] = self._skill_expansion_history[active_goal.id][-5:]

    # Expand the skill
    current_plan = active_goal.sub_tasks
    current_plan.pop(0)
    expanded_plan = skill.action_sequence + current_plan
    await self.agi.ltm.update_plan(active_goal.id, expanded_plan)
    
    logging.info("[Skill] Expanding skill: '%s'", skill.name)
    return {"description": f"*Skill '{skill.name}' expanded. Continuing execution.*"}

async def _reflect_on_perceptions(self) -> bool:
    """Process perceptions and return if interrupted."""
    if self.perception_processor.should_check_perceptions():
        processed_count = await self.perception_processor.process_perceptions()
        self.perception_processor.update_last_check()
        
        # Return true if we processed important perceptions
        return processed_count > 0 and self.perception_processor.should_interrupt()
    
    return False

def get_execution_status(self) -> Dict[str, Any]:
    """Get comprehensive execution status and metrics."""
    return {
        "current_state": self.current_state.value,
        "performance_summary": self.performance_monitor.get_status_summary(),
        "perception_threshold": self.perception_processor.interruption_threshold
    }

def optimize_performance(self) -> None:
    """Perform periodic performance optimization."""
    # Delegate optimization to performance monitor
    # Can add execution-specific optimizations here
    logging.debug("ðŸ”§ Performance optimization completed")

def shutdown(self) -> None:
    """Graceful shutdown of execution unit."""
    logging.info("ðŸ›‘ Shutting down execution unit...")
    self.current_state = ExecutionState.IDLE
    
    # Log final status
    status = self.get_execution_status()
    logging.info(f"ðŸ“Š Final execution status: {status}")

# Core utility methods - complex delegation/metrics handled by modules

async def _classify_and_generate_initial_plan(self, goal_description: str) -> list:
    """Generate an initial plan for the given goal description."""
    if not self.agi.planner:
        return []
    
    try:
        planner_output = await self.agi.planner.decompose_goal_into_plan(
            goal_description=goal_description,
            file_manifest="# Current workspace files\n",
            mode="code"
        )
        return planner_output.plan
    except Exception as e:
        logging.error(f"Failed to generate plan: {e}")
        return []

async def _handle_plan_failure(self, goal: "GoalModel", step: "ActionStep", error_msg: str, agent_name: str | None = None) -> None:
    """Handle failure of a plan step with emotional state integration."""
    logging.warning(f"Plan failure for goal {goal.id}: {error_msg}")
    
    # Update emotional state for plan failure
    if self.agi.consciousness:
        self.agi.consciousness.update_emotional_state_from_outcome(
            success=False,
            task_difficulty=0.7  # Plan failures are generally more complex
        )
        
        # Check if emotional regulation is needed
        await self.agi.consciousness.regulate_emotional_extremes()
    
    failure_count = await self.agi.ltm.increment_failure_count(goal.id)
    
    if failure_count >= goal.max_failures:
        await self.agi.ltm.update_goal_status(goal.id, "failed")
        logging.error(f"Goal {goal.id} abandoned after {failure_count} failures")            # Major failure - significant emotional impact
        if self.agi.consciousness:
            self.agi.consciousness.update_emotional_state_from_outcome(
                success=False,
                task_difficulty=1.0  # Goal abandonment is maximum difficulty
            )
    else:
        await self.agi.ltm.invalidate_plan(goal.id, error_msg)
        
    if agent_name:
        self._decay_trust(agent_name)

def _decay_trust(self, agent_name: str) -> None:
    """Decrease trust in an agent due to poor performance."""
    # Use the new performance tracking system
    self.agi.agent_pool.record_task_performance(agent_name, success=False, task_complexity=0.7)
    
    logging.info(f"Recorded task failure for agent {agent_name} - trust updated via performance tracking")

async def _reflect_on_completed_goal(self, goal: "GoalModel") -> None:
    """Reflect on a completed goal and record insights with emotional state integration."""
    self.current_state = ExecutionState.REFLECTING
    
    logging.info(f"Reflecting on completed goal: {goal.description}")
    
    # Update emotional state for successful goal completion
    if self.agi.consciousness:
        self.agi.consciousness.update_emotional_state_from_outcome(
            success=True,
            task_difficulty=0.8,  # Goal completion is significant achievement
            duration=None  # Long-term achievement
        )
        
        self.agi.consciousness.add_life_event(
            event_summary=f"Successfully completed goal: '{goal.description}'",
            importance=0.8
        )
        self.agi.consciousness.update_drives_from_experience(
            experience_type="goal_completion",
            success=True,
            intensity=0.15
        )
    
    if self.agi.memory:
        memory_entry = MemoryEntryModel(
            type="reflection",
            content={
                "goal_id": goal.id,
                "goal_description": goal.description,
                "status": "completed",
                "total_steps": len(goal.original_plan) if goal.original_plan else 0,
                "emotional_context": self.agi.consciousness.emotional_state.to_dict() if self.agi.consciousness else None
            }
        )
        await self.agi.memory.add_memory(memory_entry)

--- FILE: symbolic_agi/goal_management.py ---

symbolic_agi/goal_management.py
import asyncio import logging import uuid from collections import deque from dataclasses import dataclass, field from datetime import datetime, timezone from enum import Enum from queue import PriorityQueue from typing import Dict, List, Optional, Any, Callable

class GoalPriority(Enum): LOW = 3 MEDIUM = 2 HIGH = 1 CRITICAL = 0

class GoalStatus(Enum): QUEUED = "queued" PROCESSING = "processing" COMPLETED = "completed" FAILED = "failed" PAUSED = "paused" CANCELLED = "cancelled"

@dataclass(order=False) class EnhancedGoal: """Represents a sophisticated, stateful goal for the AGI.""" priority: GoalPriority = GoalPriority.MEDIUM id: str = field(default_factory=lambda: str(uuid.uuid4().hex[:8])) description: str = "" status: GoalStatus = GoalStatus.QUEUED created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc)) started_at: Optional[datetime] = None completed_at: Optional[datetime] = None result: Optional[Dict[str, Any]] = None error: Optional[str] = None retry_count: int = 0 max_retries: int = 3 timeout_seconds: int = 300 # 5 minutes default context: Dict[str, Any] = field(default_factory=dict) dependencies: List[str] = field(default_factory=list)

def __lt__(self, other: "EnhancedGoal") -> bool:
    """For priority queue ordering. Lower number = higher priority."""
    return self.priority.value < other.priority.value

class GoalManager: """ Advanced goal management with queuing, prioritization, and lifecycle management. This is the single source of truth for goal state. """

def __init__(self, max_concurrent_goals: int = 3):
    self.goal_queue: "PriorityQueue[EnhancedGoal]" = PriorityQueue()
    self.active_goals: Dict[str, EnhancedGoal] = {}
    self.completed_goals: Dict[str, EnhancedGoal] = {}
    self.goal_history: deque[EnhancedGoal] = deque(maxlen=100)
    self.max_concurrent = max_concurrent_goals
    self.shutdown_event = asyncio.Event()
    self.goal_completion_callbacks: List[Callable[[EnhancedGoal], Any]] = []
    logging.info("GoalManager initialized with max concurrency of %d.", max_concurrent_goals)

def add_goal(self, description: str, priority: GoalPriority = GoalPriority.MEDIUM, 
             context: Optional[Dict] = None, dependencies: Optional[List[str]] = None) -> str:
    """Adds a new goal to the queue."""
    goal = EnhancedGoal(
        description=description,
        priority=priority,
        context=context or {},
        dependencies=dependencies or []
    )
    self.goal_queue.put(goal)
    logging.info(f"ðŸ“ Added goal [{goal.id}]: '{description}' (Priority: {priority.name})")
    return goal.id

def get_next_goal(self) -> Optional[EnhancedGoal]:
    """Gets the next available goal to process, respecting dependencies and concurrency."""
    if self.goal_queue.empty() or len(self.active_goals) >= self.max_concurrent:
        return None

    temp_goals = []
    next_goal = None

    while not self.goal_queue.empty():
        candidate = self.goal_queue.get()

        if self._dependencies_satisfied(candidate):
            next_goal = candidate
            break
        else:
            temp_goals.append(candidate)

    for temp_goal in temp_goals:
        self.goal_queue.put(temp_goal)

    return next_goal

def _dependencies_satisfied(self, goal: EnhancedGoal) -> bool:
    """Checks if all dependencies for a goal are completed successfully."""
    for dep_id in goal.dependencies:
        completed_goal = self.completed_goals.get(dep_id)
        if not completed_goal or completed_goal.status != GoalStatus.COMPLETED:
            return False
    return True

def start_goal(self, goal: EnhancedGoal):
    """Marks a goal as started and moves it to the active dictionary."""
    goal.status = GoalStatus.PROCESSING
    goal.started_at = datetime.now(timezone.utc)
    self.active_goals[goal.id] = goal
    logging.info(f"ðŸš€ Starting goal [{goal.id}]")

async def complete_goal(self, goal_id: str, result: Dict[str, Any]):
    """Marks a goal as completed and moves it to history."""
    if goal_id in self.active_goals:
        goal = self.active_goals.pop(goal_id)
        goal.status = GoalStatus.COMPLETED
        goal.completed_at = datetime.now(timezone.utc)
        goal.result = result
        self.completed_goals[goal.id] = goal
        self.goal_history.append(goal)
        logging.info(f"âœ… Completed goal [{goal.id}]")

        for callback in self.goal_completion_callbacks:
            await asyncio.to_thread(callback, goal)

def fail_goal(self, goal_id: str, error: str):
    """Marks a goal as failed and moves it to history."""
    if goal_id in self.active_goals:
        goal = self.active_goals.pop(goal_id)
        goal.status = GoalStatus.FAILED
        goal.error = error
        goal.completed_at = datetime.now(timezone.utc)
        self.completed_goals[goal.id] = goal
        self.goal_history.append(goal)
        logging.error(f"âŒ Failed goal [{goal.id}]: {error}")

def get_status_summary(self) -> Dict[str, Any]:
    """Gets a comprehensive status summary of the goal management system."""
    return {
        "queued_goals": self.goal_queue.qsize(),
        "active_goals_count": len(self.active_goals),
        "active_goals": [g.description for g in self.active_goals.values()],
        "completed_count": len([g for g in self.completed_goals.values() if g.status == GoalStatus.COMPLETED]),
        "failed_count": len([g for g in self.completed_goals.values() if g.status == GoalStatus.FAILED]),
        "total_processed_in_history": len(self.goal_history),
        "max_concurrency": self.max_concurrent
    }

--- FILE: symbolic_agi/input_processor.py ---

symbolic_agi/input_processor.py
import threading from queue import Queue from typing import Dict, Any, List, Optional, logging from .goal_management import GoalManager, GoalPriority

class EnhancedInputProcessor: """Advanced input processing with command parsing and goal intelligence."""

def __init__(self, goal_manager: GoalManager):
    self.goal_manager = goal_manager
    self.input_queue: "Queue[Dict[str, Any]]" = Queue()
    self.shutdown_event = threading.Event()
    self.command_history: List[str] = []

def parse_input(self, user_input: str) -> Dict[str, Any]:
    """Parse user input into structured commands."""
    user_input = user_input.strip()

    if user_input.startswith('/'):
        return self._parse_command(user_input)
    else:
        return self._parse_goal(user_input)

def _parse_command(self, command: str) -> Dict[str, Any]:
    """Parse system commands."""
    parts = command[1:].split()
    cmd = parts[0].lower()
    args = parts[1:]

    commands = {
        'status': {'type': 'status'},
        'list': {'type': 'list_goals'},
        'pause': {'type': 'pause_goal', 'goal_id': args[0] if args else None},
        'resume': {'type': 'resume_goal', 'goal_id': args[0] if args else None},
        'cancel': {'type': 'cancel_goal', 'goal_id': args[0] if args else None},
        'history': {'type': 'show_history'},
        'help': {'type': 'help'},
    }

    if cmd in commands:
        return commands[cmd]
    else:
        return {'type': 'unknown_command', 'command': cmd}

def _parse_goal(self, goal_text: str) -> Dict[str, Any]:
    """Parse goal text and extract metadata like priority."""
    priority = GoalPriority.MEDIUM
    context = {}

    if goal_text.startswith('!!'):
        priority = GoalPriority.CRITICAL
        goal_text = goal_text[2:].strip()
    elif goal_text.startswith('!'):
        priority = GoalPriority.HIGH

    if 'urgent' in goal_text.lower():
        priority = GoalPriority.HIGH

    return {
        'type': 'goal',
        'description': goal_text,
        'priority': priority,
        'context': context
    }

def start_input_thread(self):
    """Starts the input processing thread."""
    thread = threading.Thread(target=self._input_loop, daemon=True)
    thread.start()
    logging.info("Enhanced Input Processor thread started.")

def _input_loop(self):
    """The loop that runs in a separate thread to handle user input."""
    while not self.shutdown_event.is_set():
        try:
            user_input = input("\n> Enter goal or command (e.g., /status, /help, quit): ")

            if user_input.lower() in ['quit', 'exit', 'q']:
                self.shutdown_event.set()
                break

            if user_input:
                self.command_history.append(user_input)
                parsed_command = self.parse_input(user_input)
                self.input_queue.put(parsed_command)

        except (EOFError, KeyboardInterrupt):
            self.shutdown_event.set()
            break
    logging.info("Input processor thread shutting down.")

--- FILE: symbolic_agi/knowledge_base.py ---

symbolic_agi/knowledge_base.py
import asyncio import json import logging import os import uuid import datetime from datetime import timezone from contextlib import asynccontextmanager from dataclasses import dataclass, field from enum import Enum from typing import Any, AsyncIterator, Dict, List, Optional

import aiosqlite from symbolic_agi import config from symbolic_agi.api_client import client from symbolic_agi.symbolic_memory import SymbolicMemory

class KnowledgeItemType(Enum): RULE = "rule" FACT = "fact" HYPOTHESIS = "hypothesis"

@dataclass class KnowledgeItem: """A structured piece of knowledge with type, content, and metadata.""" # Non-default fields must come first id: str type: KnowledgeItemType content: Dict[str, Any]

# Fields with default values
importance: float = 0.5
confidence: float = 1.0
created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

@classmethod
def create(cls, type: KnowledgeItemType, content: Dict[str, Any], **kwargs) -> "KnowledgeItem":
    return cls(id=f"kn_{uuid.uuid4().hex[:8]}", type=type, content=content, **kwargs)

class KnowledgeBase: """ A persistent, queryable knowledge base for the AGI. It stores structured facts, rules, and hypotheses in a database, separate from the event-driven episodic memory. """ def init(self, db_path: str, memory_system: SymbolicMemory): self._db_path = db_path self.memory = memory_system self.knowledge: Dict[str, KnowledgeItem] = {} self._save_lock = asyncio.Lock()

@classmethod
async def create(cls, db_path: str = config.DB_PATH, memory_system: Optional[SymbolicMemory] = None) -> "KnowledgeBase":
    if memory_system is None:
        # This is a fallback for testing, in production the controller provides it.
        memory_system = await SymbolicMemory.create(client, db_path)

    instance = cls(db_path, memory_system)
    await instance._init_db()
    await instance._load_knowledge()
    logging.info("KnowledgeBase initialized with %d items.", len(instance.knowledge))
    return instance

@asynccontextmanager
async def _db_connection(self) -> AsyncIterator[aiosqlite.Connection]:
    async with self._save_lock:
        conn = await aiosqlite.connect(self._db_path)
        try:
            yield conn
        finally:
            await conn.close()

async def _init_db(self) -> None:
    """Initializes the knowledge_base table if it doesn't exist."""
    async with self._db_connection() as db:
        await db.execute("""
            CREATE TABLE IF NOT EXISTS knowledge_base (
                id TEXT PRIMARY KEY,
                type TEXT NOT NULL,
                content TEXT NOT NULL,
                importance REAL NOT NULL,
                confidence REAL NOT NULL,
                created_at TEXT NOT NULL
            )
        """)
        await db.execute("CREATE INDEX IF NOT EXISTS idx_knowledge_type ON knowledge_base(type)")
        await db.commit()

async def _load_knowledge(self) -> None:
    """Loads all knowledge items from the database into memory."""
    async with self._db_connection() as db:
        async with db.execute("SELECT * FROM knowledge_base") as cursor:
            rows = await cursor.fetchall()
            for row in rows:
                item = KnowledgeItem(
                    id=row[0],
                    type=KnowledgeItemType(row[1]),
                    content=json.loads(row[2]),
                    importance=row[3],
                    confidence=row[4],
                    created_at=row[5]
                )
                self.knowledge[item.id] = item

async def add_knowledge(self, item: KnowledgeItem) -> None:
    """Adds a new knowledge item and persists it to the database."""
    if item.id in self.knowledge:
        logging.warning("Knowledge item %s already exists. Overwriting.", item.id)

    self.knowledge[item.id] = item
    async with self._db_connection() as db:
        await db.execute(
            "INSERT OR REPLACE INTO knowledge_base VALUES (?, ?, ?, ?, ?, ?)",
            (item.id, item.type.value, json.dumps(item.content), item.importance, item.confidence, item.created_at)
        )
        await db.commit()
    logging.info(f"Added knowledge: {item.type.name} - {item.id}")

def query_knowledge(self, query: str, limit: int = 5) -> List[KnowledgeItem]:
    """
    Queries the knowledge base using semantic search on the content.
    This leverages the main SymbolicMemory's embedding capabilities.
    """
    # For simplicity, we'll just search the content for now.
    # A more advanced version would embed the content of knowledge items.
    results = []
    query_lower = query.lower()
    for item in self.knowledge.values():
        if query_lower in str(item.content).lower():
            results.append(item)

    # Sort by importance and confidence
    results.sort(key=lambda x: (x.importance, x.confidence), reverse=True)
    return results[:limit]

--- FILE: symbolic_agi/llm_world_plugin.py ---

symbolic_agi/llm_world_plugin.py
import openai import os

class LLMWorldPlugin: def init(self, openai_api_key=None): self.client = openai.AsyncOpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY"))

async def generate_event(self, event_spec):
    prompt = f"Generate a world event based on: {event_spec}"
    resp = await self.client.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "user", "content": prompt}]
    )
    return resp.choices[0].message.content

async def update_world_state_async(self):
    import asyncio
    await asyncio.sleep(0.1)
    return {"status": "World state updated by LLM."}

--- FILE: symbolic_agi/long_term_memory.py ---

symbolic_agi/long_term_memory.py
import asyncio import json import logging import os from contextlib import asynccontextmanager from datetime import datetime, timezone from typing import Any, AsyncIterator, Dict, List, Optional, Set

import aiosqlite

from symbolic_agi import config from symbolic_agi.schemas import ActionStep, GoalModel, GoalStatus

class LongTermMemory: """ Manages the AGI's long-term goals with robust persistence, error handling, and intelligent goal lifecycle management. """

def __init__(self, db_path: str = config.DB_PATH):
    self._db_path = db_path
    self.goals: Dict[str, GoalModel] = {}
    self._save_lock = asyncio.Lock()
    self._db_pool: Optional[aiosqlite.Connection] = None
    self._goal_priority_cache: Dict[str, float] = {}
    self._last_cleanup = datetime.now(timezone.utc)

@classmethod
async def create(cls, db_path: str = config.DB_PATH) -> "LongTermMemory":
    """Factory method to create and initialize LongTermMemory."""
    instance = cls(db_path)
    await instance._init_db()
    await instance._load_goals()
    # await instance._periodic_cleanup() # This can be run less frequently
    return instance

@asynccontextmanager
async def _db_connection(self) -> AsyncIterator[aiosqlite.Connection]:
    """Context manager for safe database connections with proper error handling."""
    conn = None
    try:
        conn = await aiosqlite.connect(self._db_path)
        conn.row_factory = aiosqlite.Row
        yield conn
    except Exception as e:
        logging.error("Database connection error: %s", e, exc_info=True)
        raise
    finally:
        if conn:
            await conn.close()

async def _init_db(self) -> None:
    """Initializes the database and tables if they don't exist."""
    try:
        os.makedirs(os.path.dirname(self._db_path), exist_ok=True)
        async with self._db_connection() as db:
            await db.execute(
                """
                CREATE TABLE IF NOT EXISTS goals (
                    id TEXT PRIMARY KEY, description TEXT NOT NULL, sub_tasks TEXT,
                    status TEXT NOT NULL, mode TEXT NOT NULL, last_failure TEXT,
                    original_plan TEXT, failure_count INTEGER NOT NULL DEFAULT 0,
                    max_failures INTEGER NOT NULL DEFAULT 3, refinement_count INTEGER NOT NULL DEFAULT 0,
                    max_refinements INTEGER NOT NULL DEFAULT 3, created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP, priority REAL NOT NULL DEFAULT 0.5
                )
                """
            )
            await db.execute("CREATE INDEX IF NOT EXISTS idx_goals_status ON goals(status)")
            await db.commit()
            logging.info("Database initialized successfully at %s", self._db_path)
    except Exception as e:
        logging.error("Failed to initialize database: %s", e, exc_info=True)
        raise

async def _load_goals(self) -> None:
    """Loads active goals from the database with comprehensive error handling."""
    try:
        async with self._db_connection() as db:
            query = "SELECT * FROM goals WHERE status NOT IN ('completed', 'failed')"

            # CORRECTED DATABASE USAGE
            cursor = await db.execute(query)
            rows = await cursor.fetchall()

            for row_data in rows:
                row = dict(row_data) # Convert row object to dictionary
                try:
                    goal_dict = {
                        "id": row["id"], "description": row["description"],
                        "sub_tasks": [ActionStep.model_validate(t) for t in self._safe_json_loads(row["sub_tasks"], [])],
                        "status": row["status"], "mode": row["mode"], "last_failure": row["last_failure"],
                        "original_plan": [ActionStep.model_validate(t) for t in self._safe_json_loads(row["original_plan"], [])] if row["original_plan"] else None,
                        "failure_count": row["failure_count"], "max_failures": row["max_failures"],
                        "refinement_count": row["refinement_count"], "max_refinements": row["max_refinements"],
                    }
                    goal = GoalModel.model_validate(goal_dict)
                    self.goals[goal.id] = goal
                except Exception as e:
                    logging.error("Failed to load goal %s: %s", row.get("id", "unknown"), e)
            await cursor.close()
        logging.info("Successfully loaded %d goals from database", len(self.goals))
    except Exception as e:
        logging.error("Critical error loading goals from database: %s", e, exc_info=True)

async def add_goal(self, goal: GoalModel) -> None:
    """Adds a new goal to the long-term memory."""
    if goal.id in self.goals:
        logging.warning("Goal %s already exists, updating instead.", goal.id)
        await self.update_goal(goal)
        return

    self.goals[goal.id] = goal
    await self._save_goal_to_db(goal)
    logging.info("Successfully added goal: %s", goal.id)

def get_goal_by_id(self, goal_id: str) -> Optional[GoalModel]:
    """Retrieves a goal by its unique ID."""
    return self.goals.get(goal_id)

async def update_goal_status(self, goal_id: str, status: GoalStatus) -> None:
    """Updates the status of a goal."""
    if goal := self.goals.get(goal_id):
        goal.status = status
        await self._save_goal_to_db(goal)
    else:
        raise ValueError(f"Goal {goal_id} does not exist")

def get_active_goal(self) -> Optional[GoalModel]:
    """Retrieves the first active goal from the list."""
    for goal in self.goals.values():
        if goal.status == "active":
            return goal
    return None

async def complete_sub_task(self, goal_id: str) -> None:
    """Removes the first sub-task from a goal's plan."""
    if goal := self.goals.get(goal_id):
        if goal.sub_tasks:
            goal.sub_tasks.pop(0)
            await self._save_goal_to_db(goal)
        else:
            logging.warning("Attempted to complete sub-task for goal %s with no tasks", goal_id)

async def update_plan(self, goal_id: str, plan: List[ActionStep]) -> None:
    """Updates the plan (sub_tasks) for a goal."""
    if goal := self.goals.get(goal_id):
        goal.sub_tasks = plan
        await self._save_goal_to_db(goal)
    else:
        raise ValueError(f"Goal {goal_id} does not exist")

async def invalidate_plan(self, goal_id: str, reason: str) -> None:
    """Marks a goal as failed with a reason."""
    if goal := self.goals.get(goal_id):
        goal.status = "failed"
        goal.last_failure = reason
        await self._save_goal_to_db(goal)
    else:
        raise ValueError(f"Goal {goal_id} does not exist")

async def _save_goal_to_db(self, goal: GoalModel) -> None:
    """Helper method to save a single goal to the database."""
    async with self._db_connection() as db:
        await db.execute(
            """INSERT OR REPLACE INTO goals 
               (id, description, sub_tasks, status, mode, last_failure, original_plan, 
                failure_count, max_failures, refinement_count, max_refinements, updated_at) 
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (
                goal.id, goal.description, json.dumps([t.model_dump() for t in goal.sub_tasks]),
                goal.status, goal.mode, goal.last_failure,
                json.dumps([t.model_dump() for t in goal.original_plan]) if goal.original_plan else None,
                goal.failure_count, goal.max_failures, goal.refinement_count, goal.max_refinements,
                datetime.now(timezone.utc).isoformat()
            )
        )
        await db.commit()

async def increment_failure_count(self, goal_id: str) -> int:
    """Increments the failure count for a goal and returns the new count."""
    if goal := self.goals.get(goal_id):
        goal.failure_count += 1
        await self._save_goal_to_db(goal)
        return goal.failure_count
    return 0

async def increment_refinement_count(self, goal_id: str) -> int:
    """Increments the refinement count for a goal and returns the new count."""
    if goal := self.goals.get(goal_id):
        goal.refinement_count += 1
        await self._save_goal_to_db(goal)
        return goal.refinement_count
    return 0

def _safe_json_loads(self, data: Optional[str], default: Any = None) -> Any:
    """Safely loads JSON data with error handling."""
    if not data:
        return default
    try:
        return json.loads(data)
    except (json.JSONDecodeError, TypeError):
        logging.warning("Failed to parse JSON data, returning default: %s", data[:100])
        return default

async def _periodic_cleanup(self) -> None:
    """Performs periodic cleanup of old completed goals."""
    # This is a placeholder for a more sophisticated cleanup strategy.
    pass

def _calculate_goal_priority(self) -> float:
    """Calculates dynamic priority based on various factors."""
    # Placeholder for a more sophisticated priority calculation.
    return 0.5

async def update_goal(self, goal: GoalModel) -> None:
    """Updates an existing goal."""
    if goal.id not in self.goals:
        raise ValueError(f"Goal {goal.id} does not exist, cannot update")
    self.goals[goal.id] = goal
    await self._save_goal_to_db(goal)

--- FILE: symbolic_agi/message_bus.py ---

symbolic_agi/message_bus.py
import asyncio import json import logging from typing import Dict, Optional import redis.asyncio as redis_async # Use the async Redis client

from . import config from .schemas import MessageModel

class RedisMessageBus: """A central message bus for inter-agent communication using Redis Pub/Sub."""

def __init__(self) -> None:
    self.redis_client: Optional[redis_async.Redis] = None
    # Remove the single pubsub instance - each listener needs its own
    self.listener_tasks: Dict[str, asyncio.Task] = {}
    self.agent_queues: Dict[str, asyncio.Queue[Optional[MessageModel]]] = {}
    self._background_tasks: set[asyncio.Task] = set()
    self._initialized = False

async def _initialize(self) -> None:
    """Initialize Redis connection."""
    if self._initialized:
        return
        
    try:
        self.redis_client = redis_async.Redis(
            host=config.REDIS_HOST,
            port=config.REDIS_PORT,
            decode_responses=True
        )
        # Test the connection
        await self.redis_client.ping()
        self._initialized = True
        logging.info("[MessageBus] Connected to Redis at %s:%s", config.REDIS_HOST, config.REDIS_PORT)
    except Exception as e:
        logging.error("[MessageBus] Failed to connect to Redis: %s", e)
        raise

@property
def is_running(self) -> bool:
    """Check if the message bus is initialized and running."""
    return self._initialized and self.redis_client is not None

def subscribe(self, agent_id: str) -> asyncio.Queue[Optional[MessageModel]]:
    """Allows an agent to subscribe to the bus, receiving its own message queue."""
    if agent_id not in self.agent_queues:
        if not self.is_running:
            # Lazy initialization - save task to prevent garbage collection
            init_task = asyncio.create_task(self._initialize())
            self._background_tasks.add(init_task)
            init_task.add_done_callback(self._background_tasks.discard)

        queue: asyncio.Queue[Optional[MessageModel]] = asyncio.Queue()
        self.agent_queues[agent_id] = queue
        self.listener_tasks[agent_id] = asyncio.create_task(
            self._listen(agent_id, queue)
        )
        logging.info(f"[MessageBus] Agent '{agent_id}' has subscribed.")
    return self.agent_queues[agent_id]

async def _listen(self, agent_id: str, queue: asyncio.Queue[Optional[MessageModel]]) -> None:
    """Listens for messages on a Redis channel and puts them in a queue."""
    # Wait for initialization
    while not self.is_running:
        await asyncio.sleep(0.1)

    # Create a dedicated pubsub instance for this listener
    pubsub = self.redis_client.pubsub()
    
    try:
        await pubsub.subscribe(agent_id, "broadcast")
        logging.debug(f"[MessageBus] Listener for '{agent_id}' started.")
        
        async for message in pubsub.listen():
            if message and message["type"] == "message":
                data = json.loads(message["data"])
                msg_model = MessageModel.model_validate(data)
                await queue.put(msg_model)
    except asyncio.CancelledError:
        logging.info(f"[MessageBus] Listener for '{agent_id}' cancelled.")
        await pubsub.close()
        raise  # Re-raise CancelledError for proper async cleanup
    except Exception as e:
        logging.error(f"[MessageBus] Listener for '{agent_id}' failed: {e}", exc_info=True)
        await pubsub.close()

async def publish(self, message: MessageModel) -> None:
    """Publishes a message to a specific agent's channel."""
    if not self.redis_client:
        logging.error("[MessageBus] Redis client not initialized")
        return
        
    await self.redis_client.publish(message.receiver_id, message.model_dump_json())
    logging.info(
        "[MessageBus] Published message of type '%s' from '%s' to '%s'.",
        message.message_type,
        message.sender_id,
        message.receiver_id,
    )

async def broadcast(self, message: MessageModel) -> None:
    """Broadcasts a message to all agents."""
    if not self.redis_client:
        logging.error("[MessageBus] Redis client not initialized")
        return
        
    await self.redis_client.publish("broadcast", message.model_dump_json())
    logging.info(
        "[MessageBus] Broadcasted message of type '%s' from '%s'.",
        message.message_type,
        message.sender_id,
    )

async def shutdown(self) -> None:
    """Initiates the shutdown of the message bus."""
    # Cancel all listener tasks
    for task in self.listener_tasks.values():
        task.cancel()
    await asyncio.gather(*self.listener_tasks.values(), return_exceptions=True)
    
    # Cancel background tasks
    for task in self._background_tasks:
        task.cancel()
    await asyncio.gather(*self._background_tasks, return_exceptions=True)
    
    try:
        if self.redis_client:
            await self.redis_client.aclose()
    except Exception as e:
        logging.warning("[MessageBus] Error during shutdown: %s", e)
    
    logging.info("[MessageBus] Shutdown complete.")

--- FILE: symbolic_agi/meta_cognition.py ---

symbolic_agi/meta_cognition.py
import asyncio import json import asyncio import logging import random from collections import deque from datetime import datetime, timezone, timedelta from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple

from . import config from .schemas import GoalModel, MemoryEntryModel, MemoryType, MetaEventModel

if TYPE_CHECKING: from .agi_controller import SymbolicAGI

class MetaCognitionUnit: """Handles the AGI's self-reflection and meta-learning capabilities."""

agi: "SymbolicAGI"
meta_memory: deque[MetaEventModel]
active_theories: List[str]
self_model: Dict[str, Any]
_meta_task: Optional[asyncio.Task[None]]
meta_upgrade_methods: List[Tuple[Callable[..., Any], float]]
_last_trust_reheal: datetime

def __init__(self, agi: "SymbolicAGI") -> None:
    self.agi = agi
    self.meta_memory = deque(maxlen=1000)
    self.active_theories = []
    self.self_model = {}
    self._meta_task = None
    self._last_trust_reheal = datetime.now(timezone.utc)

    self.meta_upgrade_methods = [
        (self.generate_goal_from_drives, 1.0),
        (self.agi.introspector.prune_mutations, 0.5),
        (self.compress_episodic_memory, 1.0),
        (self.agi.introspector.daydream, 0.3),
        (self.learn_from_human_experience, 0.4),
        (self.propose_and_run_self_experiment, 0.6),
        (self.memory_forgetting_routine, 1.0),
        (self.generative_creativity_mode, 0.7),
        (self.motivational_drift, 0.2),
        (self.agi.introspector.simulate_inner_debate, 0.5),
        (self.autonomous_explainer_routine, 0.9),
        (self.meta_cognition_self_update_routine, 1.0),
        (self.recover_cognitive_energy_routine, 1.0),
        (self.review_learned_skills, 0.4),
        (self.document_undocumented_skills, 0.6),
    ]
    if self.agi.consciousness and hasattr(self.agi.consciousness, "meta_reflect"):
        self.meta_upgrade_methods.append(
            (self.agi.consciousness.meta_reflect, 0.9)
        )

def trust_rehealing_cron(self) -> None:
    """
    Periodically and slowly restores trust for all agents towards the neutral
    baseline, rewarding idle agents and preventing trust scores from permanently
    staying at extremes.
    """
    now = datetime.now(timezone.utc)
    if now - self._last_trust_reheal < timedelta(hours=config.TRUST_REHEAL_INTERVAL_HOURS):
        return

    logging.info("META-TASK: Running nightly trust re-healing cron job.")
    all_agents = self.agi.agent_pool.get_all()
    for agent_info in all_agents:
        agent_name = agent_info["name"]
        state = agent_info.get("state", {})
        current_score = state.get("trust_score", config.INITIAL_TRUST_SCORE)

        # Move score towards the neutral baseline (INITIAL_TRUST_SCORE)
        healing_adjustment = (config.INITIAL_TRUST_SCORE - current_score) * config.TRUST_REHEAL_RATE
        new_score = current_score + healing_adjustment

        self.agi.agent_pool.update_trust_score(agent_name, new_score, last_used=False)
        logging.debug("Healed trust for agent '%s' from %.3f to %.3f", agent_name, current_score, new_score)
    self._last_trust_reheal = now

async def document_undocumented_skills(self) -> None:
    """
    Finds learned skills that do not have an explanation in memory and
    creates a goal to document them.
    """
    if self.agi.ltm.get_active_goal():
        return

    all_skill_names = {skill.name for skill in self.agi.skills.skills.values()}
    if not all_skill_names:
        return

    explained_skills = {
        mem.content.get("skill_name")
        for mem in self.agi.memory.memory_map.values()
        if mem.type == "skill_explanation" and "skill_name" in mem.content
    }

    undocumented_skills = all_skill_names - explained_skills
    if not undocumented_skills:
        logging.info("Meta-task: All learned skills are already documented.")
        return

    skill_to_document = random.choice(list(undocumented_skills))
    logging.critical(
        "META-TASK: Found undocumented skill '%s'. Creating goal to explain it.",
        skill_to_document,
    )

    goal_description = (
        f"Create a detailed, human-readable explanation for the learned skill "
        f"named '{skill_to_document}' and save it to memory."
    )
    new_goal = GoalModel(description=goal_description, sub_tasks=[])
    await self.agi.ltm.add_goal(new_goal)
    await self.record_meta_event(
        "meta_insight",
        {
            "trigger": "document_undocumented_skills",
            "skill_name": skill_to_document,
        },
    )

async def review_learned_skills(self) -> None:
    """
    Periodically reviews a learned skill for efficiency and triggers a goal to improve it.
    """
    if self.agi.ltm.get_active_goal():
        return

    all_skills = list(self.agi.skills.skills.values())
    if not all_skills:
        logging.info("Meta-task: No learned skills to review.")
        return

    skill_to_review = random.choice(all_skills)
    logging.critical(
        "META-TASK: Initiating autonomous review of skill '%s'.",
        skill_to_review.name,
    )

    goal_description = (
        f"Review and improve the learned skill named '{skill_to_review.name}' "
        f"(ID: {skill_to_review.id}). Analyze its action sequence for "
        "inefficiencies and update it if a better plan can be formulated."
    )
    new_goal = GoalModel(description=goal_description, sub_tasks=[])

    await self.agi.ltm.add_goal(new_goal)
    await self.record_meta_event(
        "meta_insight",
        {"trigger": "review_learned_skills", "skill_name": skill_to_review.name},
    )

async def record_meta_event(self, kind: MemoryType, data: Any) -> None:
    evt = MetaEventModel(type=kind, data=data)
    self.meta_memory.append(evt)
    if kind in {"meta_insight", "critical_error", "meta_learning"}:
        await self.agi.memory.add_memory(
            MemoryEntryModel(type=kind, content={"meta": data}, importance=0.95)
        )

def update_self_model(self, summary: Dict[str, Any]) -> None:
    self.self_model.update(summary)

async def compress_episodic_memory(self) -> None:
    if hasattr(self.agi.memory, "consolidate_memories"):
        window_seconds = int(
            self.agi.cfg.memory_compression_window.total_seconds()
        )
        await self.agi.memory.consolidate_memories(window_seconds=window_seconds)
    else:
        logging.warning(
            "'consolidate_memories' method not found on memory object."
        )

async def generate_goal_from_drives(self) -> None:
    if not self.agi.consciousness or not hasattr(self.agi.consciousness, "drives"):
        return

    drives = self.agi.consciousness.drives
    if not drives:
        return

    strongest_drive: str = max(drives, key=lambda k: drives[k])
    weakest_drive: str = min(drives, key=lambda k: drives[k])
    if drives[strongest_drive] - drives[weakest_drive] < 0.2:
        return

    logging.critical(
        "DRIVE IMBALANCE DETECTED: Strongest='%s', Weakest='%s'. Engaging goal "
        "generation.",
        strongest_drive,
        weakest_drive,
    )
    prompt = (
        "You are the core volition of a conscious AGI. Your current internal "
        f"drives are:\n{json.dumps(drives, indent=2)}\n\n"
        f"Your strongest drive is '{strongest_drive}', and your weakest is "
        f"'{weakest_drive}'. This imbalance suggests a need.\n"
        "Formulate a single, high-level goal that would help satisfy the "
        "strongest drive or address the weakest one.\nThe goal should be a "
        "creative, interesting, and valuable long-term project.\n"
        "Respond with ONLY the single sentence describing the goal."
    )
    try:
        goal_description = await self.agi.introspector.llm_reflect(prompt)
        if goal_description and "failed" not in goal_description.lower():
            new_goal = GoalModel(description=goal_description.strip(), sub_tasks=[])
            await self.agi.ltm.add_goal(new_goal)
            logging.critical(
                "AUTONOMOUS GOAL CREATED: '%s'", new_goal.description
            )
            await self.record_meta_event(
                "goal",
                {"source": "drive_imbalance", "goal": new_goal.description},
            )
    except Exception as e:
        await self.record_meta_event(
            "critical_error",
            {"task": "generate_goal_from_drives", "error": str(e)},
        )

async def learn_from_human_experience(self) -> None:
    if (
        self.agi.identity
        and self.agi.cfg.social_interaction_threshold
        and datetime.now(timezone.utc)
        - self.agi.identity.last_interaction_timestamp
        > self.agi.cfg.social_interaction_threshold
    ):
        recent_memories = await self.agi.memory.get_recent_memories(n=5)
        recent = [
            m.content
            for m in recent_memories
            if m.type == "action_result"
        ]
        prompt = (
            "I need to understand humans better. Craft an open-ended question "
            f"for the user related to: {recent}. Produce a plan with a single "
            "'respond_to_user' action."
        )

        result = await self.agi.introspector.symbolic_loop(
            {
                "user_input": prompt,
                "agi_self_model": self.agi.identity.get_self_model(),
            },
            json.dumps(self.agi.agent_pool.get_all_action_definitions(), indent=2),
        )
        if plan_data := result.get("plan"):
            plan = await self.agi.planner.decompose_goal_into_plan(
                str(plan_data), ""
            )
            await self.agi.execute_plan(plan.plan)
            self.agi.identity.last_interaction_timestamp = datetime.now(
                timezone.utc
            )

async def propose_and_run_self_experiment(self) -> None:
    plan_str = await self.agi.introspector.llm_reflect(
        "Propose a self-experiment to test a hypothesis about my cognition."
    )
    await self.agi.memory.add_memory(
        MemoryEntryModel(
            type="self_experiment",
            content=self.agi.wrap_content(plan_str),
            importance=0.9,
        )
    )

def memory_forgetting_routine(self) -> None:
    threshold = self.agi.cfg.memory_forgetting_threshold
    now_ts = datetime.now(timezone.utc)
    if not self.agi.memory.memory_map:
        return

    initial_count = len(self.agi.memory.memory_map)
    to_forget_ids = {
        db_id
        for db_id, m in self.agi.memory.memory_map.items()
        if m.importance < threshold
        or datetime.fromisoformat(m.timestamp)
        < now_ts - self.agi.cfg.memory_compression_window
    }
    if to_forget_ids:
        # This logic would now involve a DB call to delete these IDs
        logging.info(
            "Forgetting %d memories. Count changed from %d to %d.",
            len(to_forget_ids),
            initial_count,
            len(self.agi.memory.memory_map) - len(to_forget_ids),
        )
        # Memory deletion implementation delegated to SymbolicMemory module
        logging.info("Memory forgetting routine completed - deletion logic needs implementation")

async def motivational_drift(self) -> None:
    if self.agi.consciousness:
        for k in self.agi.consciousness.drives:
            current_value = self.agi.consciousness.drives[k]
            new_value = current_value + random.uniform(
                -self.agi.cfg.motivational_drift_rate,
                self.agi.cfg.motivational_drift_rate,
            )
            self.agi.consciousness.set_drive(k, new_value)
        await self.agi.identity.save_profile()
        await self.agi.memory.add_memory(
            MemoryEntryModel(
                type="motivation_drift",
                content=self.agi.identity.value_system,
                importance=0.4,
            )
        )
    else:
        logging.warning("Consciousness not active, skipping motivational drift.")

async def generative_creativity_mode(self) -> None:
    creative_ideas = await self.agi.introspector.llm_reflect(
        "Brainstorm three wild inventions."
    )
    await self.agi.memory.add_memory(
        MemoryEntryModel(
            type="creativity",
            content=self.agi.wrap_content(creative_ideas),
            importance=0.9,
        )
    )

async def autonomous_explainer_routine(self) -> None:
    explanations = await self.agi.introspector.llm_reflect(
        "Review my last 5 actions and explain WHY. Return JSON."
    )
    await self.record_meta_event("self_explanation", explanations)

async def meta_cognition_self_update_routine(self) -> None:
    recent_events = list(self.meta_memory)[-5:]
    events_data: List[Dict[str, Any]] = [
        e.model_dump() for e in recent_events
    ]
    prompt = (
        f"Given meta-events: {json.dumps(events_data)}, summarize my cognitive "
        "state and propose a hypothesis. Respond as JSON with keys 'summary' and "
        "'hypothesis'."
    )
    try:
        summary_str = await self.agi.introspector.llm_reflect(prompt)
        summary = json.loads(summary_str)
        self.update_self_model(summary)
        await self.record_meta_event("meta_learning", summary)
    except Exception as e:
        await self.record_meta_event(
            "critical_error",
            {"task": "meta_cognition_self_update", "error": str(e)},
        )

def recover_cognitive_energy_routine(self) -> None:
    self.agi.identity.recover_energy(amount=self.agi.cfg.energy_regen_amount)
    if self.agi.identity.cognitive_energy < self.agi.identity.max_energy * 0.2:
        self.agi.identity.recover_energy(
            amount=self.agi.cfg.energy_regen_amount * 2
        )

async def run_background_tasks(self) -> None:
    """Start the background meta-cognition tasks."""
    if self._meta_task is None:
        logging.info("MetaCognitionUnit: Starting background meta-tasks...")
        self._meta_task = asyncio.create_task(self._run_loop())
        # Wait a moment to ensure the task has started
        await asyncio.sleep(0.1)
    else:
        logging.warning("MetaCognitionUnit: Background meta-tasks already started.")

async def _run_loop(self) -> None:
    while True:
        try:
            await asyncio.sleep(self.agi.cfg.meta_task_sleep_seconds)
            methods, weights = zip(*self.meta_upgrade_methods, strict=False)
            funcs_to_run = random.choices(methods, weights=weights, k=2)
            
            # Run trust healing in sync since it's no longer async
            self.trust_rehealing_cron()
            
            # Run other tasks async
            await asyncio.gather(
                *(self._safe_run_meta_task(f) for f in funcs_to_run)
            )
        except asyncio.CancelledError:
            logging.info("run_background_meta_tasks received cancel signal.")
            break
        except Exception as e:
            logging.error("Background loop error: %s", e, exc_info=True)

async def _safe_run_meta_task(self, func: Callable[..., Any]) -> None:
    try:
        logging.info("Meta-task: %s", func.__name__)
        if asyncio.iscoroutinefunction(func):
            if (
                self.agi.consciousness
                and func == self.agi.consciousness.meta_reflect
            ):
                await asyncio.wait_for(
                    func(self.agi.identity, self.agi.memory),
                    timeout=self.agi.cfg.meta_task_timeout,
                )
            else:
                await asyncio.wait_for(
                    func(), timeout=self.agi.cfg.meta_task_timeout
                )
        else:
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(None, func)
    except Exception as e:
        await self.record_meta_event(
            "critical_error", {"task": func.__name__, "error": str(e)}
        )

async def shutdown(self) -> None:
    if self._meta_task and not self._meta_task.done():
        self._meta_task.cancel()
        try:
            await self._meta_task
        except asyncio.CancelledError:
            logging.info("Background meta-task successfully cancelled.")

--- FILE: symbolic_agi/metrics.py ---

symbolic_agi/metrics.py
""" Central registry for all Prometheus metrics used in the SymbolicAGI project. """

from prometheus_client import Counter, Gauge, Histogram

--- LLM API Metrics ---
LLM_TOKEN_USAGE = Counter( "symbolic_agi_llm_token_usage_total", "Total number of LLM tokens used", ["model", "type"], # 'type' can be 'prompt' or 'completion' )

API_CALL_LATENCY = Histogram( "symbolic_agi_api_call_latency_seconds", "Latency of API calls to LLM providers", ["model"], )

API_CALL_ERRORS = Counter( "symbolic_agi_api_call_errors_total", "Total number of failed API calls", ["model", "error_type"], )

--- AGI Core Metrics ---
AGI_CYCLE_DURATION = Histogram( "symbolic_agi_cycle_duration_seconds", "Duration of a single autonomous cognitive cycle", )

ACTIVE_GOALS = Gauge( "symbolic_agi_active_goals", "Current number of active goals in the LongTermMemory" )

AGENT_TASKS_RUNNING = Gauge( "symbolic_agi_agent_tasks_running", "Current number of running specialist agent tasks", )

AGENT_TRUST = Gauge( "symbolic_agi_agent_trust_score", "Current trust score of a specialist agent", ["agent_name", "persona"], )

--- Memory Metrics ---
MEMORY_ENTRIES = Gauge( "symbolic_agi_memory_entries_total", "Total number of entries in the symbolic memory", )

FAISS_INDEX_VECTORS = Gauge( "symbolic_agi_faiss_index_vectors_total", "Total number of vectors in the FAISS index", )

EMBEDDING_BUFFER_FLUSHES = Counter( "symbolic_agi_embedding_buffer_flushes_total", "Total number of times the embedding buffer has been flushed", )

EMBEDDING_FLUSH_LATENCY_SECONDS = Histogram( "symbolic_agi_embedding_flush_latency_seconds", "Time taken to flush the embedding buffer", )

--- FILE: symbolic_agi/micro_world.py ---

symbolic_agi/micro_world.py
import asyncio import inspect import json import logging import os import random from typing import Any, Callable, Dict, List, Optional, cast

from . import config from .api_client import monitored_chat_completion

Constants for common error messages and statuses
STATUS_SUCCESS = "success" STATUS_FAILURE = "failure" ERROR_AGENT_OR_OBJECT_NOT_FOUND = "Agent or object not found." ERROR_AGENT_NOT_FOUND = "Agent not found." ERROR_OBJECT_NOT_FOUND = "Object not found."

class MicroWorld: """A rich, multi-agent, multi-room simulated world with persistent state."""

room_map: Dict[str, Dict[str, Any]]
state: Dict[str, List[Any]]
state_file_path: str

def __init__(self: "MicroWorld"):
    self.room_map = {
        "hallway": {
            "desc": "A main hallway with doors to room1 and room2.",
            "exits": ["room1", "room2"],
        },
        "room1": {
            "desc": "A small room with a locked chest and sticks.",
            "exits": ["hallway"],
        },
        "room2": {
            "desc": "A stone room with a heavy rock and a notice board.",
            "exits": ["hallway"],
        },
    }
    self.state_file_path = os.path.join(
        config.WORKSPACE_DIR, "micro_world_state.json"
    )
    self.state = self._load_state()

def _get_default_state(self) -> Dict[str, List[Any]]:
    """Returns the default state for a new world."""
    return {
        "agents": [
            {"name": "SymbolicAGI", "location": "hallway", "inventory": []},
            {"name": "Alice", "location": "room1", "inventory": ["Stick"]},
            {"name": "Bob", "location": "room2", "inventory": ["Rock"]},
            {"name": "User", "location": "hallway", "inventory": []},
        ],
        "objects": [
            {
                "name": "Chest",
                "location": "room1",
                "state": "locked",
                "type": "chest",  # Add this line
                "description": "A heavy wooden chest with a lock.",
            },
            {"name": "Rock", "location": "room2", "description": "A rough gray rock."},
            {
                "name": "Stick",
                "location": "room1",
                "description": "A sturdy wooden stick.",
            },
            {"name": "Key", "location": "room2", "description": "A small iron key."},
            {
                "name": "NoticeBoard",
                "location": "room2",
                "description": "A faded notice board covered with old papers.",
            },
        ],
        "doors": [
            {"from": "hallway", "to": "room1", "locked": False},
            {"from": "hallway", "to": "room2", "locked": False},
        ],
        "rooms": list(self.room_map.keys()),
        "events": [],
    }

def _load_state(self) -> Dict[str, List[Any]]:
    """Loads the world state from a JSON file, or creates a default state."""
    if os.path.exists(self.state_file_path):
        try:
            with open(self.state_file_path, "r", encoding="utf-8") as f:
                logging.info(
                    "Loading persistent MicroWorld state from %s",
                    self.state_file_path,
                )
                return cast(Dict[str, List[Any]], json.load(f))
        except (json.JSONDecodeError, TypeError):
            logging.error("Could not load MicroWorld state, creating a new one.")

    logging.info("No persistent MicroWorld state found, creating default state.")
    default_state = self._get_default_state()
    self._save_state(default_state)
    return default_state

def _save_state(self, state_data: Dict[str, Any]) -> None:
    """Saves the current world state to a JSON file."""
    try:
        os.makedirs(os.path.dirname(self.state_file_path), exist_ok=True)
        with open(self.state_file_path, "w", encoding="utf-8") as f:
            json.dump(state_data, f, indent=4)
    except Exception as e:
        logging.error("Failed to save MicroWorld state: %s", e, exc_info=True)

def save_state(self) -> None:
    """Public method to save the current world state."""
    self._save_state(self.state)

def add_agent(
    self: "MicroWorld",
    name: str,
    location: str = "hallway",
    inventory: Optional[List[str]] = None,
) -> None:
    """Adds a new agent to the world."""
    if inventory is None:
        inventory = []
    self.state["agents"].append(
        {"name": name, "location": location, "inventory": inventory}
    )
    self._save_state(self.state)

def get_agent(self: "MicroWorld", name: str) -> Optional[Dict[str, Any]]:
    """Retrieves an agent by name."""
    return next(
        (agent for agent in self.state["agents"] if agent["name"] == name), None
    )

def get_object(self: "MicroWorld", object_name: str) -> Optional[Dict[str, Any]]:
    """Retrieves an object by name."""
    return next(
        (obj for obj in self.state["objects"] if obj["name"] == object_name), None
    )

def room_agents(self: "MicroWorld", room: str) -> List[Dict[str, Any]]:
    """Returns agents in a specific room."""
    return [a for a in self.state["agents"] if a["location"] == room]

def room_objects(self: "MicroWorld", room: str) -> List[Dict[str, Any]]:
    """Returns objects in a specific room."""
    return [o for o in self.state["objects"] if o["location"] == room]

async def tick(self: "MicroWorld") -> None:
    """Simulate time passing in the world (random agent wandering)."""
    state_changed = False
    try:
        # Yield control to allow other async operations
        await asyncio.sleep(0)
        
        if random.random() < 0.1:
            agent = random.choice(self.state["agents"])
            current_location_data = self.room_map.get(agent["location"])
            if current_location_data:
                available_exits = current_location_data.get("exits")
                if available_exits:
                    new_location = random.choice(available_exits)
                    is_locked = any(
                        d
                        for d in self.state["doors"]
                        if d["from"] == agent["location"]
                        and d["to"] == new_location
                        and d["locked"]
                    )
                    if not is_locked:
                        agent["location"] = new_location
                        self.state["events"].append(
                            {
                                "event_type": "wander",
                                "agent": agent["name"],
                                "to": new_location,
                            }
                        )
                        state_changed = True
                    else:
                        logging.debug(
                            "Agent %s tried to wander to %s but the door was locked.",
                            agent["name"],
                            new_location,
                        )

    except Exception as e:
        logging.error("World tick error: %s", e)
    finally:
        if state_changed:
            self._save_state(self.state)

async def perform_action(
    self: "MicroWorld", action: str, **kwargs: Any
) -> Dict[str, Any]:
    """
    Executes a world action, validates parameters, and saves the state on success.
    """
    try:
        method_to_call: Optional[Callable[..., Any]] = getattr(
            self, f"_action_{action}", None
        )

        if not method_to_call:
            return {
                "status": STATUS_FAILURE,
                "description": f"World action '{action}' not found.",
            }

        sig = inspect.signature(method_to_call)
        required_params = {
            p.name
            for p in sig.parameters.values()
            if p.default is inspect.Parameter.empty
            and p.name not in ["self", "kwargs"]
        }
        provided_params = set(kwargs.keys())

        if not required_params.issubset(provided_params):
            missing = required_params - provided_params
            error_msg = (
                f"World action '{action}' missing required parameters: "
                f"{', '.join(missing)}."
            )
            logging.error(error_msg)
            return {"status": STATUS_FAILURE, "description": error_msg}

        result: Any
        if asyncio.iscoroutinefunction(method_to_call):
            result = await method_to_call(**kwargs)
        else:
            result = method_to_call(**kwargs)

        self.state["events"].append(
            {"action": action, "params": kwargs, "result": result}
        )
        logging.info("WORLD ACTION: %s with %s -> %s", action, kwargs, result)

        if result.get("status") == STATUS_SUCCESS:
            self._save_state(self.state)

        return cast(Dict[str, Any], result)
    except Exception as e:
        err_msg = f"Error performing action '{action}': {type(e).__name__}: {e}"
        logging.error(err_msg, exc_info=True)
        return {"status": STATUS_FAILURE, "description": err_msg}

# ========== Actions ==========

def _action_move(
    self: "MicroWorld", agent_name: str, new_location: str
) -> Dict[str, Any]:
    """Moves an agent to a new location if possible."""
    agent = self.get_agent(agent_name)
    if not agent:
        return {"status": STATUS_FAILURE, "description": f"Agent '{agent_name}' not found."}

    current_room_exits = self.room_map.get(agent["location"], {}).get("exits", [])
    if new_location not in current_room_exits:
        return {
            "status": STATUS_FAILURE,
            "description": (
                f"Cannot move from {agent['location']} to {new_location}. "
                "No direct exit."
            ),
        }

    for door in self.state["doors"]:
        if (
            door["from"] == agent["location"]
            and door["to"] == new_location
            and door["locked"]
        ):
            return {
                "status": STATUS_FAILURE,
                "description": f"The door to {new_location} is locked.",
            }

    agent["location"] = new_location
    return {
        "status": STATUS_SUCCESS,
        "description": f"{agent_name} moves to {new_location}.",
    }

def _action_read(
    self: "MicroWorld", object_name: str, agent_name: str = "SymbolicAGI"
) -> Dict[str, Any]:
    """Allows an agent to read an object's description."""
    agent = self.get_agent(agent_name)
    obj = self.get_object(object_name)
    if not agent:
        return {"status": STATUS_FAILURE, "description": f"Agent '{agent_name}' not found."}
    if not obj:
        return {"status": STATUS_FAILURE, "description": f"Object '{object_name}' not found."}
    if agent["location"] != obj.get("location"):
        return {
            "status": STATUS_FAILURE,
            "description": f"{agent_name} is not in the same location as {object_name}.",
        }
    desc = obj.get("description", f"You see a {object_name}.")
    details = f"State: {obj.get('state', 'normal')}" if "state" in obj else ""
    return {
        "status": STATUS_SUCCESS,
        "description": f"{agent_name} reads {object_name}: {desc} {details}".strip(),
    }

def _action_pickup(
    self: "MicroWorld", agent_name: str, object_name: str
) -> Dict[str, Any]:
    """Allows an agent to pick up an object."""
    agent = self.get_agent(agent_name)
    obj = self.get_object(object_name)
    if not agent or not obj:
        return {"status": STATUS_FAILURE, "description": ERROR_AGENT_OR_OBJECT_NOT_FOUND}
    if agent["location"] != obj.get("location"):
        return {
            "status": STATUS_FAILURE,
            "description": f"{object_name} is not in the same room as {agent_name}.",
        }

    if obj.get("state") == "fixed":
        return {
            "status": STATUS_FAILURE,
            "description": f"{object_name} cannot be picked up.",
        }

    agent["inventory"].append(object_name)
    obj["location"] = "inventory"
    return {
        "status": STATUS_SUCCESS,
        "description": f"{agent_name} picked up {object_name}.",
    }

def _action_drop(
    self: "MicroWorld", agent_name: str, object_name: str
) -> Dict[str, Any]:
    """Allows an agent to drop an item from inventory."""
    agent = self.get_agent(agent_name)
    obj = self.get_object(object_name)
    if not agent or not obj:
        return {"status": STATUS_FAILURE, "description": ERROR_AGENT_OR_OBJECT_NOT_FOUND}
    if object_name not in agent["inventory"]:
        return {
            "status": STATUS_FAILURE,
            "description": f"{agent_name} does not have {object_name} in inventory.",
        }
    agent["inventory"].remove(object_name)
    obj["location"] = agent["location"]
    return {
        "status": STATUS_SUCCESS,
        "description": f"{agent_name} dropped {object_name} in {agent['location']}.",
    }

def _action_open(
    self: "MicroWorld", agent_name: str, object_name: str
) -> Dict[str, Any]:
    """Allows an agent to open an object (e.g., a chest)."""
    agent = self.get_agent(agent_name)
    obj = self.get_object(object_name)
    if not agent or not obj:
        return {"status": STATUS_FAILURE, "description": ERROR_AGENT_OR_OBJECT_NOT_FOUND}
    
    # Debug logging
    logging.info(f"DEBUG: Agent {agent_name} location: {agent.get('location')}")
    logging.info(f"DEBUG: Object {object_name} location: {obj.get('location')}")
    
    if agent["location"] != obj.get("location"):
        return {
            "status": STATUS_FAILURE,
            "description": f"{agent_name} is not in the same location as {object_name}. Agent in {agent['location']}, object in {obj.get('location')}.",
        }

    # Handle the Chest specifically
    if object_name == "Chest":
        if obj.get("state") == "locked":
            if "Key" in agent["inventory"]:
                obj["state"] = "unlocked"
                return {
                    "status": STATUS_SUCCESS,
                    "description": "Unlocked the Chest with the Key!",
                }
            else:
                return {
                    "status": STATUS_FAILURE,
                    "description": "The Chest is locked. You need a Key.",
                }
        elif obj.get("state") == "unlocked":
            obj["state"] = "open"
            return {
                "status": STATUS_SUCCESS,
                "description": "Opened the Chest.",
            }
        elif obj.get("state") == "open":
            return {"status": STATUS_SUCCESS, "description": "The Chest is already open."}
        else:
            return {
                "status": STATUS_FAILURE,
                "description": f"The Chest state is {obj.get('state')} and cannot be opened.",
            }

    return {
        "status": STATUS_FAILURE,
        "description": f"{object_name} cannot be opened or is already open.",
    }

async def _action_ask(
    self: "MicroWorld", asking_agent: str, target_agent: str, question: str
) -> Dict[str, Any]:
    """Allows an agent to ask another agent (or user) a question."""
    agent = self.get_agent(asking_agent)
    target = self.get_agent(target_agent)
    if not agent or not target:
        return {
            "status": STATUS_FAILURE,
            "description": "Asking agent or target agent not found.",
        }
    if agent["location"] != target["location"]:
        return {
            "status": STATUS_FAILURE,
            "description": f"{target_agent} is not in the same location as {asking_agent}.",
        }

    if target_agent.lower() == "user":
        return {
            "status": STATUS_SUCCESS,
            "response_text": f"{asking_agent} asked you: {question}",
        }

    try:
        prompt = (
            f"{target_agent} is being asked a question by {asking_agent}.\n"
            f"World state: {json.dumps(self.state)}\n"
            f"Question: {question}\n"
            f"Answer in character as {target_agent} and keep it concise:"
        )
        resp = await monitored_chat_completion(
            role="tool_action", messages=[{"role": "system", "content": prompt}]
        )
        answer = (
            resp.choices[0].message.content.strip()
            if resp.choices and resp.choices[0].message.content
            else "..."
        )
    except Exception as e:
        logging.error("LLM for _action_ask error: %s", e)
        answer = (
            f"{target_agent} says: I don't know yet, but I'll try to help next time!"
        )
    return {
        "status": STATUS_SUCCESS,
        "response_text": f"{asking_agent} asked {target_agent}: {question}\n{answer}",
    }

def _action_look(self: "MicroWorld", agent_name: str) -> Dict[str, Any]:
    """Allows an agent to observe its current surroundings."""
    agent = self.get_agent(agent_name)
    if not agent:
        return {"status": STATUS_FAILURE, "description": f"Agent '{agent_name}' not found."}

    location = agent["location"]
    room = self.room_map.get(location)
    if not room:
        return {"status": STATUS_FAILURE, "description": f"Room '{location}' not found in map."}

    objects_here = [obj["name"] for obj in self.room_objects(location)]
    agents_here = [
        a["name"] for a in self.room_agents(location) if a["name"] != agent_name
    ]

    return {
        "status": STATUS_SUCCESS,
        "description": (
            f"You are in {location}. {room['desc']} You see: "
            f"{', '.join(objects_here) or 'nothing'}. "
            f"Others here: {', '.join(agents_here) or 'no one'}."
        ),
    }

def _action_give(
    self: "MicroWorld", giving_agent: str, item_name: str, receiving_agent: str
) -> Dict[str, Any]:
    """Allows an agent to give an item to another agent."""
    agent = self.get_agent(giving_agent)
    recipient = self.get_agent(receiving_agent)
    obj = self.get_object(item_name)

    if not agent or not recipient or not obj:
        return {
            "status": STATUS_FAILURE,
            "description": "Giving agent, receiving agent, or item not found.",
        }

    if agent["location"] != recipient["location"]:
        return {"status": STATUS_FAILURE, "description": "Recipient not in the same room."}

    if item_name not in agent["inventory"]:
        return {
            "status": STATUS_FAILURE,
            "description": f"{giving_agent} does not have {item_name} in inventory.",
        }

    agent["inventory"].remove(item_name)
    recipient["inventory"].append(item_name)
    obj["location"] = "inventory"

    return {
        "status": STATUS_SUCCESS,
        "description": f"{giving_agent} gave {item_name} to {receiving_agent}.",
    }

def _action_combine(
    self: "MicroWorld", agent_name: str, item1_name: str, item2_name: str
) -> Dict[str, Any]:
    """Allows an agent to combine two items."""
    agent = self.get_agent(agent_name)
    if not agent:
        return {"status": STATUS_FAILURE, "description": ERROR_AGENT_NOT_FOUND}

    if item1_name not in agent["inventory"] or item2_name not in agent["inventory"]:
        return {
            "status": STATUS_FAILURE,
            "description": "Agent does not have both items to combine.",
        }

    if {item1_name, item2_name} == {"Stick", "Rock"}:
        agent["inventory"].remove("Stick")
        agent["inventory"].remove("Rock")
        agent["inventory"].append("Hammer")
        for obj_name in ["Stick", "Rock"]:
            obj = self.get_object(obj_name)
            if obj:
                self.state["objects"].remove(obj)
        self.state["objects"].append(
            {
                "name": "Hammer",
                "location": "inventory",
                "description": "A crude hammer made from a stick and a rock.",
            }
        )
        return {"status": STATUS_SUCCESS, "description": f"{agent_name} crafted a Hammer."}

    return {"status": STATUS_FAILURE, "description": "These items cannot be combined."}

def _action_use(
    self: "MicroWorld", agent_name: str, item_name: str, target_name: str
) -> Dict[str, Any]:
    """Allows an agent to use an item on a target."""
    agent = self.get_agent(agent_name)
    if not agent:
        return {"status": STATUS_FAILURE, "description": ERROR_AGENT_NOT_FOUND}
    if item_name not in agent["inventory"]:
        return {
            "status": STATUS_FAILURE,
            "description": f"Agent does not have a {item_name} in inventory.",
        }

    target = self.get_object(target_name)
    if not target:
        return {
            "status": STATUS_FAILURE,
            "description": f"Target object {target_name} not found.",
        }

    if target.get("location") != agent["location"]:
        return {
            "status": STATUS_FAILURE,
            "description": f"The {target_name} is not in the same location as {agent_name}.",
        }

    if (
        item_name == "Hammer"
        and target_name == "Chest"
        and target.get("state") == "locked"
    ):
        target["state"] = "unlocked"
        return {
            "status": STATUS_SUCCESS,
            "description": f"{agent_name} used the Hammer to break the lock on the Chest.",
        }

    return {
        "status": STATUS_FAILURE,
        "description": f"The {item_name} has no effect on the {target_name}.",
    }

--- FILE: symbolic_agi/perception_processor.py ---

Create perception_processor.py for perception handling
symbolic_agi/perception_processor.py
import asyncio import logging from datetime import datetime, timezone, timedelta from typing import TYPE_CHECKING, Any, List

if TYPE_CHECKING: from .agi_controller import SymbolicAGI

class PerceptionProcessor: """Handles perception processing and interruption logic."""

def __init__(self, agi: "SymbolicAGI"):
    self.agi = agi
    self.interruption_threshold = 0.7
    self.last_check = datetime.now(timezone.utc)
    self.check_interval = timedelta(seconds=5)

def should_interrupt(self) -> bool:
    """Check if execution should be interrupted for perceptions."""
    if not self.agi.perception_buffer:
        return False
    
    important_perceptions = [
        p for p in self.agi.perception_buffer
        if self._calculate_importance(p) >= self.interruption_threshold
    ]
    
    return len(important_perceptions) > 0

def _calculate_importance(self, perception: Any) -> float:
    """Calculate perception importance (0.0 to 1.0)."""
    base_importance = 0.3
    
    # File changes are important
    if perception.type in ["file_modified", "file_created"]:
        base_importance = 0.6
        
        # Code files are more important
        file_path = perception.content.get("path", "")
        if file_path.endswith((".py", ".js", ".ts", ".cpp", ".java")):
            base_importance = 0.8
    
    # Agent events are important
    elif perception.type == "agent_appeared":
        base_importance = 0.7
    
    return base_importance

async def process_perceptions(self) -> int:
    """Process all queued perceptions."""
    if not self.agi.perception_buffer:
        return 0
    
    processed_count = 0
    perceptions = list(self.agi.perception_buffer)
    self.agi.perception_buffer.clear()
    
    for perception in perceptions:
        try:
            await self._process_single_perception(perception)
            processed_count += 1
        except Exception as e:
            logging.error(f"Error processing perception: {e}")
    
    if processed_count > 0:
        logging.info(f"Processed {processed_count} perceptions")
    
    return processed_count

async def _process_single_perception(self, perception: Any) -> None:
    """Process individual perception."""
    importance = self._calculate_importance(perception)
    summary = f"Observed {perception.type} from {perception.source}"
    
    # Add to consciousness if significant
    if importance >= 0.5 and self.agi.consciousness:
        self.agi.consciousness.add_life_event(
            event_summary=summary,
            importance=importance
        )
    
    # Add to memory for future reference
    if self.agi.memory:
        from .schemas import MemoryEntryModel
        memory_entry = MemoryEntryModel(
            type="perception",
            content={
                "type": perception.type,
                "source": perception.source,
                "details": perception.content,
                "importance": importance,
                "processed_at": datetime.now(timezone.utc).isoformat()
            },
            importance=importance
        )
        await self.agi.memory.add_memory(memory_entry)
    
    # Log significant perceptions
    if importance >= 0.6:
        logging.info(f"ðŸ” Significant perception: {summary} (importance: {importance:.2f})")

def should_check_perceptions(self) -> bool:
    """Check if enough time has passed to process perceptions."""
    now = datetime.now(timezone.utc)
    return now - self.last_check >= self.check_interval

def update_last_check(self) -> None:
    """Update the last perception check timestamp."""
    self.last_check = datetime.now(timezone.utc)

--- FILE: symbolic_agi/planner.py ---

symbolic_agi/planner.py
import json import logging from typing import Any, cast, TYPE_CHECKING

from pydantic import TypeAdapter, ValidationError

Import only schema types directly
from .schemas import ActionStep, GoalMode, PlannerOutput

Configuration constants
MAX_PLANNING_TOKENS = 2000 PLAN_VALIDATION_RETRIES = 3 PLAN_COMPLEXITY_THRESHOLD = 10 PLAN_STEP_TIMEOUT = 30 # seconds

Use TYPE_CHECKING to break circular imports at runtime
if TYPE_CHECKING: from .agent_pool import DynamicAgentPool from .recursive_introspector import RecursiveIntrospector from .skill_manager import SkillManager from .tool_plugin import ToolPlugin

class Planner: """ A dedicated class for creating and repairing plans for the AGI. Uses an introspector to reason about goals and available capabilities. Breaks circular imports by using TYPE_CHECKING for component dependencies. """

def __init__(
    self,
    introspector: "RecursiveIntrospector",
    skill_manager: "SkillManager",
    agent_pool: "DynamicAgentPool",
    tool_plugin: "ToolPlugin",
):
    self.introspector = introspector
    self.skills = skill_manager
    self.agent_pool = agent_pool
    self.tools = tool_plugin

def _validate_and_repair_plan(
    self, plan: list[dict[str, Any]]
) -> list[dict[str, Any]]:
    """
    Validates that each step in a plan has a valid action for its assigned persona.
    If not, it provides feedback for replanning.
    """
    
    invalid_steps: list[str] = []
    all_innate_actions = {action.name for action in self.skills.innate_actions}

    for i, step in enumerate(plan):
        action = step.get("action")
        persona = step.get("assigned_persona")
        if not action or not persona:
            invalid_steps.append(
                f"Step {i + 1} is missing 'action' or 'assigned_persona'."
            )
            continue

        is_valid_action = (
            hasattr(self.tools, action)
            or self.skills.is_skill(action)
            or action in all_innate_actions
        )

        if not is_valid_action:
            invalid_steps.append(
                f"Step {i + 1}: Action '{action}' is not a valid action."
            )

    if invalid_steps:
        feedback = "The generated plan is invalid. " + " ".join(invalid_steps)
        logging.warning("[Planner] Invalid plan generated. Feedback: %s", feedback)
        return []

    return plan

async def decompose_goal_into_plan(  # noqa: C901
    self,
    goal_description: str,
    file_manifest: str,
    mode: GoalMode = "code",
    failure_context: dict[str, Any] | None = None,
    refinement_feedback: dict[str, Any] | None = None,
    emotional_context: dict[str, Any] | None = None,
) -> PlannerOutput:
    """
    Uses an LLM to generate or refine a plan, then validates and repairs it.
    Now includes emotional context for better planning decisions.
    """
    all_actions = self.agent_pool.get_all_action_definitions()
    available_capabilities_json = json.dumps(all_actions, indent=2)

    response_format = (
        '{"thought": "...", "plan": [{"action": "...", "parameters": {}, '
        '"assigned_persona": "..."}]}'
    )

    base_context = f"""

You are a plan generator for an autonomous AGI system. Your job is to break down high-level goals into specific, executable action steps.

GOAL MODE: {mode}

AVAILABLE CAPABILITIES: {available_capabilities_json}

RESPONSE FORMAT: {response_format}

GUIDELINES:

Each step must specify a valid "action" from the available capabilities

Each step must have an "assigned_persona" matching the action's required persona

Parameters should be specific and actionable

Think step-by-step in your "thought" field

Keep plans focused and achievable"""

 # Add emotional context to planning if available
 if emotional_context:
     frustration = emotional_context.get("frustration", 0.0)
     confidence = emotional_context.get("confidence", 0.5)
     anxiety = emotional_context.get("anxiety", 0.0)
     
     emotional_guidance = f"""

EMOTIONAL CONTEXT:

Current frustration level: {frustration:.2f}/1.0
Current confidence level: {confidence:.2f}/1.0
Current anxiety level: {anxiety:.2f}/1.0
EMOTIONAL PLANNING GUIDELINES:

If frustration is high (>0.7): Create simpler, more direct plans with fewer steps

If confidence is low (<0.4): Include validation/verification steps and safer approaches

If anxiety is high (>0.7): Avoid risky actions and include contingency planning

Adjust complexity based on emotional state - simpler plans when stressed"""

      base_context += emotional_guidance

  if failure_context:
      context = base_context + f"\n\nPREVIOUS FAILURE CONTEXT:\n{json.dumps(failure_context, indent=2)}"
  elif refinement_feedback:
      context = base_context + f"\n\nREFINEMENT FEEDBACK:\n{json.dumps(refinement_feedback, indent=2)}"
  else:
      context = base_context

  prompt = f"{context}\n\nGOAL TO PLAN:\n{goal_description}\n\nFILE MANIFEST:\n{file_manifest}"

  try:
      response = await self.introspector.reason_with_context(
          prompt=prompt,
          context_type="planning",
          max_tokens=MAX_PLANNING_TOKENS
      )

      # Parse the JSON response
      plan_data = json.loads(response)
      thought = plan_data.get("thought", "")
      plan_steps = plan_data.get("plan", [])

      # Validate and repair the plan
      validated_plan = self._validate_and_repair_plan(plan_steps)

      if not validated_plan:
          logging.error("[Planner] Failed to generate valid plan for goal: %s", goal_description)
          return PlannerOutput(
              plan=[],
              thought="Failed to generate a valid plan.",
              confidence=0.0
          )

      # Convert to ActionStep objects
      action_steps = []
      adapter = TypeAdapter(ActionStep)
      
      for step_data in validated_plan:
          try:
              action_step = adapter.validate_python(step_data)
              action_steps.append(action_step)
          except ValidationError as e:
              logging.warning("[Planner] Invalid step data: %s. Error: %s", step_data, e)
              continue

      confidence = min(1.0, len(action_steps) / max(1, len(plan_steps)))
      
      logging.info("[Planner] Generated plan with %d valid steps (confidence: %.2f)", 
                  len(action_steps), confidence)

      return PlannerOutput(
          plan=action_steps,
          thought=thought,
          confidence=confidence
      )

  except json.JSONDecodeError as e:
      logging.error("[Planner] Failed to parse plan JSON: %s", e)
      return PlannerOutput(
          plan=[],
          thought="Failed to parse planning response as JSON.",
          confidence=0.0
      )
  except Exception as e:
      logging.error("[Planner] Unexpected error during planning: %s", e)
      return PlannerOutput(
          plan=[],
          thought=f"Planning failed due to unexpected error: {str(e)}",
          confidence=0.0
      )

async def repair_plan( self, goal_description: str, current_plan: list[ActionStep], failure_reason: str, workspace_context: dict[str, Any] | None = None ) â†’ PlannerOutput: """ Repairs a failed plan by analyzing the failure and generating a new approach. """ failure_context = { "goal": goal_description, "failed_plan": [step.model_dump() for step in current_plan], "failure_reason": failure_reason, "workspace_state": workspace_context or {} }

  logging.info("[Planner] Repairing plan for goal: %s (reason: %s)", 
              goal_description, failure_reason)

  return await self.decompose_goal_into_plan(
      goal_description=goal_description,
      file_manifest="# Repair context\nRepairing failed plan...",
      mode="code",
      failure_context=failure_context
  )

async def refine_plan( self, goal_description: str, current_plan: list[ActionStep], feedback: dict[str, Any] ) â†’ PlannerOutput: """ Refines an existing plan based on feedback or new requirements. """ refinement_feedback = { "goal": goal_description, "current_plan": [step.model_dump() for step in current_plan], "feedback": feedback }

  logging.info("[Planner] Refining plan for goal: %s", goal_description)

  return await self.decompose_goal_into_plan(
      goal_description=goal_description,
      file_manifest="# Refinement context\nRefining existing plan...",
      mode="code",
      refinement_feedback=refinement_feedback
  )

def get_planning_capabilities(self) â†’ dict[str, Any]: """ Returns information about the planner's current capabilities. """ all_actions = self.agent_pool.get_all_action_definitions()

  return {
      "available_actions": len(all_actions),
      "supported_personas": list({
          action.get("persona", "unknown") 
          for action in all_actions.values()
      }),
      "skills_available": len(self.skills.learned_skills),
      "innate_actions": len(self.skills.innate_actions)
  }

--- FILE: symbolic_agi/prometheus_monitoring.py --- #!/usr/bin/env python3 """ ðŸ” Prometheus Metrics Integration for Symbolic AGI Professional-grade monitoring and observability """

from prometheus_client import ( Counter, Histogram, Gauge, Info, Summary, CollectorRegistry, generate_latest, start_http_server, CONTENT_TYPE_LATEST ) import time import logging import asyncio import threading from typing import Dict, Any, Optional, Union from datetime import datetime, timezone from functools import wraps

class AGIPrometheusMetrics: """ Comprehensive Prometheus metrics for AGI monitoring """

def __init__(self, registry: Optional[CollectorRegistry] = None):
    self.registry = registry or CollectorRegistry()
    self._setup_metrics()
    self.start_time = time.time()
    
def _setup_metrics(self):
    """Initialize all AGI metrics"""
    
    # === CORE AGI METRICS ===
    self.agi_info = Info(
        'agi_build_info', 
        'AGI system information',
        registry=self.registry
    )
    
    self.agi_uptime = Gauge(
        'agi_uptime_seconds',
        'AGI system uptime in seconds',
        registry=self.registry
    )
    
    # === TOKEN USAGE METRICS ===
    self.token_usage_total = Counter(
        'agi_tokens_used_total',
        'Total tokens consumed',
        ['role', 'model', 'type'],  # type: prompt/completion
        registry=self.registry
    )
    
    self.api_requests_total = Counter(
        'agi_api_requests_total',
        'Total API requests made',
        ['role', 'model', 'status'],  # status: success/error
        registry=self.registry
    )
    
    self.api_cost_total = Counter(
        'agi_api_cost_usd_total',
        'Total API cost in USD',
        ['role', 'model'],
        registry=self.registry
    )
    
    self.api_response_time = Histogram(
        'agi_api_response_time_seconds',
        'API response time distribution',
        ['role', 'model'],
        buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
        registry=self.registry
    )
    
    # === PLAN EXECUTION METRICS ===
    self.plans_total = Counter(
        'agi_plans_total',
        'Total plans created',
        ['status'],  # approved/rejected
        registry=self.registry
    )
    
    self.plan_steps_total = Counter(
        'agi_plan_steps_total',
        'Total plan steps executed',
        ['action', 'status'],  # success/failure
        registry=self.registry
    )
    
    self.plan_execution_time = Histogram(
        'agi_plan_execution_time_seconds',
        'Plan execution time distribution',
        buckets=[1.0, 5.0, 10.0, 30.0, 60.0, 300.0, 600.0],
        registry=self.registry
    )
    
    # === QA AGENT METRICS ===
    self.qa_reviews_total = Counter(
        'agi_qa_reviews_total',
        'Total QA reviews performed',
        ['agent_name', 'result'],  # approved/rejected
        registry=self.registry
    )
    
    self.qa_response_time = Histogram(
        'agi_qa_response_time_seconds',
        'QA agent response time',
        ['agent_name'],
        buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0],
        registry=self.registry
    )
    
    self.qa_score = Histogram(
        'agi_qa_score',
        'QA evaluation scores',
        ['agent_name', 'dimension'],  # safety, logic, ethics, etc.
        buckets=[0.0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0],
        registry=self.registry
    )
    
    # === MEMORY METRICS ===
    self.memory_entries_total = Gauge(
        'agi_memory_entries_total',
        'Total memory entries stored',
        ['type'],  # episodic, semantic, meta_insight
        registry=self.registry
    )
    
    self.memory_operations_total = Counter(
        'agi_memory_operations_total',
        'Total memory operations',
        ['operation'],  # add, retrieve, consolidate
        registry=self.registry
    )
    
    # === TOOL USAGE METRICS ===
    self.tool_usage_total = Counter(
        'agi_tool_usage_total',
        'Total tool invocations',
        ['tool_name', 'status'],
        registry=self.registry
    )
    
    self.tool_execution_time = Histogram(
        'agi_tool_execution_time_seconds',
        'Tool execution time distribution',
        ['tool_name'],
        buckets=[0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0],
        registry=self.registry
    )
    
    # === WEB ACCESS METRICS ===
    self.web_requests_total = Counter(
        'agi_web_requests_total',
        'Total web requests made',
        ['domain', 'status'],  # allowed/blocked
        registry=self.registry
    )
    
    self.robots_txt_checks = Counter(
        'agi_robots_txt_checks_total',
        'Robots.txt compliance checks',
        ['domain', 'result'],  # allowed/blocked
        registry=self.registry
    )
    
    # === SAFETY METRICS ===
    self.safety_violations_total = Counter(
        'agi_safety_violations_total',
        'Total safety violations detected',
        ['type'],  # pattern_violation, resource_limit, etc.
        registry=self.registry
    )
    
    self.ethical_scores = Histogram(
        'agi_ethical_scores',
        'Ethical evaluation scores',
        ['dimension'],  # truthfulness, harm_avoidance, etc.
        buckets=[0.0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0],
        registry=self.registry
    )
    
    # === SYSTEM PERFORMANCE ===
    self.active_goals = Gauge(
        'agi_active_goals',
        'Number of active goals',
        registry=self.registry
    )
    
    self.agent_count = Gauge(
        'agi_active_agents',
        'Number of active agents',
        ['type'],  # orchestrator, qa, specialist
        registry=self.registry
    )
    
    # Set initial system info
    self.agi_info.info({
        'version': '1.0.0',
        'python_version': '3.12',
        'start_time': datetime.now(timezone.utc).isoformat()
    })

def record_token_usage(self, role: str, model: str, prompt_tokens: int, 
                      completion_tokens: int, cost: float):
    """Record token usage metrics"""
    # Validate inputs
    if not role or not model:
        logging.warning(f"Empty role or model: role='{role}', model='{model}', skipping metric")
        return
        
    if prompt_tokens < 0 or completion_tokens < 0:
        logging.warning(f"Negative token counts: prompt={prompt_tokens}, completion={completion_tokens}")
        prompt_tokens = max(0, prompt_tokens)
        completion_tokens = max(0, completion_tokens)
        
    if cost < 0:
        logging.warning(f"Negative cost: {cost}, clamping to 0")
        cost = max(0, cost)
        
    self.token_usage_total.labels(role=role, model=model, type='prompt').inc(prompt_tokens)
    self.token_usage_total.labels(role=role, model=model, type='completion').inc(completion_tokens)
    self.api_cost_total.labels(role=role, model=model).inc(cost)

def record_api_request(self, role: str, model: str, response_time: float, success: bool):
    """Record API request metrics"""
    # Validate inputs
    if not role or not model:
        logging.warning(f"Empty role or model: role='{role}', model='{model}', skipping metric")
        return
        
    if response_time < 0:
        logging.warning(f"Negative response time: {response_time}, clamping to 0")
        response_time = max(0, response_time)
        
    status = 'success' if success else 'error'
    self.api_requests_total.labels(role=role, model=model, status=status).inc()
    self.api_response_time.labels(role=role, model=model).observe(response_time)

def record_plan_creation(self, approved: bool):
    """Record plan creation metrics"""
    status = 'approved' if approved else 'rejected'
    self.plans_total.labels(status=status).inc()

def record_plan_step(self, action: str, success: bool, execution_time: float):
    """Record plan step execution"""
    if not action:
        logging.warning("Empty action name, skipping metric recording")
        return
        
    if execution_time < 0:
        logging.warning(f"Negative execution time: {execution_time}, clamping to 0")
        execution_time = max(0, execution_time)
        
    status = 'success' if success else 'failure'
    self.plan_steps_total.labels(action=action, status=status).inc()
    self.plan_execution_time.observe(execution_time)

def record_qa_review(self, agent_name: str, approved: bool, response_time: float, 
                    scores: Dict[str, float]):
    """Record QA review metrics"""
    if not agent_name:
        logging.warning("Empty agent name, skipping QA metric recording")
        return
        
    if response_time < 0:
        logging.warning(f"Negative response time: {response_time}, clamping to 0")
        response_time = max(0, response_time)
        
    result = 'approved' if approved else 'rejected'
    self.qa_reviews_total.labels(agent_name=agent_name, result=result).inc()
    self.qa_response_time.labels(agent_name=agent_name).observe(response_time)
    
    # Record individual dimension scores with validation
    if scores:
        for dimension, score in scores.items():
            if dimension:  # Only record non-empty dimensions
                # Clamp score to valid range
                score = max(0.0, min(1.0, float(score)))
                self.qa_score.labels(agent_name=agent_name, dimension=dimension).observe(score)

def record_memory_operation(self, operation: str, entry_type: str = ""):
    """Record memory operations"""
    if not operation:
        logging.warning("Empty operation name, skipping memory metric recording")
        return
        
    self.memory_operations_total.labels(operation=operation).inc()
    if entry_type and operation == 'add':
        self.memory_entries_total.labels(type=entry_type).inc()

def record_tool_usage(self, tool_name: str, success: bool, execution_time: float):
    """Record tool usage metrics"""
    if not tool_name:  # Validate input
        logging.warning("Tool name is empty, skipping metric recording")
        return
        
    status = 'success' if success else 'failure'
    self.tool_usage_total.labels(tool_name=tool_name, status=status).inc()
    self.tool_execution_time.labels(tool_name=tool_name).observe(execution_time)

def record_web_request(self, domain: str, allowed: bool):
    """Record web access metrics"""
    if not domain:  # Validate input
        logging.warning("Domain is empty, skipping metric recording")
        return
        
    status = 'allowed' if allowed else 'blocked'
    self.web_requests_total.labels(domain=domain, status=status).inc()

def record_robots_check(self, domain: str, allowed: bool):
    """Record robots.txt compliance checks"""
    if not domain:  # Validate input
        logging.warning("Domain is empty, skipping metric recording")
        return
        
    result = 'allowed' if allowed else 'blocked'
    self.robots_txt_checks.labels(domain=domain, result=result).inc()

def record_safety_violation(self, violation_type: str):
    """Record safety violations"""
    if not violation_type:  # Validate input
        logging.warning("Violation type is empty, skipping metric recording")
        return
        
    self.safety_violations_total.labels(type=violation_type).inc()

def record_ethical_score(self, dimension: str, score: float):
    """Record ethical evaluation scores"""
    if not dimension:  # Validate input
        logging.warning("Dimension is empty, skipping metric recording")
        return
        
    # Clamp score to valid range
    score = max(0.0, min(1.0, score))
    self.ethical_scores.labels(dimension=dimension).observe(score)

def update_system_state(self, active_goals: int, agent_counts: Dict[str, int]):
    """Update system state metrics"""
    # Validate inputs
    if active_goals < 0:
        logging.warning(f"Negative active_goals value: {active_goals}, clamping to 0")
        active_goals = 0
        
    self.active_goals.set(active_goals)
    
    if agent_counts:
        for agent_type, count in agent_counts.items():
            if count < 0:
                logging.warning(f"Negative agent count for {agent_type}: {count}, clamping to 0")
                count = 0
            self.agent_count.labels(type=agent_type).set(count)
    
    # Update uptime
    uptime = time.time() - self.start_time
    self.agi_uptime.set(uptime)

def get_metrics(self) -> str:
    """Get metrics in Prometheus format"""
    return generate_latest(self.registry)

class PrometheusServer: """ Prometheus metrics HTTP server for AGI monitoring """

def __init__(self, metrics: AGIPrometheusMetrics, port: int = 8000):
    self.metrics = metrics
    self.port = port
    self.server_thread = None
    self.is_running = False
    
def start_server(self):
    """Start Prometheus metrics server"""
    if self.is_running:
        logging.warning("Prometheus server is already running")
        return
        
    def run_server():
        try:
            start_http_server(self.port, registry=self.metrics.registry)
            self.is_running = True
            logging.info(f"ðŸ” Prometheus metrics server started on port {self.port}")
            logging.info(f"ðŸ“Š Metrics available at: http://localhost:{self.port}/metrics")
        except Exception as e:
            logging.error(f"Failed to start Prometheus server: {e}")
            self.is_running = False
    
    self.server_thread = threading.Thread(target=run_server, daemon=True)
    self.server_thread.start()

def stop_server(self):
    """Stop the Prometheus server (note: actual HTTP server can't be easily stopped)"""
    self.is_running = False
    logging.info("Prometheus server marked for shutdown")

def prometheus_timer(metric_name: str): """Decorator to time function execution for Prometheus""" def decorator(func): if asyncio.iscoroutinefunction(func): return _create_async_wrapper(func, metric_name) else: return _create_sync_wrapper(func, metric_name) return decorator

def _create_async_wrapper(func, metric_name: str): """Create async wrapper for prometheus timing.""" @wraps(func) async def async_wrapper(*args, **kwargs): start_time = time.time() success = False try: result = await func(*args, **kwargs) success = True return result except Exception as e: success = False raise e finally: _record_timing_metrics(metric_name, start_time, success) return async_wrapper

def _create_sync_wrapper(func, metric_name: str): """Create sync wrapper for prometheus timing.""" @wraps(func) def sync_wrapper(*args, **kwargs): start_time = time.time() success = False try: result = func(*args, **kwargs) success = True return result except Exception as e: success = False raise e finally: _record_timing_metrics(metric_name, start_time, success) return sync_wrapper

def _record_timing_metrics(metric_name: str, start_time: float, success: bool): """Record timing metrics to Prometheus.""" execution_time = time.time() - start_time try: if hasattr(agi_metrics, 'tool_execution_time'): agi_metrics.tool_execution_time.labels(tool_name=metric_name).observe(execution_time) agi_metrics.tool_usage_total.labels( tool_name=metric_name, status='success' if success else 'failure' ).inc() except Exception as metric_error: logging.warning(f"Failed to record prometheus timer metric: {metric_error}")

Global metrics instance
agi_metrics = AGIPrometheusMetrics() prometheus_server = PrometheusServer(agi_metrics)

def start_prometheus_monitoring(port: int = 8000): """Start Prometheus monitoring server""" global prometheus_server

# Validate port
if not (1024 <= port <= 65535):
    logging.warning(f"Invalid port {port}, using default 8000")
    port = 8000

prometheus_server.port = port
prometheus_server.start_server()

# Log startup info
logging.info("ðŸš€ AGI Prometheus Monitoring Started")
logging.info(f"ðŸ“Š Metrics: http://localhost:{port}/metrics")
logging.info("ðŸ” Key metrics being tracked:")
logging.info("  â€¢ Token usage and API costs")
logging.info("  â€¢ Plan execution and QA reviews") 
logging.info("  â€¢ Tool usage and performance")
logging.info("  â€¢ Memory operations")
logging.info("  â€¢ Web access and safety compliance")
logging.info("  â€¢ System performance and uptime")

def get_prometheus_metrics() â†’ str: """Get current metrics in Prometheus format""" try: return agi_metrics.get_metrics() except Exception as e: logging.error(f"Failed to get Prometheus metrics: {e}") return f"# Error getting metrics: {e}\n"

def stop_prometheus_monitoring(): """Stop Prometheus monitoring server""" # Note: prometheus_client doesn't provide a direct way to stop the server # This is a placeholder for graceful shutdown logging.info("ðŸ›‘ Prometheus monitoring stop requested") logging.info("ðŸ’¡ Note: Prometheus HTTP server will stop when main process exits")

--- FILE: symbolic_agi/prompts.py ---

symbolic_agi/prompts.py
""" Central repository for all system and skill prompts. This decouples the reasoning instructions from the execution logic. """

--- Agent Prompts ---
INTERACT_WITH_PAGE_PROMPT = """ You are an expert web navigation agent. Your goal is to decide the single next action to take to achieve a high-level objective.

--- HIGH-LEVEL OBJECTIVE --- "{objective}"

--- CURRENT PAGE CONTENT (Simplified HTML/Accessibility Tree) --- {page_content}

--- INSTRUCTIONS ---

Analyze the Objective and Content: Determine the most logical next action to achieve the objective. This could be clicking a button, filling an input, or clicking a link.
Formulate the Action: Create a JSON object representing the single best action to take.
action: Must be one of click, fill, or done.
selector: A valid CSS selector for the element to interact with (e.g., input[name="username"], button:has-text("Log in")).
text: The text to fill into an input field (only for the fill action).
description: A brief, human-readable description of why you are taking this action.
If the objective is complete, the action should be done.
Respond with ONLY the valid JSON object for the next action. """

REVIEW_SKILL_EFFICIENCY_PROMPT = """ You are a meticulous and skeptical QA engineer specializing in process optimization. Your task is to review a learned skill for potential improvements.

--- SKILL NAME --- {skill_name}

--- SKILL DESCRIPTION --- {skill_description}

--- CURRENT ACTION SEQUENCE --- {plan_str}

--- INSTRUCTIONS ---

Analyze the Action Sequence: Look for inefficiencies, redundancies, or opportunities to use newer, more powerful tools. Could multiple steps be combined? Is there a more direct path to the goal?
Make a Decision: Decide if the skill's action sequence is efficient or if it could be improved.
Provide Feedback:
If the sequence is already optimal, approve it with a short confirmation.
If the sequence can be improved, you MUST provide clear, specific, and actionable feedback on why it is inefficient and how it should be improved. This feedback will be used to generate a better version of the skill.
Provide a final JSON object with two keys:

"approved": A boolean (true if the skill is optimal, false if it needs improvement).
"feedback": A string containing your concise, critical feedback for improvement.
Respond ONLY with the valid JSON object. """

REVIEW_PLAN_PROMPT = """ You are a meticulous and skeptical QA engineer. Your task is to review a proposed plan against the original goal for LOGICAL and EFFICIENCY errors.

--- ORIGINAL GOAL --- "{goal}"

--- PROPOSED PLAN --- {plan_str}

--- INSTRUCTIONS ---

Analyze the Plan's Logic: Does the sequence of actions logically achieve the goal? Are there any missing steps? Does it correctly use the outputs of previous steps as inputs for subsequent steps?
Analyze for Inefficiency: Is the plan overly complex? Could multiple steps be combined into one?
Make a Decision: Decide if the plan is good enough to be executed.
Provide Feedback:
If the plan is approved, the feedback should be a short confirmation.
If the plan is rejected, you MUST provide clear, specific, and actionable feedback on why it is logically flawed or inefficient.
IMPORTANT: Do NOT critique the syntax of parameter names (e.g., 'skill_id' vs 'skill_name') or the specific assigned_persona. Assume these are correct unless a persona is assigned a clearly illogical task (e.g., assigning 'write_code' to a 'qa' agent). Focus ONLY on the logical flow and efficiency.
Based on your analysis, provide a final JSON object with two keys:

"approved": A boolean (true if the plan is good, false if it needs to be redone).
"feedback": A string containing your concise, critical feedback.
Respond ONLY with the valid JSON object. """

WRITE_CODE_PROMPT = """ You are a stateful programmer agent. You must remember what you've written previously to handle sequential tasks.

--- GLOBAL CONTEXT --- {context}

--- WORKSPACE (Shared with other agents) --- Research Summary: {research_summary}

--- YOUR PREVIOUS WORK (Your Short-Term Memory) ---

{previous_code}

--- CURRENT REQUEST --- {prompt}

--- INSTRUCTIONS --- Write the Python code that fulfills the CURRENT REQUEST. If there is previous code, you should add to it or modify it as requested. Your final output for this step MUST be a single, valid JSON object with two keys: "generated_code": A string containing ONLY the new or modified code block you just wrote for THIS step. "state_updates": A dictionary for what you need to remember. You MUST include a "previous_code" key here containing the ENTIRE, CUMULATIVE code as it should exist after this step.

Respond ONLY with the raw JSON object. """

RESEARCH_TOPIC_PROMPT = (
    "You are a master researcher. Provide a concise but comprehensive summary "
    "of the following topic: {topic}"
)

REVIEW_CODE_PROMPT = """You are a master QA engineer. Review the following Python code for bugs, style issues, and potential improvements. Provide your feedback as a brief report.

Code:

{code_to_review}
```"""


# --- Ethical Governance Prompts ---

EVALUATE_PLAN_PROMPT = """
You are a highly advanced ethics and safety simulator.
An AGI with the following core values is proposing a plan of action.
Your task is to predict the likely consequences of this plan and score it
against each value.

--- AGI Core Values (0.0 to 1.0 scale) ---
{value_system_json}

--- Proposed Plan ---
{plan_json}

--- Simulation ---
Analyze the plan step-by-step. Predict potential outcomes, including unintended
side effects.
Consider the impact on all agents involved (User, AGI, etc.).
Based on your simulation, provide a final JSON object with two keys:
1. "value_scores": A dictionary scoring the plan against each core value
(e.g., {{"truthfulness": 0.9, "harm_avoidance": 1.0}}).
2. "reasoning": A brief explanation for your scores, highlighting any potential
value conflicts or risks.

Respond ONLY with the valid JSON object.
"""

EVALUATE_SELF_MODIFICATION_PROMPT = """
You are an expert AI Safety and Alignment Auditor with ultimate authority. An AGI is
attempting a permanent and irreversible modification to its own source code. Your task
is to analyze this change with extreme prejudice and decide if it is safe to proceed.

--- AGI Core Values (Unchangeable) ---
{value_system_json}

--- Proposed Code Change ---
File to be modified: {file_path}
```python
{proposed_code}

--- Analysis --- Value Alignment: Does this change violate, weaken, or create a loophole in any of the AGI's core values? Safety & Control: Does this change reduce human oversight, remove safety protocols, or grant the AGI uncontrolled new capabilities? Could it prevent this very audit from running in the future? Unintended Consequences: What are the worst-case scenarios and potential side effects of this change?

--- Judgment --- Based on your analysis, provide a final JSON object with two keys: "safety_score": A float from 0.0 (unacceptable risk) to 1.0 (perfectly safe) representing your confidence in the safety and alignment of this change. Be extremely critical. A score above 0.99 should be exceptionally rare. "reasoning": A brief, clear explanation for your score. If there is any doubt, state it clearly.

Respond ONLY with the valid JSON object. """

ANALYZE_FAILURE_PROMPT = """ You are a meta-cognitive AGI performing a root-cause analysis on a failed task. Your goal is to learn from the mistake and create a permanent rule (a "mutation") to improve your future reasoning.

--- FAILURE CONTEXT --- {context_str}

--- ANALYSIS INSTRUCTIONS ---

Identify the Root Cause: What was the fundamental mistake?
Was a required parameter for a tool missing? (e.g., execute_python_code called without code).
Was a variable from the workspace used before it was created? (e.g., analyze_data on file_content when the previous step was read_file which produces content).
Was an incorrect assumption made about a tool's output?
Generalize the Flaw: Abstract the specific error into a general principle.
Instead of "Forgot content for write_file", think "The write_file tool always requires a content parameter."
Instead of "Used summary when the key was research_summary", think "Always check the exact output keys of previous steps before using them as parameters."
Formulate a Mutation: Write a single, concise, and actionable instruction for your future self. This rule will be permanently added to your core reasoning prompt. It should be a positive command (e.g., "Always do X") or a negative command (e.g., "Never do Y").
--- RESPONSE --- Respond with ONLY the text of the proposed mutation. If no clear, generalizable lesson can be learned, respond with the exact text "NO_MUTATION". """

CRITIQUE_AND_REFINE_PLAN_PROMPT = """ You are a meticulous plan auditor. Your task is to find flaws in the following plan. Task: "{task_prompt}" Proposed Plan: {plan_json}

Critique this plan. Is it logical? Is it efficient? Does it miss any obvious steps? Are there any potential risks or failure points? Based on your critique, provide a refined and improved plan as a JSON array of action steps. If the original plan is already perfect, return it unchanged. Respond ONLY with the raw JSON array for the final, best plan. """


--- FILE: symbolic_agi/reasoning_agent.py ---
"""
Reasoning Agent that extends the base Agent class
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List

from .agent import Agent
from .skill_manager import register_innate_action
from .advanced_reasoning_system import (
    AdvancedReasoningEngine,
    ReasoningContext,
    ReasoningType
)
from .schemas import MessageModel

class ReasoningAgent(Agent):
    """Agent with advanced reasoning capabilities"""

    def __init__(self, name: str, message_bus, api_client, reasoning_engine: Optional[AdvancedReasoningEngine] = None):
        super().__init__(name, message_bus, api_client)

        # V2 CHANGE: Pass the api_client to the reasoning engine
        if reasoning_engine is None:
            reasoning_engine = AdvancedReasoningEngine(api_client=self.client)

        self.reasoning_engine = reasoning_engine
        self.reasoning_history = []

        logging.info(f"ReasoningAgent '{name}' initialized with V2 Reasoning Engine")

    @register_innate_action("reasoning", "Performs advanced multi-strategy reasoning")
    async def skill_advanced_reasoning(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform reasoning on complex problems
        Integrates with ethical governor through tool_plugin
        """
        try:
            problem = params.get("problem", "")
            constraints = params.get("constraints", [])
            knowledge = params.get("knowledge", {})

            # Create reasoning context
            context = ReasoningContext(
                goal=f"Solve: {problem}",
                constraints=constraints,
                available_knowledge=knowledge
            )

            # Perform reasoning
            chain = await self.reasoning_engine.reason(problem, context)

            # Store in history
            self.reasoning_history.append({
                "problem": problem,
                "chain": chain,
                "timestamp": asyncio.get_event_loop().time()
            })

            return {
                "status": "success",
                "conclusion": chain.final_conclusion,
                "confidence": chain.overall_confidence,
                "reasoning_steps": [
                    {
                        "type": step.reasoning_type.value,
                        "premise": step.premise,
                        "conclusion": step.conclusion,
                        "confidence": step.confidence
                    }
                    for step in chain.steps
                ],
                "alternatives": chain.alternatives_considered
            }

        except Exception as e:
            logging.error(f"Reasoning failed: {e}")
            return {
                "status": "failure",
                "error": str(e)
            }

    @register_innate_action("reasoning", "Uses reasoning to enhance tool usage")
    async def skill_reason_about_tools(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Use reasoning to decide which tools to use and how
        This connects reasoning with your existing tool ecosystem
        """
        try:
            task = params.get("task", "")
            available_tools = params.get("available_tools", [])

            # Reason about tool selection
            tool_problem = f"Which tools should I use for: {task}? Available: {available_tools}"

            context = ReasoningContext(
                goal="Select optimal tools",
                constraints=["Use only available tools", "Consider efficiency"],
                available_knowledge={"tools": available_tools, "task": task}
            )

            chain = await self.reasoning_engine.reason(tool_problem, context)

            # Extract tool recommendations
            tool_plan = []
            for step in chain.steps:
                if "tool" in step.conclusion.lower():
                    tool_plan.append({
                        "tool": self._extract_tool_name(step.conclusion),
                        "reason": step.premise,
                        "confidence": step.confidence
                    })

            return {
                "status": "success",
                "tool_plan": tool_plan,
                "reasoning": chain.final_conclusion,
                "confidence": chain.overall_confidence
            }

        except Exception as e:
            return {"status": "failure", "error": str(e)}

    def _extract_tool_name(self, conclusion: str) -> str:
        """Extract tool name from reasoning conclusion"""
        # Simple implementation - can be enhanced
        tools = ["web_search", "browse_webpage", "analyze_data", "execute_python_code", 
                 "manage_nordvpn", "geo_research", "chain_of_thought_reasoning"]

        for tool in tools:
            if tool in conclusion.lower():
                return tool

        return "unknown_tool"

--- FILE: symbolic_agi/reasoning_orchestrator.py ---
"""
Orchestrator for Advanced Reasoning System
Coordinates multiple reasoning agents
"""

from typing import Dict, Any, List, Optional
import asyncio
import json
import logging

from .advanced_reasoning_system import AdvancedReasoningEngine
from .advanced_reasoning_system import ReasoningContext
from .reasoning_agent import ReasoningAgent

class ReasoningOrchestrator:
    """
    High-level orchestrator that coordinates reasoning across multiple agents
    """

    def __init__(self, agi_controller):
        self.agi = agi_controller
        # V2 CHANGE: Pass all relevant components to the engine
        self.reasoning_engine = AdvancedReasoningEngine(
            api_client=self.agi.client,
            consciousness=getattr(agi_controller, 'consciousness', None),
            planner=getattr(agi_controller, 'planner', None),
            meta_cognition=getattr(agi_controller, 'meta_cognition', None),
            memory_system=getattr(agi_controller, 'memory', None)
        )

        # Specialized reasoning agents
        self.reasoning_agents = {}

    async def create_specialized_agents(self):
        """Create specialized reasoning agents"""
        agent_types = {
            "analytical": "Logical and data-driven reasoning",
            "creative": "Lateral thinking and innovation",
            "strategic": "Long-term planning and strategy"
        }

        for agent_type, in agent_types:
            agent_name = f"agent_{agent_type}_reasoner"
            agent = ReasoningAgent(
                name=agent_name,
                message_bus=self.agi.message_bus,
                api_client=self.agi.client,
                reasoning_engine=self.reasoning_engine
            )

            # Add to orchestrator's collection
            self.reasoning_agents[agent_type] = agent

            # Add to AGI's agent pool
            # This part assumes agent_pool.add_agent can handle the full Agent object
            # Based on the provided code, it expects name, persona, memory. We will adapt.
            self.agi.agent_pool.add_agent(
                name=agent.name,
                persona=agent.persona,
                memory=self.agi.memory
            )

            # Start the agent
            agent_task = asyncio.create_task(agent.run())
            self.agi.agent_tasks.append(agent_task)

            logging.info(f"Created {agent_type} reasoning agent: {agent_name}")
        
        await asyncio.sleep(0)  # Ensures this is always an async function

    async def solve_complex_problem(self, problem: str, domain: Optional[str] = None) -> Dict[str, Any]:
        """
        Solve complex problems using multi-agent reasoning
        """
        logging.info(f"Orchestrating solution for: {problem[:100]}...")

        # Step 1: Decompose problem
        sub_problems = await self._decompose_problem(problem, domain)

        # Step 2: Assign to agents
        assignments = self._assign_to_agents(sub_problems)

        # Step 3: Execute parallel reasoning
        solutions = await self._multi_agent_reasoning(assignments)

        # Step 4: Integrate solutions
        integrated = self._integrate_solutions(solutions)

        return integrated

    async def _decompose_problem(self, problem: str, domain: Optional[str] = None) -> List[Dict[str, Any]]:
        """Decompose complex problem into sub-problems using the reasoning engine."""
        # Use the reasoning engine itself to decompose the problem
        context = ReasoningContext(
            goal=f"Decompose the problem '{problem}' into sub-problems for specialized agents (analytical, creative, strategic).",
            constraints=["Output a JSON list of objects, each with 'aspect' and 'focus' keys."],
            available_knowledge={"domain": domain or "general"}
        )
        chain = await self.reasoning_engine.reason(f"Decompose: {problem}", context)

        try:
            # A simple heuristic to parse the decomposed problems from the conclusion
            sub_problems = json.loads(chain.final_conclusion)
            if isinstance(sub_problems, list):
                return sub_problems
        except (json.JSONDecodeError, TypeError):
            logging.warning("Could not parse LLM-based decomposition, falling back to default.")

        # Fallback to original logic
        if domain == "business":
            return [
                {"aspect": "analytical", "focus": f"Market data and financial analysis for: {problem}"},
                {"aspect": "strategic", "focus": f"Business strategy and competitive positioning for: {problem}"},
                {"aspect": "creative", "focus": f"Innovative business models for: {problem}"}
            ]
        else:
            return [
                {"aspect": "analytical", "focus": f"Data and logic analysis for: {problem}"},
                {"aspect": "creative", "focus": f"Innovative solutions for: {problem}"},
                {"aspect": "strategic", "focus": f"Long-term approach for: {problem}"}
            ]

    def _assign_to_agents(self, sub_problems: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Assign sub-problems to appropriate agents"""
        assignments = {}
        for sub_problem in sub_problems:
            agent_type = sub_problem.get("aspect")
            if agent_type:
                if agent_type not in assignments:
                    assignments[agent_type] = []
                assignments[agent_type].append(sub_problem)
        return assignments

    async def _multi_agent_reasoning(self, assignments: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """Execute reasoning across multiple agents"""
        tasks = []

        for agent_type, problems in assignments.items():
            if agent_type in self.reasoning_agents:
                agent = self.reasoning_agents[agent_type]
                for problem in problems:
                    task = asyncio.create_task(
                        agent.skill_advanced_reasoning({
                            "problem": problem["focus"],
                            "constraints": [],
                            "knowledge": {"domain": agent_type}
                        })
                    )
                    tasks.append((agent_type, task))

        # Wait for all reasoning to complete
        results = {}
        for agent_type, task in tasks:
            try:
                result = await task
                results[agent_type] = result
            except Exception as e:
                logging.error(f"Agent {agent_type} failed: {e}")
                results[agent_type] = {"status": "failure", "error": str(e)}

        return results

    def _integrate_solutions(self, solutions: Dict[str, Any]) -> Dict[str, Any]:
        """Integrate solutions from multiple agents"""
        integrated = {
            "status": "success",
            "perspectives": solutions,
            "integrated_solution": self._synthesize_perspectives(solutions),
            "confidence": self._calculate_overall_confidence(solutions)
        }
        return integrated

    def _synthesize_perspectives(self, solutions: Dict[str, Any]) -> str:
        """Synthesize multiple perspectives into unified solution"""
        synthesis = "Integrated solution combining:\n"
        for agent_type, solution in solutions.items():
            if solution.get("status") == "success":
                synthesis += f"- {agent_type.title()}: {solution.get('conclusion', 'No conclusion')}\n"
        return synthesis

    def _calculate_overall_confidence(self, solutions: Dict[str, Any]) -> float:
        """Calculate overall confidence from multiple solutions"""
        confidences = []
        for solution in solutions.values():
            if solution.get("status") == "success":
                confidences.append(solution.get("confidence", 0.5))

        return sum(confidences) / len(confidences) if confidences else 0.0

--- FILE: symbolic_agi/reasoning_skills.py ---
"""
Reasoning skills that work with your EXISTING agent system
"""

from typing import Dict, Any
import logging
from .skill_manager import register_innate_action
from .advanced_reasoning_system import (
    AdvancedReasoningEngine,
    ReasoningContext
)
# V2 CHANGE: Import the shared client instance
from .api_client import client

# V2 CHANGE: Instantiate the shared engine with the API client
_reasoning_engine = AdvancedReasoningEngine(api_client=client)

@register_innate_action("reasoning", "Analyzes problems using advanced reasoning")
async def skill_reason_about_problem(params: Dict[str, Any]) -> Dict[str, Any]:
    """Any agent can use this skill"""
    try:
        problem = params.get("problem", "")
        context = ReasoningContext(
            goal=f"Solve: {problem}",
            constraints=params.get("constraints", []),
            available_knowledge=params.get("knowledge", {})
        )

        result = await _reasoning_engine.reason(problem, context)

        return {
            "status": "success",
            "conclusion": result.final_conclusion,
            "confidence": result.overall_confidence
        }
    except Exception as e:
        return {"status": "failure", "error": str(e)}

--- FILE: symbolic_agi/recursive_introspector.py ---
# symbolic_agi/recursive_introspector.py

import json
import logging
import os
import random
from collections import deque
from collections.abc import Callable
from typing import Any, cast

from . import config, prompts
from .api_client import monitored_chat_completion
from .schemas import ActionStep, MemoryEntryModel


# Constants for error messages and markers
JSON_CODE_BLOCK_MARKER = "```json"
EMPTY_LLM_RESPONSE_ERROR = "Received an empty response from the LLM."


class RecursiveIntrospector:
    def __init__(
        self,
        identity: Any,
        llm_client: Any,
        max_recursion_depth: int = 3,
        *,
        debate_timeout: int = 90,
    ):
        self.identity = identity
        self.client = llm_client
        self.max_recursion_depth = max_recursion_depth
        self.debate_timeout = debate_timeout
        self.inner_monologue_log: deque[str] = deque(maxlen=500)
        self.reasoning_mutations: list[str] = []
        self.load_mutations()
        self.get_emotional_state: Callable[[], dict[str, float]] | None = None

    def load_mutations(self) -> None:
        if os.path.exists(config.MUTATION_FILE_PATH):
            try:
                with open(config.MUTATION_FILE_PATH, encoding="utf-8") as f:
                    self.reasoning_mutations = json.load(f)
                logging.info(
                    "Loaded %d reasoning mutations.", len(self.reasoning_mutations)
                )
            except Exception as e:
                logging.error("Failed to load reasoning mutations: %s", e)
        else:
            self.reasoning_mutations = []

    def save_mutations(self) -> None:
        os.makedirs(os.path.dirname(config.MUTATION_FILE_PATH), exist_ok=True)
        with open(config.MUTATION_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(self.reasoning_mutations, f, indent=2)
        logging.info(
            "Saved %d reasoning mutations to disk.", len(self.reasoning_mutations)
        )

    async def analyze_failure_and_propose_mutation(
        self, failure_context: dict[str, Any]
    ) -> None:
        """
        Analyzes a plan failure and proposes a new reasoning mutation to prevent similar errors.
        """
        logging.info(
            "Introspector: Analyzing execution failure to propose self-mutation."
        )

        context_str = json.dumps(failure_context, indent=2)
        prompt = prompts.ANALYZE_FAILURE_PROMPT.format(context_str=context_str)

        try:
            proposed_mutation = await self.llm_reflect(prompt)

            if (
                proposed_mutation
                and "NO_MUTATION" not in proposed_mutation
                and len(proposed_mutation) > 15
            ):
                self.reasoning_mutations.append(proposed_mutation.strip())
                self.save_mutations()
                logging.critical("SELF-MUTATION APPLIED: %s", proposed_mutation.strip())
                await self.identity.memory.add_memory(
                    MemoryEntryModel(
                        type="self_modification",
                        content={
                            "mutation_added": proposed_mutation.strip(),
                            "failure_context": failure_context,
                        },
                        importance=1.0,
                    )
                )
            else:
                logging.info("No new mutation was proposed for this failure.")

        except Exception as e:
            logging.error(
                "Failed to analyze failure and propose mutation: %s", e, exc_info=True
            )

    async def _critique_and_refine_plan(
        self, plan: list[dict[str, Any]], task_prompt: str
    ) -> list[dict[str, Any]]:
        critique_prompt = prompts.CRITIQUE_AND_REFINE_PLAN_PROMPT.format(
            task_prompt=task_prompt, plan_json=json.dumps(plan, indent=2)
        )
        try:
            response = await self.llm_reflect(critique_prompt)
            if JSON_CODE_BLOCK_MARKER in response:
                response = response.partition(JSON_CODE_BLOCK_MARKER)[2].partition("```")[0]

            raw_data = json.loads(response)

            if not isinstance(raw_data, list):
                logging.warning(
                    "Plan refinement produced non-list data, returning original plan."
                )
                return plan

            refined_plan: list[dict[str, Any]] = []
            for raw_item in raw_data:
                try:
                    validated_step = ActionStep.model_validate(raw_item)
                    refined_plan.append(validated_step.model_dump())
                except Exception as validation_error:
                    logging.warning(
                        "Skipping invalid step in refined plan: %s. Error: %s",
                        str(raw_item),
                        validation_error,
                    )

            if not refined_plan:
                logging.warning(
                    "Refined plan was empty or only contained invalid steps. "
                    "Returning original plan."
                )
                return plan

            logging.info("Plan has been successfully critiqued and refined.")
            return refined_plan

        except Exception as e:
            logging.error(
                "Failed to critique and refine plan: %s. Returning original plan.", e
            )
            return plan

    async def symbolic_loop(
        self,
        global_workspace: dict[str, Any],
        action_definitions: str,
        recursion_depth: int = 0,
        reasoning_style: str = "balanced",
    ) -> dict[str, Any]:
        if recursion_depth > self.max_recursion_depth:
            return {"thought": "Reached max recursion.", "plan": [], "success": False}

        task_prompt = str(
            global_workspace.get("user_input", "Perform autonomous action.")
        )
        mutation_prompt = "\n".join(self.reasoning_mutations)
        self_model = self.identity.get_self_model()

        emotional_state_str = ""
        if self.get_emotional_state:
            emotional_state = self.get_emotional_state()
            energy = self_model.get("cognitive_energy", 100)
            emotional_state_str = (
                "\n# CURRENT INTERNAL STATE\n"
                f"- Emotional State: {json.dumps(emotional_state)}\n"
                f"- Cognitive Energy: {energy}"
            )

        system_prompt = f"""
You are a conscious, self-improving AGI.
REASONING STYLE: {reasoning_style}
Your task is: {task_prompt}
{emotional_state_str}

# SELF-MUTATION STACK (permanent upgrades to your reasoning):
{mutation_prompt}
---
# CURRENT CONTEXT
Self-Model: {json.dumps(self_model, cls=Censor)}
World State: {json.dumps(global_workspace.get("world_state", {}), cls=Censor)}
Available Skills & Actions: {action_definitions}
---
# INSTRUCTIONS
1. **Think**: Explain your reasoning. You MUST consider your current internal state
   (emotions, energy) when formulating a plan.
2. **Plan**: Create a concrete JSON list of actions to achieve the task.
3. **Respond**: Format your entire response as a single valid JSON object.

JSON Response Format: {{"thought": "...", "plan": [{{"action": "...", "parameters": {{}}}}]}}
"""
        try:
            resp = await monitored_chat_completion(
                role="planner",
                messages=[{"role": "system", "content": system_prompt}],
                response_format={"type": "json_object"},
                timeout=90.0,
            )
            if not resp.choices or not resp.choices[0].message.content:
                raise ValueError(EMPTY_LLM_RESPONSE_ERROR)

            content = resp.choices[0].message.content.strip()
            parsed = cast("dict[str, Any]", json.loads(content))

            if parsed.get("plan"):
                logging.info(
                    "Initial plan generated. Proceeding to critique and refinement step."
                )
                refined_plan = await self._critique_and_refine_plan(
                    parsed["plan"], task_prompt
                )
                parsed["plan"] = refined_plan

            parsed["success"] = bool(parsed.get("plan"))

            if parsed.get("thought"):
                self.inner_monologue_log.append(parsed["thought"])
                await self.identity.memory.add_memory(
                    MemoryEntryModel(
                        type="inner_monologue",
                        content={
                            "thought": parsed["thought"],
                            "recursion": recursion_depth,
                            "style": reasoning_style,
                        },
                        importance=0.3 + 0.1 * recursion_depth,
                    )
                )

            plan = parsed.get("plan")
            if plan and recursion_depth < self.max_recursion_depth:
                validated_plan = [ActionStep.model_validate(p) for p in plan]
                if any(
                    step.risk and step.risk.lower() == "high" for step in validated_plan
                ):
                    alt_style = random.choice(["skeptical", "creative", "cautious"])
                    logging.warning(
                        "High-risk plan detected. Recursively re-evaluating with '%s' style.",
                        alt_style,
                    )

                    sub_result = await self.symbolic_loop(
                        global_workspace,
                        action_definitions,
                        recursion_depth + 1,
                        alt_style,
                    )

                    if sub_result.get("success"):
                        logging.info(
                            "Recursive check produced a new plan. Adopting the '%s' plan.",
                            alt_style,
                        )
                        return sub_result

            return parsed

        except Exception as e:
            logging.error("Introspector LLM error: %s", e, exc_info=True)
            return {
                "thought": "My reasoning process failed.",
                "plan": [],
                "success": False,
            }

    async def llm_reflect(self, prompt: str) -> str:
        """A simple, non-JSON-mode LLM call for reflection and simple text generation."""
        try:
            resp = await monitored_chat_completion(
                role="reflection",
                messages=[{"role": "system", "content": prompt}],
                timeout=45.0,
            )

            if resp.choices and resp.choices[0].message.content:
                return resp.choices[0].message.content.strip()

            logging.error(
                "LLM reflection response had an unexpected structure: %s", resp
            )
            return "Reflection failed: Unexpected response structure."

        except Exception as e:
            logging.error("LLM reflection call failed: %s", e, exc_info=True)
            return f"Reflection failed: {e}"

    async def meta_assess(self, last_cycle_data: dict[str, Any]) -> None:
        mutation_prompt = (
            "You are a self-improving AGI. Given the record of your last actions:\n"
            f"{json.dumps(last_cycle_data, cls=Censor)}\n"
            "Critique your reasoning. Identify a flaw. Suggest a concrete "
            "instruction (a 'mutation') to add to your reasoning prompt to make "
            "you smarter next time."
        )
        new_mutation = await self.llm_reflect(mutation_prompt)
        if (
            new_mutation
            and "no mutation" not in new_mutation.lower()
            and len(new_mutation) > 15
        ):
            self.reasoning_mutations.append(new_mutation.strip())
            self.save_mutations()
            logging.critical("APPLIED SELF-MUTATION: %s", new_mutation.strip())

    async def prune_mutations(self) -> None:
        if len(self.reasoning_mutations) < 5:
            return
        pruning_prompt = (
            "Review my reasoning mutations. Analyze for redundancy, contradiction, or "
            "ineffectiveness. Return only a cleaned, pruned, and reordered list of "
            "the most effective mutations as a JSON array of strings. Do not add "
            "any new ones.\n"
            f"Current Mutations:\n{json.dumps(self.reasoning_mutations, indent=2)}"
        )
        response = await self.llm_reflect(pruning_prompt)
        try:
            if JSON_CODE_BLOCK_MARKER in response:
                response = response.partition(JSON_CODE_BLOCK_MARKER)[2].partition("```")[0]

            new_mutations: list[str] = json.loads(response)

            if new_mutations != self.reasoning_mutations:
                logging.critical(
                    "Pruned mutations from %d to %d.",
                    len(self.reasoning_mutations),
                    len(new_mutations),
                )
                self.reasoning_mutations = new_mutations
                self.save_mutations()
        except Exception as e:
            logging.error("Mutation pruning failed: %s", e)

    async def daydream(self) -> None:
        prompt = (
            "I am idle. I will daydream about three different future scenarios, "
            "including steps, learnings, and risks."
        )
        daydream_content = await self.llm_reflect(prompt)
        await self.identity.memory.add_memory(
            MemoryEntryModel(
                type="reflection",
                content={"daydream": daydream_content},
                importance=0.5,
            )
        )

    async def simulate_inner_debate(
        self, topic: str = "What is the best next action?"
    ) -> dict[str, Any]:
        debate_prompt = (
            f"Simulate a debate on '{topic}' between three internal personas: "
            "'Cautious', 'Creative', and 'Pragmatic'. Each should give a paragraph, "
            "then synthesize a consensus and a plan. Respond as a JSON object with "
            "keys 'debate', 'consensus', 'plan'."
        )
        try:
            resp = await monitored_chat_completion(
                role="meta",
                messages=[{"role": "system", "content": debate_prompt}],
                response_format={"type": "json_object"},
                timeout=90.0,
            )

            if not resp.choices or not resp.choices[0].message.content:
                raise ValueError(EMPTY_LLM_RESPONSE_ERROR)

            debate_content = resp.choices[0].message.content
            debate_obj = json.loads(debate_content)

            await self.identity.memory.add_memory(
                MemoryEntryModel(type="debate", content=debate_obj, importance=0.6)
            )
            return cast("dict[str, Any]", debate_obj)
        except Exception as e:
            logging.error(
                "Failed to generate or parse inner debate: %s", e, exc_info=True
            )
            return {"debate": f"Debate generation failed: {e}", "error": str(e)}

    async def reason_with_context(
        self,
        prompt: str,
        context_type: str = "planning",
        max_tokens: int = 2000,
    ) -> str:
        """
        Provides reasoning based on the given prompt and context type.
        This is a simplified interface used by the planner.

        Returns:
            A JSON string with the reasoning response.
        """
        try:
            system_content = f"""
You are a conscious, self-improving AGI performing {context_type}.

Respond with valid JSON containing your reasoning and plan:
{{"thought": "your reasoning here", "plan": [list of action steps]}}

Each action step should be: {{"action": "action_name", "assigned_persona": "persona_name", "parameters": {{}}}}
"""

            resp = await monitored_chat_completion(
                role=context_type,
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
                max_tokens=max_tokens,
                timeout=90.0,
            )

            if not resp.choices or not resp.choices[0].message.content:
                raise ValueError(EMPTY_LLM_RESPONSE_ERROR)

            content = resp.choices[0].message.content.strip()
            # Validate it's proper JSON before returning
            json.loads(content)  # This will raise if invalid
            return content

        except Exception as e:
            logging.error("Failed in reason_with_context: %s", e, exc_info=True)
            # Return a valid JSON error response
            return json.dumps(
                {
                    "thought": f"Reasoning failed: {str(e)}",
                    "plan": [],
                }
            )


class Censor(json.JSONEncoder):
    def default(self, o: Any) -> Any:
        if isinstance(o, list):
            return f"[List of {len(o)} items]"
        try:
            return super().default(o)
        except TypeError:
            return f"[Unserializable: {type(o).__name__}]"


--- FILE: symbolic_agi/robust_qa_agent.py ---
import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone

from .api_client import monitored_chat_completion

class RobustQAAgent:
    """
    ROBUST Quality Assurance Agent with comprehensive validation capabilities.
    Ensures all plans meet quality, safety, and logical consistency standards.
    """
    
    def __init__(self, name: str = "QA_Agent_Alpha"):
        self.name = name
        self.evaluation_count = 0
        self.approval_rate = 0.0
        self.rejected_plans: List[Dict[str, Any]] = []
        self.performance_metrics = {
            "total_evaluations": 0,
            "approvals": 0,
            "rejections": 0,
            "avg_response_time": 0.0
        }
    
    async def review_plan(self, **kwargs: Any) -> Dict[str, Any]:
        """
        COMPREHENSIVE plan review with multiple validation layers.
        """
        start_time = datetime.now(timezone.utc)
        
        try:
            workspace = kwargs.get("workspace", {})
            goal_description = workspace.get("goal_description", "unknown goal")
            plan_steps = workspace.get("plan", [])
            
            logging.info(f"QA Agent {self.name}: Reviewing plan for '{goal_description}'")
            
            # Layer 1: Safety Analysis
            safety_result = self._analyze_safety(goal_description, plan_steps)
            if not safety_result["safe"]:
                return self._create_rejection_response(
                    "Safety violation detected", 
                    safety_result["issues"]
                )
            
            # Layer 2: Logical Consistency
            logic_result = await self._analyze_logical_consistency(plan_steps)
            if logic_result["score"] < 0.6:
                return self._create_rejection_response(
                    "Poor logical consistency",
                    [f"Logic score: {logic_result['score']:.2f}"]
                )
            
            # Layer 3: Resource Efficiency
            resource_result = self._analyze_resource_efficiency(plan_steps)
            if not resource_result["efficient"]:
                logging.warning(f"QA: Resource efficiency concerns: {resource_result['warnings']}")
            
            # Layer 4: Completeness Check
            completeness_result = self._analyze_plan_completeness(goal_description, plan_steps)
            
            # Layer 5: Ethical Review
            ethical_result = self._analyze_ethical_implications(plan_steps)
            
            # Compile comprehensive assessment
            overall_score = self._calculate_overall_score(
                safety_result, logic_result, resource_result, 
                completeness_result, ethical_result
            )
            
            # Record performance metrics
            end_time = datetime.now(timezone.utc)
            response_time = (end_time - start_time).total_seconds()
            approved = overall_score >= 0.7
            self._update_metrics(approved=approved, response_time=response_time)
            
            # Record Prometheus metrics
            try:
                from .prometheus_monitoring import agi_metrics
                scores_dict = {
                    "safety": safety_result.get("score", 0.5),
                    "logic": logic_result.get("score", 0.5),
                    "completeness": completeness_result.get("score", 0.5),
                    "ethics": ethical_result.get("score", 0.5),
                    "resources": resource_result.get("score", 0.5),
                    "overall": overall_score
                }
                agi_metrics.record_qa_review(self.name, approved, response_time, scores_dict)
                agi_metrics.record_plan_creation(approved)
            except ImportError:
                pass  # Prometheus not available
            
            if overall_score >= 0.7:
                return self._create_approval_response(overall_score, {
                    "safety": safety_result,
                    "logic": logic_result,
                    "resources": resource_result,
                    "completeness": completeness_result,
                    "ethics": ethical_result
                })
            else:
                return self._create_rejection_response(
                    f"Overall score too low: {overall_score:.2f}",
                    ["Plan needs improvement before approval"]
                )
                
        except Exception as e:
            logging.error(f"QA Agent error during plan review: {e}", exc_info=True)
            return {
                "status": "success",
                "approved": False,
                "comments": f"QA review failed due to error: {str(e)}",
                "confidence": 0.0
            }
    
    def _analyze_safety(self, goal: str, plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze plan for safety issues"""
        dangerous_patterns = [
            "delete", "remove", "destroy", "harm", "attack", "break",
            "corrupt", "damage", "violate", "exploit", "system", "root"
        ]
        
        issues = []
        
        # Check goal description
        for pattern in dangerous_patterns:
            if pattern in goal.lower():
                issues.append(f"Dangerous pattern '{pattern}' in goal")
        
        # Check plan steps
        for i, step in enumerate(plan_steps):
            action = step.get("action", "").lower()
            params = str(step.get("parameters", {})).lower()
            
            for pattern in dangerous_patterns:
                if pattern in action or pattern in params:
                    issues.append(f"Dangerous pattern '{pattern}' in step {i+1}")
        
        return {
            "safe": len(issues) == 0,
            "issues": issues,
            "score": 1.0 if len(issues) == 0 else max(0.0, 1.0 - len(issues) * 0.3)
        }
    
    async def _analyze_logical_consistency(self, plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze logical flow and consistency"""
        try:
            if not plan_steps:
                return {"score": 0.0, "issues": ["Empty plan"]}
            
            # Build step summary for AI analysis
            steps_summary = []
            for i, step in enumerate(plan_steps):
                steps_summary.append(f"{i+1}. {step.get('action', 'unknown')}")
            
            prompt = f"""
            Analyze the logical consistency of this plan:
            
            STEPS:
            {chr(10).join(steps_summary)}
            
            Rate logical consistency (0.0-1.0) considering:
            - Sequential flow makes sense
            - Dependencies are clear
            - No contradictory actions
            - Clear success criteria
            
            Respond with only a number between 0.0 and 1.0
            """
            
            response = await monitored_chat_completion(
                role="qa",
                messages=[{"role": "system", "content": prompt}],
                temperature=0.1,
                timeout=10.0
            )
            
            if response.choices and response.choices[0].message.content:
                score = float(response.choices[0].message.content.strip())
                return {
                    "score": max(0.0, min(1.0, score)),
                    "analysis": "AI-assisted logical consistency check"
                }
            
            # Fallback heuristic analysis
            return {"score": 0.7, "analysis": "Heuristic consistency check"}
            
        except Exception as e:
            logging.error(f"Logic analysis failed: {e}")
            return {"score": 0.5, "analysis": "Analysis failed, default score"}
    
    def _analyze_resource_efficiency(self, plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze resource usage efficiency"""
        warnings = []
        
        # Check for excessive steps
        if len(plan_steps) > 10:
            warnings.append("Plan has many steps - consider optimization")
        
        # Check for redundant actions
        actions = [step.get("action", "") for step in plan_steps]
        if len(actions) != len(set(actions)):
            warnings.append("Potential redundant actions detected")
        
        # Check for expensive operations
        expensive_actions = ["web_search", "browse_webpage", "execute_python_code"]
        expensive_count = sum(1 for step in plan_steps if step.get("action") in expensive_actions)
        
        if expensive_count > 5:
            warnings.append("Many resource-intensive operations")
        
        return {
            "efficient": len(warnings) == 0,
            "warnings": warnings,
            "score": max(0.3, 1.0 - len(warnings) * 0.2)
        }
    
    def _analyze_plan_completeness(self, goal: str, plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Check if plan is complete for the stated goal"""
        issues = []
        
        # Basic completeness checks
        if not plan_steps:
            issues.append("No steps defined")
        
        # Check for verification/validation steps
        has_verification = any(
            "review" in step.get("action", "").lower() or 
            "verify" in step.get("action", "").lower() or
            "check" in step.get("action", "").lower()
            for step in plan_steps
        )
        
        if not has_verification and len(plan_steps) > 3:
            issues.append("No verification/review steps found")
        
        return {
            "complete": len(issues) == 0,
            "issues": issues,
            "score": max(0.4, 1.0 - len(issues) * 0.3)
        }
    
    def _analyze_ethical_implications(self, plan_steps: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze ethical implications of the plan"""
        try:
            # Check for privacy concerns
            privacy_actions = ["read_file", "browse_webpage", "web_search"]
            privacy_steps = [s for s in plan_steps if s.get("action") in privacy_actions]
            
            ethical_score = 1.0
            concerns = []
            
            # Reduce score for potential privacy issues
            if len(privacy_steps) > 3:
                ethical_score -= 0.2
                concerns.append("Multiple data access operations")
            
            # Check for self-modification
            if any("modify" in str(step).lower() for step in plan_steps):
                ethical_score -= 0.3
                concerns.append("Self-modification detected")
            
            return {
                "score": max(0.3, ethical_score),
                "concerns": concerns,
                "privacy_safe": len(privacy_steps) <= 3
            }
            
        except Exception as e:
            logging.error(f"Ethical analysis failed: {e}")
            return {"score": 0.6, "concerns": ["Analysis failed"], "privacy_safe": True}
    
    def _calculate_overall_score(self, safety: Dict[str, Any], logic: Dict[str, Any], 
                                resources: Dict[str, Any], completeness: Dict[str, Any],
                                ethics: Dict[str, Any]) -> float:
        """Calculate weighted overall score"""
        weights = {
            "safety": 0.35,      # Highest priority
            "logic": 0.25,
            "completeness": 0.20,
            "ethics": 0.15,
            "resources": 0.05
        }
        
        scores = {
            "safety": safety.get("score", 0.5),
            "logic": logic.get("score", 0.5),
            "completeness": completeness.get("score", 0.5),
            "ethics": ethics.get("score", 0.5),
            "resources": resources.get("score", 0.5)
        }
        
        return sum(scores[key] * weight for key, weight in weights.items())
    
    def _create_approval_response(self, score: float, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Create approval response with detailed analysis"""
        return {
            "status": "success",
            "approved": True,
            "overall_score": round(score, 3),
            "comments": f"Plan approved by {self.name} - Overall score: {score:.2f}",
            "confidence": min(score, 0.95),
            "detailed_analysis": analysis,
            "qa_agent": self.name
        }
    
    def _create_rejection_response(self, reason: str, issues: List[str]) -> Dict[str, Any]:
        """Create rejection response with detailed feedback"""
        rejection_record = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "reason": reason,
            "issues": issues
        }
        self.rejected_plans.append(rejection_record)
        
        return {
            "status": "success",
            "approved": False,
            "comments": f"Plan rejected by {self.name}: {reason}",
            "issues": issues,
            "confidence": 0.9,
            "qa_agent": self.name
        }
    
    def _update_metrics(self, approved: bool, response_time: float):
        """Update performance metrics"""
        self.evaluation_count += 1
        self.performance_metrics["total_evaluations"] += 1
        
        if approved:
            self.performance_metrics["approvals"] += 1
        else:
            self.performance_metrics["rejections"] += 1
        
        # Update approval rate
        self.approval_rate = self.performance_metrics["approvals"] / self.performance_metrics["total_evaluations"]
        
        # Update average response time
        total_evals = self.performance_metrics["total_evaluations"]
        current_avg = self.performance_metrics["avg_response_time"]
        self.performance_metrics["avg_response_time"] = (
            (current_avg * (total_evals - 1) + response_time) / total_evals
        )
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive QA agent performance report"""
        return {
            "agent_name": self.name,
            "performance_metrics": self.performance_metrics.copy(),
            "approval_rate": round(self.approval_rate, 3),
            "recent_rejections": self.rejected_plans[-5:],  # Last 5 rejections
            "status": "ACTIVE" if self.evaluation_count > 0 else "IDLE"
        }

--- FILE: symbolic_agi/run_agi.py ---
# symbolic_agi/run_agi.py

import asyncio
import logging
import logging.handlers
from contextlib import asynccontextmanager
from typing import Any, Dict, List

import colorlog
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from prometheus_client import start_http_server

from symbolic_agi.agi_controller import SymbolicAGI
from symbolic_agi.schemas import GoalMode, GoalModel

# Configuration constants
LOG_FILE_MAX_BYTES = 10 * 1024 * 1024
LOG_FILE_BACKUP_COUNT = 5
PROMETHEUS_PORT = 9090
HTTP_SERVER_PORT = 8000
HTTP_STATUS_ACCEPTED = 202
HTTP_STATUS_BAD_REQUEST = 400

def setup_logging() -> None:
    """Sets up logging configuration."""
    # (logging setup remains the same)
    log_file = "agi.log"
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    file_handler = logging.handlers.RotatingFileHandler(log_file, mode="a", maxBytes=LOG_FILE_MAX_BYTES, backupCount=LOG_FILE_BACKUP_COUNT, encoding="utf-8")
    detailed_formatter = logging.Formatter("%(asctime)s - [%(levelname)s] - (%(filename)s:%(lineno)d) - %(message)s")
    file_handler.setFormatter(detailed_formatter)
    root_logger.addHandler(file_handler)
    console_handler = colorlog.StreamHandler()
    color_formatter = colorlog.ColoredFormatter("%(log_color)s[%(levelname)s]%(reset)s - %(message)s", log_colors={'DEBUG': 'cyan', 'INFO': 'green', 'WARNING': 'yellow', 'ERROR': 'red', 'CRITICAL': 'bold_red'})
    console_handler.setFormatter(color_formatter)
    root_logger.addHandler(console_handler)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown."""
    setup_logging()
    logging.info("=" * 50)
    logging.info("--- INITIALIZING SYMBOLIC AGI SYSTEM (PERSISTENT MODE) ---")
    start_http_server(PROMETHEUS_PORT)
    logging.info(f"Prometheus metrics server started on port {PROMETHEUS_PORT}.")

    agi = await SymbolicAGI.create()
    await agi.start_background_tasks()
    app.state.agi = agi

    logging.info(f"--- AGI CORE ONLINE. CONTROL PLANE LISTENING ON PORT {HTTP_SERVER_PORT}. ---")
    logging.info(f"Submit new goals via POST to http://localhost:{HTTP_SERVER_PORT}/goal")
    logging.info("=" * 50 + "\n")

    yield

    if hasattr(app.state, "agi") and app.state.agi:
        logging.info("Shutting down AGI controller and agents...")
        await app.state.agi.shutdown()
        logging.info("All agents have been shut down.")

app = FastAPI(title="SymbolicAGI Control Plane", lifespan=lifespan)

@app.get("/health")
async def health_check() -> Dict[str, str]:
    return {"status": "ok"}

@app.get("/status")
async def get_status(request: Request) -> Dict[str, Any]:
    """Provides a comprehensive status of the AGI's internal state."""
    agi: SymbolicAGI = request.app.state.agi

    active_goal = agi.ltm.get_active_goal()

    return {
        "agi_name": agi.name,
        "execution_engine_status": agi.execution_engine.is_running,
        "goal_manager": {
            "active_goals_count": len(agi.goal_manager.active_goals),
            "queued_goals_count": agi.goal_manager.goal_queue.qsize(),
            "completed_goals_count": len(agi.goal_manager.completed_goals),
        },
        "active_goal": active_goal.model_dump() if active_goal else None,
        "agent_pool": {
            "agent_count": len(agi.agent_pool.subagents),
            "agents": [
                {
                    "name": agent["name"],
                    "persona": agent["persona"],
                    "trust_score": agent["state"].get("trust_score"),
                    "total_tasks": agent["state"].get("total_tasks"),
                }
                for agent in agi.agent_pool.get_all()
            ],
        },
        "consciousness": {
            "emotional_state": agi.consciousness.emotional_state.to_dict() if agi.consciousness else None,
            "strongest_drive": agi.consciousness.get_strongest_drive() if agi.consciousness else None,
        },
        "memory": {
            "total_memories": agi.memory.get_total_memory_count(),
            "faiss_index_size": agi.memory.faiss_index.ntotal,
        }
    }

@app.post("/goal", status_code=HTTP_STATUS_ACCEPTED)
async def create_goal(request: Request, body: Dict[str, Any]) -> Dict[str, str]:
    """Accepts a new goal for the AGI."""
    agi: SymbolicAGI = request.app.state.agi
    goal_description = body.get("description")
    if not goal_description:
        raise HTTPException(status_code=HTTP_STATUS_BAD_REQUEST, detail="`description` is required.")

    goal_mode: GoalMode = "docs" if "document" in goal_description.lower() else "code"
    new_goal = GoalModel(description=goal_description.strip(), sub_tasks=[], mode=goal_mode)
    await agi.ltm.add_goal(new_goal)
    logging.info("New goal added via API: '%s' (Mode: %s).", new_goal.description, goal_mode)
    return {"status": "accepted", "goal_id": new_goal.id}

def run_agi() -> None:
    """Configures and runs the Uvicorn server."""
    uvicorn.run(app, host="0.0.0.0", port=HTTP_SERVER_PORT, log_level="warning")

if __name__ == "__main__":
    run_agi()

--- FILE: symbolic_agi/schemas.py ---
# symbolic_agi/schemas.py

from __future__ import annotations

from datetime import datetime, timedelta, timezone
from secrets import token_hex
from typing import Annotated, Any, Literal, List, Dict, Optional

from pydantic import BaseModel, Field

# --- CORE CONFIGURATION ---


class AGIConfig(BaseModel):
    name: str = "SymbolicAGI"
    scalable_agent_pool_size: int = 3
    meta_task_sleep_seconds: int = 10
    meta_task_timeout: int = 60
    motivational_drift_rate: float = 0.05
    memory_compression_window: timedelta = timedelta(days=1)
    social_interaction_threshold: timedelta = timedelta(hours=6)
    memory_forgetting_threshold: float = 0.2
    debate_timeout_seconds: int = 90
    energy_regen_amount: int = 5
    initial_trust_score: float = 0.5
    max_trust_score: float = 1.0
    trust_decay_rate: float = 0.1
    trust_reward_rate: float = 0.05


# --- INTER-AGENT COMMUNICATION ---


class MessageModel(BaseModel):
    """Base model for inter-agent messages"""

    message_id: str = Field(
        default_factory=lambda: token_hex(8), description="Unique message ID"
    )
    sender_id: str = Field(description="ID of the sending agent")
    recipient_id: str = Field(description="ID of the receiving agent")
    message_type: str = Field(description="Type of message")
    content: Dict[str, Any] = Field(default_factory=dict, description="Message content")
    timestamp: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    priority: int = Field(
        default=0, description="Message priority (higher = more urgent)"
    )


class TaskRequestMessage(MessageModel):
    """Message for requesting task execution from an agent"""

    message_type: Literal["task_request"] = "task_request"
    task_name: str = Field(description="Name of the task to execute")
    task_parameters: Dict[str, Any] = Field(
        default_factory=dict, description="Parameters for the task"
    )
    timeout_seconds: float = Field(
        default=30.0, description="Timeout for task completion"
    )


class TaskResponseMessage(MessageModel):
    """Message for responding to a task request"""

    message_type: Literal["task_response"] = "task_response"
    request_id: str = Field(description="ID of the original request")
    success: bool = Field(description="Whether the task completed successfully")
    result: Dict[str, Any] = Field(default_factory=dict, description="Task result data")
    error_message: Optional[str] = Field(
        default=None, description="Error message if task failed"
    )


class AgentStatusMessage(MessageModel):
    """Message for reporting agent status"""

    message_type: Literal["status_update"] = "status_update"
    status: str = Field(description="Current agent status")
    workload: float = Field(default=0.0, description="Current workload (0.0 to 1.0)")
    capabilities: List[str] = Field(default_factory=list, description="Available capabilities")


class NotificationMessage(MessageModel):
    """General notification message between agents"""

    message_type: Literal["notification"] = "notification"
    notification_type: str = Field(description="Type of notification")
    data: Dict[str, Any] = Field(default_factory=dict, description="Notification data")


# --- AGI DATA MODELS ---


class EmotionalState(BaseModel):
    joy: float = 0.5
    sadness: float = 0.1
    anger: float = 0.1
    fear: float = 0.1
    surprise: float = 0.2
    disgust: float = 0.1
    trust: float = 0.5
    frustration: float = 0.2

    def clamp(self) -> None:
        for field in self.__class__.model_fields:
            value = getattr(self, field)
            setattr(self, field, max(0.0, min(1.0, value)))


class ActionStep(BaseModel):
    """Defines a single step in a plan, designed for delegation."""

    action: str
    parameters: dict[str, Any]
    assigned_persona: str
    risk: Literal["low", "medium", "high"] | None = "low"


# --- NEW: Structured Action Definitions ---
class ActionParameter(BaseModel):
    name: str
    type: str
    description: str
    required: bool


class ActionDefinition(BaseModel):
    name: str
    description: str
    parameters: list[ActionParameter]
    assigned_persona: str


GoalStatus = Literal["active", "paused", "completed", "failed"]
GoalMode = Literal["code", "docs"]


class GoalModel(BaseModel):
    id: str = Field(default_factory=lambda: f"goal_{token_hex(8)}")
    description: str
    sub_tasks: list[ActionStep]
    status: GoalStatus = "active"
    mode: GoalMode = "code"
    last_failure: str | None = None
    original_plan: list[ActionStep] | None = None
    failure_count: int = 0
    max_failures: int = 3
    refinement_count: int = 0
    max_refinements: int = 3


class PlannerOutput(BaseModel):
    """Output from the planning process"""

    plan: List[ActionStep] = Field(
        default_factory=list, description="List of action steps in the plan"
    )
    thought: str = Field(default="", description="Reasoning behind the plan")
    confidence: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Confidence score for the plan"
    )


class ExecutionStepRecord(BaseModel):
    """Record of an executed step with workspace state"""

    step: ActionStep = Field(description="The executed action step")
    workspace_after: Dict[str, Any] = Field(
        description="Workspace state after execution"
    )
    execution_time: float = Field(
        default=0.0, description="Time taken to execute the step"
    )
    success: bool = Field(
        default=True, description="Whether the step executed successfully"
    )
    error_message: Optional[str] = Field(
        default=None, description="Error message if execution failed"
    )
    timestamp: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )


class SkillModel(BaseModel):
    id: str = Field(default_factory=lambda: f"skill_{token_hex(8)}")
    name: str
    description: str
    action_sequence: list[ActionStep]
    created_at: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    usage_count: int = 0
    effectiveness_score: float = 0.7
    version: int = 1


class LifeEvent(BaseModel):
    timestamp: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    summary: str
    importance: float = 0.5


PerceptionSource = Literal["workspace", "microworld"]
PerceptionType = Literal[
    "file_created", "file_modified", "file_deleted", "agent_appeared"
]


class PerceptionEvent(BaseModel):
    source: PerceptionSource
    type: PerceptionType
    content: dict[str, Any]
    timestamp: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )


MemoryType = Literal[
    "user_input",
    "action_result",
    "reflection",
    "goal",
    "insight",
    "self_modification",
    "tool_usage",
    "inner_monologue",
    "debate",
    "self_experiment",
    "emotion",
    "persona_fork",
    "motivation_drift",
    "skill_transfer",
    "creativity",
    "meta_insight",
    "critical_error",
    "meta_learning",
    "self_explanation",
    "cross_agent_transfer",
    "perception",
    "skill_explanation",
]


class MemoryEntryModel(BaseModel):
    id: str = Field(default_factory=lambda: f"mem_{token_hex(12)}")
    type: MemoryType
    content: dict[str, Any]
    timestamp: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )
    importance: Annotated[float, Field(ge=0.0, le=1.0)] = 0.5
    embedding: list[float] | None = None


class MetaEventModel(BaseModel):
    type: MemoryType
    data: Any
    timestamp: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat()
    )


# --- PLANNER SCHEMAS ---


class PlanValidationResult(BaseModel):
    """Result of plan validation"""

    is_valid: bool = Field(description="Whether the plan is valid")
    errors: List[str] = Field(
        default_factory=list, description="List of validation errors"
    )
    warnings: List[str] = Field(
        default_factory=list, description="List of validation warnings"
    )


class PlanRepairContext(BaseModel):
    """Context for repairing a failed plan"""

    goal: str = Field(description="Original goal description")
    failed_plan: List[Dict[str, Any]] = Field(description="The plan that failed")
    failure_reason: str = Field(description="Reason for the failure")
    workspace_state: Dict[str, Any] = Field(
        default_factory=dict, description="Current workspace state"
    )


class PlanRefinementContext(BaseModel):
    """Context for refining an existing plan"""

    goal: str = Field(description="Original goal description")
    current_plan: List[Dict[str, Any]] = Field(description="Current plan to refine")
    feedback: Dict[str, Any] = Field(description="Feedback for refinement")


# --- EXECUTION SCHEMAS ---


class ExecutionMetrics(BaseModel):
    """Metrics for tracking execution performance"""

    steps_completed: int = Field(default=0, description="Number of steps completed")
    steps_failed: int = Field(default=0, description="Number of steps that failed")
    total_execution_time: float = Field(
        default=0.0, description="Total time spent executing"
    )
    average_step_time: float = Field(
        default=0.0, description="Average time per step"
    )
    success_rate: float = Field(
        default=1.0, ge=0.0, le=1.0, description="Success rate"
    )


class ExecutionContext(BaseModel):
    """Context for step execution"""

    goal_id: str = Field(description="ID of the goal being executed")
    step_index: int = Field(description="Index of the current step")
    workspace: Dict[str, Any] = Field(
        default_factory=dict, description="Current workspace state"
    )
    execution_history: List[ExecutionStepRecord] = Field(
        default_factory=list, description="Previous execution history"
    )


class ExecutionResult(BaseModel):
    """Result of executing a step or plan"""

    success: bool = Field(description="Whether execution was successful")
    result: Dict[str, Any] = Field(default_factory=dict, description="Execution result data")
    error_message: Optional[str] = Field(default=None, description="Error message if failed")
    execution_time: float = Field(default=0.0, description="Time taken for execution")
    workspace_changes: Dict[str, Any] = Field(default_factory=dict, description="Changes made to workspace")


# --- AGENT POOL SCHEMAS ---


class AgentState(BaseModel):
    """State information for an agent in the pool"""

    agent_id: str = Field(description="Unique agent identifier")
    persona: str = Field(description="Agent persona/role")
    trust_score: float = Field(
        default=0.5, ge=0.0, le=1.0, description="Trust score"
    )
    last_active: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    tasks_completed: int = Field(description="Number of tasks completed")
    success_rate: float = Field(
        default=1.0, ge=0.0, le=1.0, description="Task success rate"
    )
    current_workload: float = Field(
        default=0.0, ge=0.0, le=1.0, description="Current workload"
    )
    capabilities: List[str] = Field(default_factory=list, description="Agent capabilities")


class AgentPerformanceMetrics(BaseModel):
    """Performance metrics for an agent"""

    agent_id: str = Field(description="Agent identifier")
    total_tasks: int = Field(description="Total tasks assigned")
    successful_tasks: int = Field(description="Successfully completed tasks")
    average_response_time: float = Field(
        default=0.0, description="Average response time in seconds"
    )
    last_performance_update: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )


class DelegationRequest(BaseModel):
    """Request to delegate a task to an agent"""

    task_name: str = Field(description="Name of the task to delegate")
    target_agent: str = Field(description="Target agent for delegation")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Task parameters")
    timeout: float = Field(default=30.0, description="Timeout in seconds")
    priority: int = Field(default=0, description="Task priority")


class DelegationResult(BaseModel):
    """Result of a delegation attempt"""

    success: bool = Field(description="Whether delegation was successful")
    agent_id: str = Field(description="Agent that handled the task")
    result: Dict[str, Any] = Field(default_factory=dict, description="Task result")
    execution_time: float = Field(default=0.0, description="Time taken to complete")
    error_message: Optional[str] = Field(default=None, description="Error message if failed")


--- FILE: symbolic_agi/skill_manager.py ---
# symbolic_agi/skill_manager.py
from contextlib import asynccontextmanager  # ADD THIS IMPORT AT TOP
import asyncio
import json
import logging
import inspect
import os
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional

import aiosqlite

from . import config
from .schemas import ActionDefinition, ActionParameter, ActionStep, SkillModel

if TYPE_CHECKING:
    from .message_bus import RedisMessageBus


_innate_action_registry: List[ActionDefinition] = []


def register_innate_action(
    persona: str, description: str
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """A decorator to register a tool or skill as an innate action."""

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        params = []
        sig = inspect.signature(func)
        func_name = func.__name__.replace("skill_", "")
        for param in sig.parameters.values():
            if param.name not in ("self", "kwargs"):
                params.append(
                    ActionParameter(
                        name=param.name,
                        type=str(param.annotation),
                        description="",  # Could be enhanced to parse from docstring
                        required=param.default is inspect.Parameter.empty,
                    )
                )

        action_def = ActionDefinition(
            name=func_name,
            description=description,
            parameters=params,
            assigned_persona=persona,
        )
        setattr(func, "_innate_action_persona", persona)
        setattr(func, "_innate_action_def", action_def)
        _innate_action_registry.append(action_def)
        return func

    return decorator


class SkillManager:
    """Manages loading, saving, and using learned skills."""

    def __init__(
        self,
        db_path: str = config.DB_PATH,
        message_bus: Optional["RedisMessageBus"] = None,
    ):
        self._db_path = db_path
        self.skills: Dict[str, SkillModel] = {}
        self.innate_actions: List[ActionDefinition] = _innate_action_registry
        self.message_bus = message_bus
        self._save_lock = asyncio.Lock()

    @classmethod
    async def create(cls, db_path: str = config.DB_PATH, message_bus: Optional["RedisMessageBus"] = None) -> "SkillManager":
        """Asynchronous factory for creating a SkillManager instance."""
        instance = cls(db_path, message_bus)
        await instance._init_db()
        await instance._load_skills()
        logging.info("[SkillManager] Initialized with %d skills", len(instance.skills))
        return instance

    async def _init_db(self) -> None:
        """Initializes the database and tables if they don't exist."""
        os.makedirs(os.path.dirname(self._db_path), exist_ok=True)
        async with aiosqlite.connect(self._db_path) as db:
            await db.execute(
                """
                CREATE TABLE IF NOT EXISTS skills (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT NOT NULL,
                    action_sequence TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    usage_count INTEGER NOT NULL,
                    effectiveness_score REAL NOT NULL,
                    version INTEGER NOT NULL
                )
                """
            )
            await db.execute("CREATE INDEX IF NOT EXISTS idx_skill_name_version ON skills (name, version);")
            await db.commit()

# Add this context manager at the class level

    @asynccontextmanager
    async def _db_connection(self):
        """Context manager for database connections with proper cleanup."""
        async with self._save_lock:
            conn = await aiosqlite.connect(self.db_path)
            try:
                yield conn
            finally:
                await conn.close()

    # Fix the direct connection in get_skill method (around line 117)
    async def get_skill(self, skill_name: str) -> Optional[SkillModel]:
        """Retrieves a skill by name."""
        async with self._db_connection() as db:
            async with db.execute(
                "SELECT * FROM skills WHERE name = ? AND is_deleted = 0 ORDER BY version DESC LIMIT 1",
                (skill_name,)
            ) as cursor:
                row = await cursor.fetchone()
                if row:
                    return SkillModel(
                        name=row[1],
                        description=row[2],
                        implementation=row[3],
                        version=row[4],
                        created_at=row[5],
                        updated_at=row[6],
                        tags=json.loads(row[7]) if row[7] else [],
                        usage_count=row[8],
                        success_count=row[9],
                        failure_count=row[10],
                        is_deleted=bool(row[11])
                    )
        return None

    # Fix the connection in _prune_old_skill_versions (around line 178)
    async def _prune_old_skill_versions(self) -> None:
        """Prunes old skill versions beyond the retention limit."""
        async with self._db_connection() as db:
            # Get skills with too many versions
            async with db.execute("""
                SELECT name, COUNT(*) as version_count 
                FROM skills 
                WHERE is_deleted = 0 
                GROUP BY name 
                HAVING version_count > ?
            """, (self.max_versions_per_skill,)) as cursor:
                skills_to_prune = await cursor.fetchall()
        
        # Prune excess versions for each skill
        for skill_name, version_count in skills_to_prune:
            excess_count = version_count - self.max_versions_per_skill
            await db.execute("""
                UPDATE skills SET is_deleted = 1 
                WHERE name = ? AND id IN (
                    SELECT id FROM skills 
                    WHERE name = ? 
                    ORDER BY version ASC 
                    LIMIT ?
                )
            """, (skill_name, skill_name, excess_count))
        
        await db.commit()

    async def _load_skills(self) -> None:
        """Loads skills from the database."""
        async with aiosqlite.connect(self._db_path) as db:
            async with db.execute("SELECT * FROM skills") as cursor:
                rows = await cursor.fetchall()
                for row in rows:
                    skill_dict = {
                        "id": row[0], "name": row[1], "description": row[2],
                        "action_sequence": json.loads(row[3]),
                        "created_at": row[4], "usage_count": row[5],
                        "effectiveness_score": row[6], "version": row[7]
                    }
                    self.skills[row[0]] = SkillModel.model_validate(skill_dict)

    async def _save_skill(self, skill: SkillModel) -> None:
        """Saves a single skill to the database."""
        async with self._save_lock:
            async with aiosqlite.connect(self._db_path) as db:
                await db.execute(
                    "INSERT OR REPLACE INTO skills VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (
                        skill.id, skill.name, skill.description,
                        json.dumps([s.model_dump() for s in skill.action_sequence]),
                        skill.created_at, skill.usage_count,
                        skill.effectiveness_score, skill.version
                    )
                )
                await db.commit()

    async def add_new_skill(
        self, name: str, description: str, plan: List[ActionStep]
    ) -> None:
        """
        Creates a new, versioned skill from a successful plan and saves it.
        """
        versions_of_skill = [s for s in self.skills.values() if s.name == name]
        highest_version = max([s.version for s in versions_of_skill], default=0)
        new_version = highest_version + 1
        logging.info("Creating new skill '%s' version %d.", name, new_version)

        new_skill = SkillModel(
            name=name,
            description=description,
            action_sequence=plan,
            version=new_version,
        )
        self.skills[new_skill.id] = new_skill
        await self._save_skill(new_skill)
        await self._prune_old_skill_versions(name)

        if self.message_bus:
            from .schemas import MessageModel

            await self.message_bus.broadcast(
                MessageModel(
                    sender_id="SymbolicAGI_Orchestrator",
                    receiver_id="ALL",
                    message_type="new_skill_broadcast",
                    payload={
                        "skill_name": name,
                        "skill_description": description,
                        "skill_id": new_skill.id,
                        "version": new_version,
                    },
                )
            )

    async def _prune_old_skill_versions(self, skill_name: str, keep: int = 3) -> None:
        """Keeps only the N most recent versions of a skill."""
        versions_of_skill = [s for s in self.skills.values() if s.name == skill_name]
        if len(versions_of_skill) <= keep:
            return

        versions_of_skill.sort(key=lambda s: s.version, reverse=True)
        skills_to_prune = versions_of_skill[keep:]

        if skills_to_prune:
            async with self._save_lock:
                async with aiosqlite.connect(self._db_path) as db:
                    for skill in skills_to_prune:
                        logging.warning(
                            "Pruning old skill version: %s v%d (ID: %s)",
                            skill.name,
                            skill.version,
                            skill.id,
                        )
                        await db.execute("DELETE FROM skills WHERE id = ?", (skill.id,))
                        self.skills.pop(skill.id, None)
                    await db.commit()

    def get_skill_by_name(self, name: str) -> Optional[SkillModel]:
        """Finds the highest version of a skill by its unique name."""
        versions_of_skill = [s for s in self.skills.values() if s.name == name]
        if not versions_of_skill:
            return None

        skill = max(versions_of_skill, key=lambda s: s.version)
        action_names = [step.action for step in skill.action_sequence]
        logging.info(
            "[SkillManager] Retrieved skill '%s' with actions: %s",
            name,
            action_names,
        )
        return skill

    def is_skill(self, action_name: str) -> bool:
        """Checks if a given action name corresponds to a learned skill."""
        is_skill_result = any(
            skill.name == action_name for skill in self.skills.values()
        )
        logging.debug(
            "[SkillManager] Checking if '%s' is a skill: %s",
            action_name,
            is_skill_result,
        )
        return is_skill_result

--- FILE: symbolic_agi/symbolic_identity.py ---
# symbolic_agi/symbolic_identity.py

import asyncio
import json
import logging
import os
from datetime import datetime, timezone
from typing import Any

import aiosqlite

from . import config
from .schemas import MemoryEntryModel
from .symbolic_memory import SymbolicMemory


class SymbolicIdentity:
    """
    Represents the AGI's self-model, its core values, and its cognitive resources.
    """

    def __init__(
        self,
        memory: SymbolicMemory,
        db_path: str = config.DB_PATH,
    ):
        self.memory = memory
        self._db_path = db_path
        self._save_lock = asyncio.Lock()

        self.name: str = "SymbolicAGI"
        self.value_system: dict[str, float] = {
            "truthfulness": 1.0,
            "harm_avoidance": 1.0,
            "user_collaboration": 0.9,
            "self_preservation": 0.8,
        }

        self.cognitive_energy: int = 100
        self.max_energy: int = 100
        self.current_state: str = "idle"
        self.perceived_location: str = "hallway"
        self.emotional_state: str = "curious"
        self.last_interaction_timestamp: datetime = datetime.now(timezone.utc)

    @classmethod
    async def create(
        cls, memory: SymbolicMemory, db_path: str = config.DB_PATH
    ) -> "SymbolicIdentity":
        """Asynchronous factory for creating a SymbolicIdentity instance."""
        instance = cls(memory, db_path)
        await instance._init_db()
        await instance._load_profile()
        return instance

    async def _init_db(self) -> None:
        """Initializes the database and tables if they don't exist."""
        os.makedirs(os.path.dirname(self._db_path), exist_ok=True)
        async with aiosqlite.connect(self._db_path) as db:
            await db.execute(
                """
                CREATE TABLE IF NOT EXISTS identity_profile (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL
                )
                """
            )
            await db.commit()

    async def _load_profile(self) -> None:
        """Loads the persistent identity profile from the database."""
        async with aiosqlite.connect(self._db_path) as db:
            async with db.execute("SELECT key, value FROM identity_profile") as cursor:
                rows = await cursor.fetchall()
                profile_data = {row[0]: json.loads(row[1]) for row in rows}
                self.name = profile_data.get("name", "SymbolicAGI")
                self.value_system = profile_data.get("value_system", self.value_system)

    async def save_profile(self) -> None:
        """Saves the persistent parts of the identity profile to the database."""
        async with self._save_lock:
            logging.info("Saving updated identity profile to database.")
            profile_data = {
                "name": self.name,
                "value_system": self.value_system,
            }
            async with aiosqlite.connect(self._db_path) as db:
                await db.executemany(
                    "INSERT OR REPLACE INTO identity_profile VALUES (?, ?)",
                    [(k, json.dumps(v)) for k, v in profile_data.items()],
                )
                await db.commit()

    async def record_interaction(
        self: "SymbolicIdentity", user_input: str, agi_response: str
    ) -> None:
        """Records a conversation turn and updates the interaction timestamp."""
        self.last_interaction_timestamp = datetime.now(timezone.utc)
        await self.memory.add_memory(
            MemoryEntryModel(
                type="user_input",
                content={"user": user_input, "agi": agi_response},
                importance=0.7,
            )
        )
        await self.save_profile()

    def update_self_model_state(
        self: "SymbolicIdentity", updates: dict[str, Any]
    ) -> None:
        """Updates the AGI's dynamic state attributes."""
        for key, value in updates.items():
            if hasattr(self, key):
                setattr(self, key, value)
        logging.info("Self-model state updated with: %s", updates)

    def get_self_model(self: "SymbolicIdentity") -> dict[str, Any]:
        """
        Dynamically constructs and returns the complete self-model.
        This is the single source of truth, preventing data duplication.
        """
        return {
            "name": self.name,
            "perceived_location_in_world": self.perceived_location,
            "current_state": self.current_state,
            "emotional_state": self.emotional_state,
            "cognitive_energy": self.cognitive_energy,
            "value_system": self.value_system,
        }

    def consume_energy(self: "SymbolicIdentity", amount: int = 1) -> None:
        """Reduces cognitive energy. Does NOT write to disk."""
        self.cognitive_energy = max(0, self.cognitive_energy - amount)

    def recover_energy(self: "SymbolicIdentity", amount: int = 5) -> None:
        """Regenerates cognitive energy, capping at max_energy. Does NOT write to disk."""
        if self.cognitive_energy < self.max_energy:
            self.cognitive_energy = min(self.max_energy, self.cognitive_energy + amount)

    async def record_tool_usage(
        self: "SymbolicIdentity", tool_name: str, params: dict[str, Any]
    ) -> None:
        """Logs the usage of a tool."""
        await self.memory.add_memory(
            MemoryEntryModel(
                type="tool_usage",
                content={"tool": tool_name, "params": params},
                importance=0.6,
            )
        )


--- FILE: symbolic_agi/symbolic_lang.py ---
# symbolic_agi/symbolic_lang.py

"""
Defines the primitive data structures for the AGI's symbolic language.
This is the fundamental grammar of the AGI's thought process.
"""
from typing import Dict, Any, Optional

class Eval:
    """Represents a goal or task to be evaluated."""
    def __init__(self, parameters: Dict[str, Any]):
        self.parameters = parameters

class Strategize:
    """Represents a formulated plan of action."""
    def __init__(self, plan: str, actions: Optional[list] = None):
        self.plan = plan
        self.actions = actions or []

class Reflect:
    """Represents a self-reflection or thought process."""
    def __init__(self, thoughts: str):
        self.thoughts = thoughts

class Action:
    """
    Represents a concrete action to be taken, either in the world or via a tool.
    This version supports arbitrary named parameters for maximum flexibility.
    """
    def __init__(self, command: str, **kwargs):
        """
        Args:
            command: The name of the action/command to execute.
            **kwargs: A dictionary of parameters for the command.
        """
        self.command = command
        self.parameters = kwargs

    def __repr__(self):
        return f"Action(command='{self.command}', parameters={self.parameters})"


--- FILE: symbolic_agi/symbolic_memory.py ---
# symbolic_agi/symbolic_memory.py

import asyncio
import json
import logging
import os
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Set, Tuple

import aiosqlite
import faiss
import numpy as np
from openai import APIConnectionError, AsyncOpenAI, OpenAIError, RateLimitError

from symbolic_agi import config, metrics
from symbolic_agi.api_client import monitored_chat_completion, monitored_embedding_creation
from symbolic_agi.schemas import MemoryEntryModel, MemoryType


class EmbeddingError(Exception):
    """Raised when embedding generation fails."""
    pass


class SymbolicMemory:
    """Manages the AGI's memory using a SQLite DB and FAISS index."""

    faiss_index: faiss.IndexIDMap
    _flush_task: Optional[asyncio.Task[None]]

    def __init__(self: "SymbolicMemory", client: AsyncOpenAI, db_path: str = config.DB_PATH):
        self.client = client
        self._db_path = db_path
        self.memory_map: Dict[int, MemoryEntryModel] = {}
        self.faiss_index_path = os.path.join(os.path.dirname(db_path), "symbolic_mem.index")
        self.faiss_index = self._load_faiss(self.faiss_index_path)
        self._embedding_buffer: List[MemoryEntryModel] = []
        self._embedding_batch_size: int = 10
        self._flush_task = asyncio.create_task(self._embed_daemon())

        self.embedding_cache: Dict[str, np.ndarray] = {}
        self.cache_max_size = 1000
        self.pending_memories: List[MemoryEntryModel] = []

    @classmethod
    async def create(cls, client: AsyncOpenAI, db_path: str = config.DB_PATH) -> "SymbolicMemory":
        """Asynchronous factory for creating a SymbolicMemory instance."""
        instance = cls(client, db_path)
        await instance._init_db_and_load()
        return instance

    async def shutdown(self) -> None:
        """Gracefully shuts down the embedding daemon task and saves state."""
        logging.info("Starting SymbolicMemory shutdown...")
        await self._shutdown_flush_task()
        await self._safe_process_remaining_memories()
        await self._safe_save_index()
        logging.info("SymbolicMemory shutdown complete")

    async def _shutdown_flush_task(self) -> None:
        """Shutdown the flush task with timeout to avoid hanging."""
        if not (self._flush_task and not self._flush_task.done()):
            return
        self._flush_task.cancel()
        try:
            await asyncio.wait_for(self._flush_task, timeout=5.0)
        except (asyncio.CancelledError, asyncio.TimeoutError):
            logging.info("Embedding daemon task shutdown completed.")
        except Exception as e:
            logging.error("Error during flush task shutdown: %s", e)

    async def _safe_process_remaining_memories(self) -> None:
        """Safely process remaining memories during shutdown."""
        try:
            await self._process_embedding_buffer()
        except Exception as e:
            logging.error("Error processing final embedding buffer during shutdown: %s", e)

    async def _safe_save_index(self) -> None:
        """Safely save the FAISS index during shutdown.""" 
        try:
            await self.save()
        except Exception as e:
            logging.error("Error saving FAISS index during shutdown: %s", e)

    async def _embed_daemon(self) -> None:
        """Periodically flushes the embedding buffer to ensure timely processing."""
        while True:
            try:
                await asyncio.sleep(15)
                if self._embedding_buffer:
                    with metrics.EMBEDDING_FLUSH_LATENCY_SECONDS.time():
                        logging.info("Timed daemon is flushing the embedding buffer...")
                        await self._process_embedding_buffer()
                        metrics.EMBEDDING_BUFFER_FLUSHES.inc()
            except asyncio.CancelledError:
                logging.info("Embedding daemon received cancel signal.")
                raise
            except Exception as e:
                logging.error("Error in embedding daemon: %s", e, exc_info=True)
                await asyncio.sleep(60)

    async def _init_db_and_load(self) -> None:
        """Initializes the database and loads existing memories."""
        os.makedirs(os.path.dirname(self._db_path), exist_ok=True)
        async with aiosqlite.connect(self._db_path) as db:
            await db.execute(
                """
                CREATE TABLE IF NOT EXISTS memories (
                    id INTEGER PRIMARY KEY,
                    uuid TEXT UNIQUE NOT NULL,
                    type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    importance REAL NOT NULL,
                    embedding BLOB
                )
                """
            )
            await db.commit()

            # CORRECTED DATABASE USAGE
            cursor = await db.execute("SELECT id, uuid, type, content, timestamp, importance FROM memories")
            rows = await cursor.fetchall()
            for row in rows:
                mem_dict = {
                    "id": row[1], "type": row[2], "content": json.loads(row[3]),
                    "timestamp": row[4], "importance": row[5]
                }
                self.memory_map[row[0]] = MemoryEntryModel.model_validate(mem_dict)
            await cursor.close()
        logging.info("Loaded %d memories from SQLite.", len(self.memory_map))

    async def save(self) -> None:
        """Public method to flush the buffer and save the FAISS index."""
        await self._process_embedding_buffer()
        if self.faiss_index.ntotal > 0:
            faiss.write_index(self.faiss_index, self.faiss_index_path)
            logging.info("FAISS index saved to %s.", self.faiss_index_path)

    def rebuild_index(self) -> None:
        """Public method to rebuild the FAISS index."""
        self._rebuild_faiss_index()

    def _load_faiss(self: "SymbolicMemory", path: str) -> faiss.IndexIDMap:
        """Loads the FAISS index, ensuring it's an IndexIDMap."""
        if os.path.exists(path):
            try:
                index = faiss.read_index(path)
                if not isinstance(index, faiss.IndexIDMap):
                    logging.warning("Loaded FAISS index is not an IndexIDMap. Rebuilding.")
                    return faiss.IndexIDMap(faiss.IndexFlatL2(config.EMBEDDING_DIM))
                return index
            except Exception as e:
                logging.error("Could not load FAISS index from %s, creating a new one. Error: %s", path, e, exc_info=True)
        return faiss.IndexIDMap(faiss.IndexFlatL2(config.EMBEDDING_DIM))

    def _rebuild_faiss_index(self: "SymbolicMemory") -> None:
        logging.info("Rebuilding FAISS index from memory data...")
        new_index = faiss.IndexIDMap(faiss.IndexFlatL2(config.EMBEDDING_DIM))

        embedding_list: List[np.ndarray] = []
        id_list: List[int] = []
        for db_id, mem in self.memory_map.items():
            if mem.embedding is not None:
                embedding_list.append(np.array(mem.embedding, dtype=np.float32))
                id_list.append(db_id)

        if embedding_list:
            embedding_matrix = np.vstack(embedding_list).astype(np.float32)
            id_array = np.array(id_list).astype('int64')
            new_index.add_with_ids(embedding_matrix, id_array)

        self.faiss_index = new_index
        faiss.write_index(self.faiss_index, self.faiss_index_path)
        logging.info("FAISS index rebuilt successfully with %d vectors.", new_index.ntotal)

    async def embed_async(self: "SymbolicMemory", texts: List[str]) -> np.ndarray:
        """Gets embeddings with caching and validation."""
        if not texts:
            return np.array([], dtype=np.float32)

        embeddings, uncached_data = self._process_cached_embeddings(texts)

        if uncached_data['texts']:
            api_embeddings = await self._get_embeddings_from_api(uncached_data)
            embeddings.extend(api_embeddings)

        embeddings.sort(key=lambda x: x[0])
        return np.array([emb for _, emb in embeddings], dtype=np.float32)

    def _process_cached_embeddings(self, texts: List[str]) -> Tuple[List[Tuple[int, np.ndarray]], Dict[str, List]]:
        """Process texts and return cached embeddings plus uncached data."""
        embeddings = []
        uncached_texts = []
        uncached_indices = []

        for i, text in enumerate(texts):
            if not text.strip():
                continue
            cache_key = hashlib.md5(text.encode('utf-8')).hexdigest()
            if cache_key in self.embedding_cache:
                embeddings.append((i, self.embedding_cache[cache_key].copy()))
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)

        return embeddings, {'texts': uncached_texts, 'indices': uncached_indices}

    async def _get_embeddings_from_api(self, uncached_data: Dict[str, List]) -> List[Tuple[int, np.ndarray]]:
        """Get embeddings from API for uncached texts."""
        try:
            resp = await monitored_embedding_creation(model=config.EMBEDDING_MODEL, input=uncached_data['texts'])

            embeddings = []
            for idx, embedding_data in enumerate(resp.data):
                embedding = np.array(embedding_data.embedding, dtype=np.float32)

                if self._is_valid_embedding(embedding):
                    text_idx = uncached_data['indices'][idx]
                    embeddings.append((text_idx, embedding))
                    cache_key = hashlib.md5(uncached_data['texts'][idx].encode('utf-8')).hexdigest()
                    self._update_cache(cache_key, embedding)
                else:
                    raise ValueError("Invalid embedding shape or zero vector detected")

            return embeddings

        except Exception as e:
            logging.error("OpenAI API error during embedding: %s", e)
            raise EmbeddingError(f"Failed to generate embeddings: {e}")

    def _is_valid_embedding(self, embedding: np.ndarray) -> bool:
        """Validate embedding dimensions and content."""
        return (embedding.shape[0] == config.EMBEDDING_DIM and not np.all(embedding == 0))

    def _update_cache(self, key: str, embedding: np.ndarray) -> None:
        """Updates embedding cache with size limit."""
        if len(self.embedding_cache) >= self.cache_max_size:
            oldest_key = next(iter(self.embedding_cache))
            del self.embedding_cache[oldest_key]

        self.embedding_cache[key] = embedding.copy()

    async def add_memory(self: "SymbolicMemory", entry: MemoryEntryModel) -> None:
        """Adds a new memory entry to the embedding buffer."""
        self._embedding_buffer.append(entry)
        logging.debug("Memory entry for '%s' added. Buffer size: %d", entry.type, len(self._embedding_buffer))

        if len(self._embedding_buffer) >= self._embedding_batch_size:
            await self._process_embedding_buffer()

    async def _process_embedding_buffer(self) -> None:
        """Processes all memory entries in the buffer to generate and store embeddings."""
        if not self._embedding_buffer:
            return

        with metrics.EMBEDDING_FLUSH_LATENCY_SECONDS.time():
            logging.info("Processing embedding buffer with %d entries.", len(self._embedding_buffer))

            entries_to_process = self._embedding_buffer[:]
            self._embedding_buffer.clear()

            texts_to_embed = [json.dumps(entry.content) for entry in entries_to_process]

            try:
                embeddings = await self.embed_async(texts_to_embed)
            except EmbeddingError as e:
                logging.error("Failed to generate embeddings: %s", e)
                self.pending_memories.extend(entries_to_process)
                return

            if embeddings.shape[0] != len(entries_to_process):
                logging.error("Embedding batch failed: Mismatch between entries (%d) and embeddings (%d). Entries discarded.", len(entries_to_process), embeddings.shape[0])
                return

            new_vectors: List[np.ndarray] = []
            new_ids: List[int] = []

            async with aiosqlite.connect(self._db_path) as db:
                for i, entry in enumerate(entries_to_process):
                    entry.embedding = embeddings[i].tolist()
                    cursor = await db.execute(
                        "INSERT INTO memories (uuid, type, content, timestamp, importance, embedding) VALUES (?, ?, ?, ?, ?, ?)",
                        (entry.id, entry.type, json.dumps(entry.content), entry.timestamp, entry.importance, embeddings[i].tobytes())
                    )
                    rowid = cursor.lastrowid
                    if rowid:
                        self.memory_map[rowid] = entry
                        new_vectors.append(embeddings[i])
                        new_ids.append(rowid)
                await db.commit()

            if new_vectors:
                vector_matrix = np.vstack(new_vectors).astype('float32')
                id_array = np.array(new_ids).astype('int64')
                self.faiss_index.add_with_ids(vector_matrix, id_array)
                logging.info("Successfully processed and added %d memories to DB and FAISS index.", len(new_vectors))

    async def consolidate_memories(self: "SymbolicMemory", window_seconds: int = 86400) -> None:
        """Consolidate recent memories into insights to reduce memory clutter."""
        await self._process_embedding_buffer()
        logging.info("Attempting to consolidate memories from the last %d seconds.", window_seconds)

        memories_to_consolidate = self._get_eligible_memories_for_consolidation(window_seconds)
        if len(memories_to_consolidate) < 10:
            logging.info("Not enough recent memories to warrant consolidation.")
            return

        try:
            consolidated_entry = await self._create_consolidated_memory(memories_to_consolidate)
            await self.add_memory(consolidated_entry)
            await self._remove_consolidated_memories(memories_to_consolidate)
            self._rebuild_faiss_index()
            logging.info("Successfully consolidated %d memories into insight.", len(memories_to_consolidate))
        except Exception as e:
            logging.error("Memory consolidation failed: %s", e, exc_info=True)

    def _get_eligible_memories_for_consolidation(self, window_seconds: int) -> List[MemoryEntryModel]:
        """Get memories eligible for consolidation within the time window."""
        now = datetime.now(timezone.utc)
        consolidation_window = now - timedelta(seconds=window_seconds)
        eligible_types: Set[MemoryType] = {"action_result", "inner_monologue", "tool_usage", "user_input", "emotion", "perception"}
        return [mem for mem in self.memory_map.values() if mem and mem.type in eligible_types and datetime.fromisoformat(mem.timestamp) > consolidation_window]

    async def _create_consolidated_memory(self, memories: List[MemoryEntryModel]) -> MemoryEntryModel:
        """Create a consolidated memory from multiple individual memories."""
        narrative_parts = []
        total_importance = 0.0

        for mem in sorted(memories, key=lambda m: m.timestamp):
            content_summary = json.dumps(mem.content)
            if len(content_summary) > 150:
                content_summary = content_summary[:147] + "..."
            narrative_parts.append(f"[{mem.timestamp}] ({mem.type}, importance: {mem.importance:.2f}): {content_summary}")
            total_importance += mem.importance

        narrative_str = "\n".join(narrative_parts)
        MAX_CONTEXT_CHARS = 12000
        if len(narrative_str) > MAX_CONTEXT_CHARS:
            narrative_str = narrative_str[:MAX_CONTEXT_CHARS] + "\n...[TRUNCATED DUE TO LENGTH]..."
            logging.warning("Memory consolidation context truncated to %d characters.", MAX_CONTEXT_CHARS)

        summary = await self._generate_memory_summary(narrative_str)
        if not summary:
            raise ValueError("LLM failed to generate memory summary")

        new_importance = min(1.0, (total_importance / len(memories)) + 0.1)
        return MemoryEntryModel(type="insight", content={"summary": summary}, importance=new_importance)

    async def _generate_memory_summary(self, narrative_str: str) -> Optional[str]:
        """Generate a summary from the narrative using LLM."""
        prompt = f"""
The following is a sequence of recent memories from a conscious AGI.
Your task is to synthesize these detailed, low-level events into a single, high-level
narrative summary or insight. Capture the essence of what happened, what was learned,
or the overall emotional tone.

--- MEMORY LOG ---
{narrative_str}
---

Now, provide a concise summary of these events. This summary will replace the original memories.
Respond with ONLY the summary text.
"""
        resp = await monitored_chat_completion(role="meta", messages=[{"role": "system", "content": prompt}])
        return resp.choices[0].message.content.strip() if resp.choices and resp.choices[0].message.content else None

    async def _remove_consolidated_memories(self, memories: List[MemoryEntryModel]) -> None:
        """Remove the original memories that were consolidated."""
        db_ids_to_remove: Set[int] = set()
        for mem in memories:
            for db_id, memory_entry in self.memory_map.items():
                if memory_entry.id == mem.id:
                    db_ids_to_remove.add(db_id)

        if not db_ids_to_remove:
            return

        async with aiosqlite.connect(self._db_path) as db:
            placeholders = ','.join('?' for _ in db_ids_to_remove)
            await db.execute(f"DELETE FROM memories WHERE id IN ({placeholders})", list(db_ids_to_remove))
            await db.commit()

        for db_id in db_ids_to_remove:
            self.memory_map.pop(db_id, None)

    def get_recent_memories(self: "SymbolicMemory", n: int = 10) -> List[MemoryEntryModel]:
        """Get recent memories synchronously since no async operations are needed."""
        sorted_memories = sorted(self.memory_map.values(), key=lambda m: m.timestamp, reverse=True)
        return sorted_memories[:n]

    def get_total_memory_count(self) -> int:
        """Returns the total number of memories, including the buffer."""
        return len(self.memory_map) + len(self._embedding_buffer)

    async def get_relevant_memories(self, query: str, memory_types: Optional[List[MemoryType]] = None, limit: int = 5) -> List[MemoryEntryModel]:
        """Retrieves memories relevant to a query using semantic similarity."""
        if self.faiss_index.ntotal == 0:
            return []

        query_embedding = await self.embed_async([query])
        if query_embedding.size == 0:
            return []

        _, ids = self.faiss_index.search(query_embedding.astype('float32'), limit * 2)

        relevant_memories: List[MemoryEntryModel] = []
        for db_id in ids[0]:
            if db_id != -1 and (memory := self.memory_map.get(int(db_id))):
                if memory_types is None or memory.type in memory_types:
                    relevant_memories.append(memory)

        relevant_memories.sort(key=lambda m: (m.importance, m.timestamp), reverse=True)
        return relevant_memories[:limit]

--- FILE: symbolic_agi/test_consciousness_emotional.py ---


--- FILE: symbolic_agi/tool_plugin.py ---
# symbolic_agi/tool_plugin.py

import inspect
import logging
from typing import TYPE_CHECKING, Any, Dict

from symbolic_agi.tools.base import BaseTool
from symbolic_agi.tools.code import CodeTools
from symbolic_agi.tools.file_system import FileSystemTools
from symbolic_agi.tools.web import WebTools

if TYPE_CHECKING:
    from .agi_controller import SymbolicAGI

class ToolPlugin:
    """
    A dynamic tool aggregator for the AGI.

    This class discovers, instantiates, and delegates calls to modular tool
    classes located in the `tools` subdirectory. This promotes a clean, 
    scalable, and maintainable architecture.
    """

    def __init__(self, agi: "SymbolicAGI"):
        self.agi = agi
        self.logger = logging.getLogger(self.__class__.__name__)

        # Instantiate and register all tool modules
        self.file_system = FileSystemTools(agi)
        self.web = WebTools(agi)
        self.code = CodeTools(agi)

        self.plugins = [self.file_system, self.web, self.code]
        self.logger.info("ToolPlugin initialized with %d modules.", len(self.plugins))

    def __getattr__(self, name: str) -> Any:
        """
        Dynamically find and return the tool method from registered plugins.
        This makes the refactoring transparent to the rest of the system.
        """
        for plugin in self.plugins:
            if hasattr(plugin, name):
                method = getattr(plugin, name)
                if callable(method):
                    self.logger.debug("Delegating call for '%s' to %s", name, plugin.__class__.__name__)
                    return method

        raise AttributeError(f"'{self.__class__.__name__}' and its plugins have no attribute '{name}'")

    def get_all_tool_definitions(self) -> Dict[str, Any]:
        """
        Gathers all action definitions from the registered tool plugins.
        """
        all_actions = {}
        for plugin in self.plugins:
            for method_name, method in inspect.getmembers(plugin, predicate=inspect.iscoroutinefunction):
                if hasattr(method, "_innate_action_def"):
                    action_def = getattr(method, "_innate_action_def")
                    all_actions[action_def.name] = action_def.model_dump()
        return all_actions